<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Cells · LuxRecurrentLayers.jl</title><meta name="title" content="Cells · LuxRecurrentLayers.jl"/><meta property="og:title" content="Cells · LuxRecurrentLayers.jl"/><meta property="twitter:title" content="Cells · LuxRecurrentLayers.jl"/><meta name="description" content="Documentation for LuxRecurrentLayers.jl."/><meta property="og:description" content="Documentation for LuxRecurrentLayers.jl."/><meta property="twitter:description" content="Documentation for LuxRecurrentLayers.jl."/><meta property="og:url" content="https://MartinuzziFrancesco.github.io/LuxRecurrentLayers.jl/api/cells/"/><meta property="twitter:url" content="https://MartinuzziFrancesco.github.io/LuxRecurrentLayers.jl/api/cells/"/><link rel="canonical" href="https://MartinuzziFrancesco.github.io/LuxRecurrentLayers.jl/api/cells/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="LuxRecurrentLayers.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">LuxRecurrentLayers.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">API Documentation</span><ul><li class="is-active"><a class="tocitem" href>Cells</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API Documentation</a></li><li class="is-active"><a href>Cells</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Cells</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/main/docs/src/api/cells.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Cells"><a class="docs-heading-anchor" href="#Cells">Cells</a><a id="Cells-1"></a><a class="docs-heading-anchor-permalink" href="#Cells" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.AntisymmetricRNNCell" href="#LuxRecurrentLayers.AntisymmetricRNNCell"><code>LuxRecurrentLayers.AntisymmetricRNNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">AntisymmetricRNNCell(in_dims =&gt; out_dims, [activation];
    use_bias=true, train_state=false, init_bias=nothing,
    init_recurrent_bias=nothing, init_weight=nothing,
    init_recurrent_weight=nothing, init_state=zeros32,
    epsilon=1.0, gamma=0.0)</code></pre><p><a href="https://arxiv.org/abs/1902.09689">Antisymmetric recurrent cell</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{equation}
    \mathbf{h}(t) = \mathbf{h}(t-1) + \epsilon \cdot \tanh\left(
        \mathbf{W}_{ih} \mathbf{x}(t) + \mathbf{b}_{ih} +
        (\mathbf{W}_{hh} - \mathbf{W}_{hh}^\top - \gamma \cdot \mathbf{I})
        \mathbf{h}(t-1) + \mathbf{b}_{hh} \right)
\end{equation}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) dimension</li><li><code>activation</code>: activation function. Default is <code>tanh</code></li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias <span>$\mathbf{b}_{ih}$</span>. If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_bias</code>: Initializer for recurrent bias <span>$\mathbf{b}_{hh}$</span>. If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight <span>$\mathbf{W}_{ih}$</span>. If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight <span>$\mathbf{W}_{hh}$</span>. If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>epsilon</code>: step size <span>$\epsilon$</span>. Default is 1.0.</li><li><code>gamma</code>: strength of diffusion <span>$\gamma$</span>. Default is 0.0.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated weights to map from input to the hidden state <span>$\mathbf{W}_{ih}$</span>.</li><li><code>weight_hh</code>: Concatenated weights to map from hidden to the hidden state <span>$\mathbf{W}_{hh}$</span>.</li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if   <code>use_bias=false</code>) <span>$\mathbf{b}_{ih}$</span>.</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if   <code>use_bias=false</code>) <span>$\mathbf{b}_{hh}$</span>.</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/antisymmetricrnn_cell.jl#L2-L84">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.ATRCell" href="#LuxRecurrentLayers.ATRCell"><code>LuxRecurrentLayers.ATRCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ATRCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, init_bias=nothing,
    init_recurrent_bias=nothing, init_weight=nothing,
    init_recurrent_weight=nothing, init_state=zeros32)</code></pre><p><a href="https://arxiv.org/abs/1810.12546">Addition-subtraction twin-gated recurrent cell</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{p}(t) &amp;= \mathbf{W}_{ih} \mathbf{x}(t) + \mathbf{b}_{ih}, \\
    \mathbf{q}(t) &amp;= \mathbf{W}_{ih} \mathbf{h}(t-1) + \mathbf{b}_{hh}, \\
    \mathbf{i}(t) &amp;= \sigma(\mathbf{p}(t) + \mathbf{q}(t)), \\
    \mathbf{f}(t) &amp;= \sigma(\mathbf{p}(t) - \mathbf{q}(t)), \\
    \mathbf{h}(t) &amp;= \mathbf{i}(t) \circ \mathbf{p}(t) + \mathbf{f}(t)
        \circ \mathbf{h}(t-1).
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>train_memory</code>: Flag to set the initial memory state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for input to hidden bias <span>$\mathbf{b}_{ih}$</span>. If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_recurrent_bias</code>: Initializer for hidden to hidden bias <span>$\mathbf{b}_{hh}$</span>. If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for input to hidden weights <span>$\mathbf{W}_{ih}$</span>. If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight <span>$\mathbf{W}_{hh}$</span>. If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>init_memory</code>: Initializer for memory. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated weights to map from input to the hidden state <span>$\mathbf{W}_{ih}$</span>.</li><li><code>weight_hh</code>: Concatenated weights to map from hidden to the hidden state <span>$\mathbf{W}_{hh}$</span>.</li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if   <code>use_bias=false</code>) <span>$\mathbf{b}_{ih}$</span>.</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if   <code>use_bias=false</code>) <span>$\mathbf{b}_{hh}$</span>.</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/atr_cell.jl#L2-L89">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.BRCell" href="#LuxRecurrentLayers.BRCell"><code>LuxRecurrentLayers.BRCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">BRCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, init_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32)</code></pre><p><a href="https://doi.org/10.1371/journal.pone.0252676">Bistable recurrent cell</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{a}(t) &amp;= 1 + \tanh\left(\mathbf{W}_{ih}^{a} \mathbf{x}(t) +
        \mathbf{b}_{ih}^a + \mathbf{w}_{hh}^{a} \circ \mathbf{h}(t-1) +
        \mathbf{b}_{hh}^a \right) \\
    \mathbf{c}(t) &amp;= \sigma\left(\mathbf{W}_{ih}^{c} \mathbf{x}(t) +
        \mathbf{b}_{ih}^c + \mathbf{w}_{hh}^{c} \circ \mathbf{h}(t-1) +
        \mathbf{b}_{hh}^c \right)\\
    \mathbf{h}(t) &amp;= \mathbf{c}(t) \circ \mathbf{h}(t-1) + (1 - \mathbf{c}(t))
        \circ \tanh\left(\mathbf{W}_{ih}^{h} \mathbf{x}(t) + \mathbf{b}_{ih}^h +
        \mathbf{a}(t) \circ \mathbf{h}(t-1)\right)
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for input to hidden bias <span>$\mathbf{b}_{ih}^a, \mathbf{b}_{ih}^c, \mathbf{b}_{ih}^h$</span>. Must be a tuple containing 3 functions, e.g., <code>(glorot_normal, kaiming_uniform)</code>. If a single function <code>fn</code> is provided, it is automatically expanded into a  3-element tuple (fn, fn). If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_recurrent_bias</code>: Initializer for hidden to hidden bias <span>$\mathbf{b}_{hh}^a, \mathbf{b}_{hh}^c$</span>. Must be a tuple containing 2 functions, e.g., <code>(glorot_normal, kaiming_uniform)</code>. If a single function <code>fn</code> is provided, it is automatically expanded into a  2-element tuple (fn, fn). If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for input to hidden weights <span>$\mathbf{W}_{ih}^a, \mathbf{W}_{ih}^c, \mathbf{W}_{ih}^h$</span>. Must be a tuple containing 3 functions, e.g., <code>(glorot_normal, kaiming_uniform)</code>. If a single function <code>fn</code> is provided, it is automatically expanded into a 3-element tuple (fn, fn). If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for input to hidden weights <span>$\mathbf{w}_{hh}^a, \mathbf{w}_{hh}^c$</span>. Must be a tuple containing 2 functions, e.g., <code>(glorot_normal, kaiming_uniform)</code>. If a single function <code>fn</code> is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated weights to map from input to the hidden state              <span>$\{ \mathbf{W}_{ih}^a, \mathbf{W}_{ih}^c, \mathbf{W}_{ih}^h \}$</span> The initializers in <code>init_weight</code> are applied in the order they appear: the first function is used for <span>$\mathbf{W}_{ih}^a$</span>, the second for <span>$\mathbf{W}_{ih}^c$</span>, and the third for <span>$\mathbf{W}_{ih}^h$</span>.</li><li><code>weight_hh</code>: Weights to map the hidden state to the hidden state              <span>$\{ \mathbf{w}_{hh}^a, \mathbf{w}_{hh}^c \}$</span> The initializers in <code>init_weight</code> are applied in the order they appear: the first function is used for <span>$\mathbf{w}_{hh}^a$</span>, and the second for <span>$\mathbf{w}_{hh}^c$</span>.</li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)              <span>$\{ \mathbf{b}_{ih}^a, \mathbf{b}_{ih}^c, \mathbf{b}_{ih}^h \}$</span> The initializers in <code>init_bias</code> are applied in the order they appear: the first function is used for <span>$\mathbf{b}_{ih}^z$</span>, the second for <span>$\mathbf{b}_{ih}^c$</span>, and the third for <span>$\mathbf{b}_{ih}^h$</span>.</li><li><code>bias_hh</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)              <span>$\{ \mathbf{b}_{hh}^a, \mathbf{b}_{hh}^c \}$</span> The initializers in <code>init_bias</code> are applied in the order they appear: the first function is used for <span>$\mathbf{b}_{hh}^z$</span>, and the second for <span>$\mathbf{b}_{hh}^c$</span>.</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/br_cell.jl#L2-L114">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.CFNCell" href="#LuxRecurrentLayers.CFNCell"><code>LuxRecurrentLayers.CFNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">CFNCell(in_dims =&gt; out_dims, [activation];
    use_bias=true, train_state=false, init_bias=nothing,
    init_recurrent_bias=nothing, init_weight=nothing,
    init_recurrent_weight=nothing, init_state=zeros32)</code></pre><p><a href="https://arxiv.org/abs/1612.06212">Chaos free network unit</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \boldsymbol{\theta}(t) &amp;= \sigma\left(\mathbf{W}_{ih}^{\theta} \mathbf{x}(t)
        + \mathbf{b}_{ih}^{\theta} + \mathbf{W}_{hh}^{\theta} \mathbf{h}(t-1) +
        \mathbf{b}_{hh}^{\theta}\right) \\
    \boldsymbol{\eta}(t) &amp;= \sigma\left(\mathbf{W}_{ih}^{\eta} \mathbf{x}(t) +
        \mathbf{b}_{ih}^{\eta} + \mathbf{W}_{hh}^{\eta} \mathbf{h}(t-1) + 
        \mathbf{b}_{hh}^{\eta} \right) \\
    \mathbf{h}(t) &amp;= \boldsymbol{\theta}(t) \circ \tanh(\mathbf{h}(t-1)) +
        \boldsymbol{\eta}(t) \circ \tanh(\mathbf{W}_{ih}^h \mathbf{x}(t) +
        \mathbf{b}_{ih}^{h})
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li><li><code>activation</code>: activation function. Default is <code>tanh</code></li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for input to hidden bias <span>$\mathbf{b}_{ih}^{\theta}, \mathbf{b}_{ih}^{\eta}, \mathbf{b}_{ih}^{h}$</span>. Must be a tuple containing 3 functions, e.g., <code>(glorot_normal, kaiming_uniform)</code>. If a single function <code>fn</code> is provided, it is automatically expanded into a 3-element tuple (fn, fn, fn). If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_recurrent_bias</code>: Initializer for hidden to hidden bias <span>$\mathbf{b}_{hh}^{\theta}, \mathbf{b}_{hh}^{\eta}$</span>. Must be a tuple containing 2 functions, e.g., <code>(glorot_normal, kaiming_uniform)</code>. If a single function <code>fn</code> is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for input to hidden weights <span>$\mathbf{W}_{ih}^{\theta}, \mathbf{W}_{ih}^{\eta}, \mathbf{W}_{ih}^{h}$</span>. Must be a tuple containing 3 functions, e.g., <code>(glorot_normal, kaiming_uniform)</code>. If a single function <code>fn</code> is provided, it is automatically expanded into a 3-element tuple (fn, fn, fn). If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for input to hidden weights <span>$\mathbf{W}_{hh}^{\theta}, \mathbf{W}_{hh}^{\eta}$</span>. Must be a tuple containing 2 functions, e.g., <code>(glorot_normal, kaiming_uniform)</code>. If a single function <code>fn</code> is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated weights to map from input to the hidden state.              <span>$\{ \mathbf{W}_{ih}^{\theta}, \mathbf{W}_{ih}^{\eta}, \mathbf{W}_{ih}^{h} \}$</span> The initializers in <code>init_weight</code> are applied in the order they appear: the first function is used for <span>$\mathbf{W}_{ih}^{\theta}$</span>, the second for <span>$\mathbf{W}_{ih}^{\eta}$</span>, and the third for <span>$\mathbf{W}_{ih}^h$</span>.</li><li><code>weight_hh</code>: Concatenated weights to map from hidden to hidden state.              <span>$\{ \mathbf{W}_{hh}^{\theta}, \mathbf{W}_{hh}^{\eta} \}$</span> The initializers in <code>init_recurrent_weight</code> are applied in the order they appear: the first function is used for <span>$\mathbf{W}_{hh}^{\theta}$</span>, and the second for <span>$\mathbf{W}_{hh}^{\eta}$</span>.</li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)              <span>$\{ \mathbf{b}_{ih}^{\theta}, \mathbf{b}_{ih}^{\eta}, \mathbf{b}_{ih}^{h} \}$</span> The initializers in <code>init_bias</code> are applied in the order they appear: the first function is used for <span>$\mathbf{b}_{ih}^{\theta}$</span>, the second for   <span>$\mathbf{b}_{ih}^{\eta}$</span>, and the third for <span>$\mathbf{b}_{ih}^{h}$</span>.</li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)              <span>$\{ \mathbf{b}_{hh}^{\theta}, \mathbf{b}_{hh}^{\eta} \}$</span> The initializers in <code>init_recurrent_bias</code> are applied in the order they appear: the first function is used for <span>$\mathbf{b}_{hh}^{\theta}$</span>, and the second for   <span>$\mathbf{b}_{hh}^{\eta}$</span>.</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/cfn_cell.jl#L2-L117">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.coRNNCell" href="#LuxRecurrentLayers.coRNNCell"><code>LuxRecurrentLayers.coRNNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">coRNNCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, train_memory=false,
    init_bias=nothing, init_recurrent_bias=nothing, init_cell_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_cell_weight=nothing, init_state=zeros32, init_memory=zeros32)
    gamma=0.0, epsilon=0.0, dt=1.0)</code></pre><p><a href="https://arxiv.org/abs/2010.00951">Coupled oscillatory recurrent neural unit</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{c}(t) &amp;= \mathbf{c}(t-1) + \Delta t \, \sigma\left( 
        \mathbf{W}_{ih} \mathbf{x}(t) + \mathbf{b}_{ih} + 
        \mathbf{W}_{hh} \mathbf{h}(t-1) + \mathbf{b}_{hh} + 
        \mathbf{W}_{ch} \mathbf{c}(t-1) + \mathbf{b}_{ch} \right) 
        - \Delta t \, \gamma \, \mathbf{h}(t-1) - \Delta t \, \epsilon \,
        \mathbf{c}(t), \\
    \mathbf{h}(t) &amp;= \mathbf{h}(t-1) + \Delta t \, \mathbf{c}(t)
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>train_memory</code>: Flag to set the initial memory state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for input to hidden bias <span>$\mathbf{b}_{ih}$</span>. If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_recurrent_bias</code>: Initializer for hidden to hidden bias <span>$\mathbf{b}_{hh}$</span>. If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_cell_bias</code>: Initializer for cell to hidden bias <span>$\mathbf{b}_{ch}$</span>. If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for input to hidden weight <span>$\mathbf{W}_{ih}$</span>. If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for hidden to hidden weight <span>$\mathbf{W}_{hh}$</span>. If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_cell_weight</code>: Initializer for cell to hidden weight <span>$\mathbf{W}_{ch}$</span>. If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>init_memory</code>: Initializer for memory. Default set to <code>zeros32</code>.</li><li><code>dt</code>: time step. Default is 1.0.</li><li><code>gamma</code>: Damping for state. Default is 0.0.</li><li><code>epsilon</code>: Damping for candidate state. Default is 0.0.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>false</code> - Creates a hidden state using          <code>init_state</code>, hidden memory using <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>false</code> - Repeats <code>hidden_state</code> vector          from the parameters to match the shape of <code>x</code>, creates hidden memory using          <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1c: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>true</code> - Creates a hidden state using          <code>init_state</code>, repeats the memory vector from parameters to match the shape of          <code>x</code> and proceeds to Case 2.</li><li>Case 1d: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>true</code> - Repeats the hidden state and          memory vectors from the parameters to match the shape of  <code>x</code> and proceeds to          Case 2.</li><li>Case 2: Tuple <code>(x, (h, c))</code> is provided, then the output and a tuple containing the          updated hidden state and memory is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple Containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span> and new memory <span>$c_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Weights to map the input to the hidden state <span>$\mathbf{W}_{ih}$</span>.</li><li><code>weight_hh</code>: Weights to map the hidden state to the hidden state <span>$\mathbf{W}_{hh}$</span>.</li><li><code>weight_ch</code>: Weights to map the cell state to the hidden state <span>$\mathbf{W}_{ch}$</span>.</li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>) <span>$\mathbf{b}_{ih}$</span></li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>) <span>$\mathbf{b}_{hh}$</span></li><li><code>bias_ch</code>: Bias vector for the cell-hidden connection (not present if <code>use_bias=false</code>) <span>$\mathbf{b}_{ch}$</span></li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li><li><code>memory</code>: Initial memory vector (not present if <code>train_memory=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/cornn_cell.jl#L2-L116">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.FastGRNNCell" href="#LuxRecurrentLayers.FastGRNNCell"><code>LuxRecurrentLayers.FastGRNNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">FastGRNNCell(input_size =&gt; hidden_size, [activation];
    use_bias=true, train_state=false, init_bias=nothing,
    init_recurrent_bias=nothing, init_weight=nothing,
    init_recurrent_weight=nothing, init_state=zeros32,
    init_zeta=1.0, init_nu=4.0)</code></pre><p><a href="https://arxiv.org/abs/1901.02358">Fast gated recurrent neural network cell</a>.</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) dimension</li><li><code>activation</code>: activation function. Default is <code>tanh</code></li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for input to hidden bias <span>$\mathbf{b}_{ih}^z, \mathbf{b}_{ih}^h$</span>. Must be a tuple containing 2 functions, e.g., <code>(glorot_normal, kaiming_uniform)</code>. If a single function <code>fn</code> is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_recurrent_bias</code>: Initializer for hidden to hidden bias <span>$\mathbf{b}_{hh}^z, \mathbf{b}_{hh}^h$</span>. Must be a tuple containing 2 functions, e.g., <code>(glorot_normal, kaiming_uniform)</code>. If a single function <code>fn</code> is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight <span>$\mathbf{W}_{ih}$</span>. If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight <span>$\mathbf{W}_{hh}$</span>. If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>init_zeta</code>: initializer for the <span>$\zeta$</span> learnable parameter. Default is 1.0.</li><li><code>init_nu</code>: initializer for the <span>$\nu$</span> learnable parameter. Default is 4.0.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{z}(t) &amp;= \sigma\left( \mathbf{W}_{ih} \mathbf{x}(t) +
        \mathbf{b}_{ih}^{z} + \mathbf{W}_{hh} \mathbf{h}(t-1) +
        \mathbf{b}_{hh}^{z} \right), \\
    \tilde{\mathbf{h}}(t) &amp;= \tanh\left( \mathbf{W}_{ih} \mathbf{x}(t) +
        \mathbf{b}_{ih}^{h} + \mathbf{W}_{hh} \mathbf{h}(t-1) +
        \mathbf{b}_{hh}^{h} \right), \\
    \mathbf{h}(t) &amp;= \left( \left( \zeta (1 - \mathbf{z}(t)) + \nu \right)
        \circ \tilde{\mathbf{h}}(t) \right) + \mathbf{z}(t) \circ
        \mathbf{h}(t-1)
\end{aligned}\]</p><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated weights to map from input to the hidden state <span>$\mathbf{W}_{ih}$</span>.</li><li><code>weight_hh</code>: Concatenated weights to map from hidden to the hidden state <span>$\mathbf{W}_{hh}$</span>.</li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)              <span>$\{ \mathbf{b}_{ih}^z, \mathbf{b}_{ih}^h \}$</span> The initializers in <code>init_bias</code> are applied in the order they appear: the first function is used for <span>$\mathbf{b}_{ih}^z$</span>, and the second for <span>$\mathbf{b}_{ih}^h$</span>.</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)              <span>$\{ \mathbf{b}_{hh}^z, \mathbf{b}_{hh}^h \}$</span> The initializers in <code>init_bias</code> are applied in the order they appear: the first function is used for <span>$\mathbf{b}_{hh}^z$</span>, and the second for <span>$\mathbf{b}_{hh}^h$</span>.</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li><li><code>zeta</code>: Learnable scalar to modulate candidate state.</li><li><code>nu</code>: Learnable scalar to modulate previous state.</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/fastrnn_cell.jl#L153-L253">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.FastRNNCell" href="#LuxRecurrentLayers.FastRNNCell"><code>LuxRecurrentLayers.FastRNNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">FastRNNCell(in_dims =&gt; out_dims, [activation];
    use_bias=true, train_state=false, init_bias=nothing,
    init_recurrent_bias=nothing, init_weight=nothing,
    init_recurrent_weight=nothing, init_state=zeros32,
    init_alpha=-3.0, init_beta=3.0)</code></pre><p><a href="https://arxiv.org/abs/1901.02358">Fast recurrent neural network cell</a>.</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) dimension</li><li><code>activation</code>: activation function. Default is <code>tanh</code></li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias <span>$\mathbf{b}_{ih}$</span>. If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_recurrent_bias</code>: Initializer for hidden to hidden bias <span>$\mathbf{b}_{hh}^z, \mathbf{b}_{hh}^h$</span>. If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight <span>$\mathbf{W}_{ih}$</span>. If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight <span>$\mathbf{W}_{hh}$</span>. If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>init_alpha</code>: initializer for the <span>$\alpha$</span> learnable parameter. Default is -3.0.</li><li><code>init_beta</code>: initializer for the <span>$\beta$</span> learnable parameter. Default is 3.0.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \tilde{\mathbf{h}}(t) &amp;= \sigma\left( \mathbf{W}_{ih} \mathbf{x}(t) +
        \mathbf{b}_{ih} + \mathbf{W}_{hh} \mathbf{h}(t-1) + \mathbf{b}_{hh}
        \right), \\
    \mathbf{h}(t) &amp;= \alpha \, \tilde{\mathbf{h}}(t) + \beta \,
        \mathbf{h}(t-1)
\end{aligned}\]</p><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated weights to map from input to the hidden state <span>$\mathbf{W}_{ih}$</span>.</li><li><code>weight_hh</code>: Concatenated weights to map from hidden to the hidden state <span>$\mathbf{W}_{hh}$</span>.</li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if   <code>use_bias=false</code>) <span>$\mathbf{b}_{ih}$</span>.</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if   <code>use_bias=false</code>) <span>$\mathbf{b}_{hh}$</span>.</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li><li><code>alpha</code>: Learnable scalar to modulate candidate state.</li><li><code>beta</code>: Learnable scalar to modulate previous state.</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/fastrnn_cell.jl#L2-L88">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.GatedAntisymmetricRNNCell" href="#LuxRecurrentLayers.GatedAntisymmetricRNNCell"><code>LuxRecurrentLayers.GatedAntisymmetricRNNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">GatedAntisymmetricRNNCell(in_dims =&gt; out_dims, [activation];
    use_bias=true, train_state=false, init_bias=nothing,
    init_recurrent_bias=nothing, init_weight=nothing,
    init_recurrent_weight=nothing, init_state=zeros32,
    epsilon=1.0, gamma=0.0)</code></pre><p><a href="https://arxiv.org/abs/1902.09689">Antisymmetric recurrent cell with gating</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{z}(t) &amp;= \sigma\left( 
        (\mathbf{W}_{hh} - \mathbf{W}_{hh}^\top - \gamma \cdot \mathbf{I})
        \mathbf{h}(t-1) + \mathbf{b}_{hh} + \mathbf{W}_{ih}^z \mathbf{x}(t)
        + \mathbf{b}_{ih}^z \right), \\
    \mathbf{h}(t) &amp;= \mathbf{h}(t-1) + \epsilon \cdot \mathbf{z}(t) \circ
        \tanh\left( (\mathbf{W}_{hh} - \mathbf{W}_{hh}^\top - \gamma \cdot
        \mathbf{I}) \mathbf{h}(t-1) + \mathbf{b}_{hh} + \mathbf{W}_{ih}^x
        \mathbf{x}(t) + \mathbf{b}_{ih}^h \right).
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li><li><code>activation</code>: activation function. Default is <code>tanh</code></li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for input to hidden bias <span>$\mathbf{b}_{ih}^z, \mathbf{b}_{ih}^h$</span>. Must be a tuple containing 2 functions, e.g., <code>(glorot_normal, kaiming_uniform)</code>. If a single function <code>fn</code> is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_recurrent_bias</code>: Initializer for hidden to hidden bias <span>$\mathbf{b}_{hh}$</span>. If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for input to hidden weights <span>$\mathbf{W}_{ih}^z, \mathbf{W}_{ih}^x$</span>. Must be a tuple containing 2 functions, e.g., <code>(glorot_normal, kaiming_uniform)</code>. If a single function <code>fn</code> is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight <span>$\mathbf{W}_{hh}$</span>. If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>epsilon</code>: step size. Default is 1.0.</li><li><code>gamma</code>: strength of diffusion. Default is 0.0.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated weights to map from input to the hidden state.              <span>$\{ \mathbf{W}_{ih}^z, \mathbf{W}_{ih}^h \}$</span> The initializers in <code>init_weight</code> are applied in the order they appear: the first function is used for <span>$\mathbf{W}_{ih}^z$</span>, and the second for <span>$\mathbf{W}_{ih}^h$</span>.</li><li><code>weight_hh</code>: Weights to map the hidden state to the hidden state <span>$\mathbf{W}_{hh}$</span>.</li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)              <span>$\{ \mathbf{b}_{ih}^z, \mathbf{b}_{ih}^h \}$</span> The initializers in <code>init_bias</code> are applied in the order they appear: the first function is used for <span>$\mathbf{b}_{ih}^z$</span>, and the second for <span>$\mathbf{b}_{ih}^h$</span>.</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>) <span>$\mathbf{b}_{hh}$</span></li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/antisymmetricrnn_cell.jl#L147-L246">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.IndRNNCell" href="#LuxRecurrentLayers.IndRNNCell"><code>LuxRecurrentLayers.IndRNNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">IndRNNCell(in_dims =&gt; out_dims, [activation];
    use_bias=true, train_state=false, init_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32)</code></pre><p><a href="https://arxiv.org/pdf/1803.04831">Independently recurrent cell</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\mathbf{h}_{t} = \sigma(\mathbf{W} \mathbf{x}_t + \mathbf{u} \odot \mathbf{h}_{t-1} +
    \mathbf{b})\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li><li>&#39;activation&#39;: Activation function. Defaults to <code>tanh_fast</code></li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Weights to map from input space              <span>$\{W \}$</span>.</li><li><code>weight_hh</code>: Weights to map from hidden space              <span>$\{ w_h \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/indrnn_cell.jl#L2-L75">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.JANETCell" href="#LuxRecurrentLayers.JANETCell"><code>LuxRecurrentLayers.JANETCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">JANETCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, train_memory=false,
    init_bias=nothing, init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32, init_memory=zeros32, beta=1.0)</code></pre><p><a href="https://arxiv.org/abs/1804.04849">Just another network unit</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{s}_t &amp;= \mathbf{U}_f \mathbf{h}_{t-1} + \mathbf{W}_f \mathbf{x}_t + \mathbf{b}_f \\
    \tilde{\mathbf{c}}_t &amp;= \tanh (\mathbf{U}_c \mathbf{h}_{t-1} + \mathbf{W}_c \mathbf{x}_t + \mathbf{b}_c) \\
    \mathbf{c}_t &amp;= \sigma(\mathbf{s}_t) \odot \mathbf{c}_{t-1} + (1 - \sigma (\mathbf{s}_t - \beta)) \odot \tilde{\mathbf{c}}_t \\
    \mathbf{h}_t &amp;= \mathbf{c}_t.
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>train_memory</code>: Flag to set the initial memory state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>init_memory</code>: Initializer for memory. Default set to <code>zeros32</code>.</li><li><code>beta</code>: Control parameter over the input data flow. Default is 1.0.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>false</code> - Creates a hidden state using          <code>init_state</code>, hidden memory using <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>false</code> - Repeats <code>hidden_state</code> vector          from the parameters to match the shape of <code>x</code>, creates hidden memory using          <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1c: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>true</code> - Creates a hidden state using          <code>init_state</code>, repeats the memory vector from parameters to match the shape of          <code>x</code> and proceeds to Case 2.</li><li>Case 1d: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>true</code> - Repeats the hidden state and          memory vectors from the parameters to match the shape of  <code>x</code> and proceeds to          Case 2.</li><li>Case 2: Tuple <code>(x, (h, c))</code> is provided, then the output and a tuple containing the          updated hidden state and memory is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple Containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span> and new memory <span>$c_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated Weights to map from input space              <span>$\{ W_{if}, W_{ic} \}$</span>.</li><li><code>weight_hh</code>: Concatenated Weights to map from hidden space              <span>$\{ W_{hf}, W_{hc} \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Concatenated Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li><li><code>memory</code>: Initial memory vector (not present if <code>train_memory=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/janet_cell.jl#L2-L94">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.LEMCell" href="#LuxRecurrentLayers.LEMCell"><code>LuxRecurrentLayers.LEMCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">LEMCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, train_memory=false,
    init_bias=nothing, init_weight=nothing, init_recurrent_weight=nothing,
    init_context_weight=nothing, init_state=zeros32, init_memory=zeros32)</code></pre><p><a href="https://arxiv.org/pdf/2110.04744">Long expressive memory unit</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
\boldsymbol{\Delta t_n} &amp;= \Delta \hat{t} \hat{\sigma}
    (W_1 y_{n-1} + V_1 u_n + b_1) \\
\overline{\boldsymbol{\Delta t_n}} &amp;= \Delta \hat{t}
    \hat{\sigma} (W_2 y_{n-1} + V_2 u_n + b_2) \\
z_n &amp;= (1 - \boldsymbol{\Delta t_n}) \odot z_{n-1} +
    \boldsymbol{\Delta t_n} \odot \sigma (W_z y_{n-1} + V_z u_n + b_z) \\
y_n &amp;= (1 - \boldsymbol{\Delta t_n}) \odot y_{n-1} +
    \boldsymbol{\Delta t_n} \odot \sigma (W_y z_n + V_y u_n + b_y)
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>train_memory</code>: Flag to set the initial memory state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_context_weight</code>: Initializer for context weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>init_memory</code>: Initializer for memory. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>false</code> - Creates a hidden state using          <code>init_state</code>, hidden memory using <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>false</code> - Repeats <code>hidden_state</code> vector          from the parameters to match the shape of <code>x</code>, creates hidden memory using          <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1c: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>true</code> - Creates a hidden state using          <code>init_state</code>, repeats the memory vector from parameters to match the shape of          <code>x</code> and proceeds to Case 2.</li><li>Case 1d: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>true</code> - Repeats the hidden state and          memory vectors from the parameters to match the shape of  <code>x</code> and proceeds to          Case 2.</li><li>Case 2: Tuple <code>(x, (h, c))</code> is provided, then the output and a tuple containing the          updated hidden state and memory is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple Containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span> and new memory <span>$c_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated Weights to map from input space              <span>$\{ W_{if}, W_{ic} \}$</span>.</li><li><code>weight_hh</code>: Concatenated Weights to map from hidden space              <span>$\{ W_{hf}, W_{hc} \}$</span></li><li><code>weight_hh</code>: Concatenated Weights to map from context space              <span>$\{ W_{cf}, W_{cc} \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Concatenated Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>alpha</code>: Initial context strength.</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li><li><code>memory</code>: Initial memory vector (not present if <code>train_memory=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/lem_cell.jl#L3-L104">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.LightRUCell" href="#LuxRecurrentLayers.LightRUCell"><code>LuxRecurrentLayers.LightRUCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">LightRUCell(in_dims =&gt; out_dims, [activation];
    use_bias=true, train_state=false, init_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32)</code></pre><p><a href="https://www.mdpi.com/2079-9292/13/16/3204">Light recurrent unit</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \tilde{h}_t &amp;= \tanh(W_h x_t), \\
    f_t         &amp;= \delta(W_f x_t + U_f h_{t-1} + b_f), \\
    h_t         &amp;= (1 - f_t) \odot h_{t-1} + f_t \odot \tilde{h}_t.
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Weights to map from input space              <span>$\{W \}$</span>.</li><li><code>weight_hh</code>: Weights to map from hidden space              <span>$\{ w_h \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/lightru_cell.jl#L2-L77">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.LiGRUCell" href="#LuxRecurrentLayers.LiGRUCell"><code>LuxRecurrentLayers.LiGRUCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">LiGRUCell(in_dims =&gt; out_dims, [activation];
    use_bias=true, train_state=false, init_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32)</code></pre><p><a href="https://arxiv.org/pdf/1803.10225">Light gated recurrent unit</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    z_t &amp;= \sigma(W_z x_t + U_z h_{t-1}), \\
    \tilde{h}_t &amp;= \text{ReLU}(W_h x_t + U_h h_{t-1}), \\
    h_t &amp;= z_t \odot h_{t-1} + (1 - z_t) \odot \tilde{h}_t
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Weights to map from input space              <span>$\{W \}$</span>.</li><li><code>weight_hh</code>: Weights to map from hidden space              <span>$\{ w_h \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/ligru_cell.jl#L2-L77">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.MGUCell" href="#LuxRecurrentLayers.MGUCell"><code>LuxRecurrentLayers.MGUCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MGUCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, init_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32)</code></pre><p><a href="https://arxiv.org/pdf/1603.09420">Minimal gated unit</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    f_t         &amp;= \sigma(W_f x_t + U_f h_{t-1} + b_f), \\
    \tilde{h}_t &amp;= \tanh(W_h x_t + U_h (f_t \odot h_{t-1}) + b_h), \\
    h_t         &amp;= (1 - f_t) \odot h_{t-1} + f_t \odot \tilde{h}_t
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Weights to map from input space              <span>$\{W \}$</span>.</li><li><code>weight_hh</code>: Weights to map from hidden space              <span>$\{ w_h \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/mgu_cell.jl#L2-L77">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.MinimalRNNCell" href="#LuxRecurrentLayers.MinimalRNNCell"><code>LuxRecurrentLayers.MinimalRNNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MinimalRNNCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, init_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32,)</code></pre><p><a href="https://arxiv.org/abs/1711.06788">Minimal recurrent neural network unit</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{z}_t &amp;= \Phi(\mathbf{x}_t) = \tanh(\mathbf{W}_x \mathbf{x}_t +
        \mathbf{b}_z), \\
    \mathbf{u}_t &amp;= \sigma(\mathbf{U}_h \mathbf{h}_{t-1} + \mathbf{U}_z \mathbf{z}_t +
        \mathbf{b}_u), \\
    \mathbf{h}_t &amp;= \mathbf{u}_t \circ \mathbf{h}_{t-1} + (1 - \mathbf{u}_t) \circ
        \mathbf{z}_t.
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>use_bias</code>: Set to false to deactivate bias</li><li><code>train_state</code>: Trainable initial hidden state can be activated by setting this to <code>true</code></li><li><code>train_memory</code>: Trainable initial memory can be activated by setting this to <code>true</code></li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>.</li><li><code>init_state</code>: Initializer for hidden state</li><li><code>init_memory</code>: Initializer for memory</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>false</code> - Creates a hidden state using          <code>init_state</code>, hidden memory using <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>false</code> - Repeats <code>hidden_state</code> vector          from the parameters to match the shape of <code>x</code>, creates hidden memory using          <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1c: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>true</code> - Creates a hidden state using          <code>init_state</code>, repeats the memory vector from parameters to match the shape of          <code>x</code> and proceeds to Case 2.</li><li>Case 1d: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>true</code> - Repeats the hidden state and          memory vectors from the parameters to match the shape of  <code>x</code> and proceeds to          Case 2.</li><li>Case 2: Tuple <code>(x, (h, c))</code> is provided, then the output and a tuple containing the          updated hidden state and memory is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple Containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span> and new memory <span>$c_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Weights to map from input space.</li><li><code>weight_hh</code>: Weights to map from hidden space.</li><li><code>weight_mm</code>: Weights to map from memory space.</li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>).</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>).</li><li><code>bias_hh</code>: Bias vector for the mnemrory-memory connection (not present if <code>use_bias=false</code>).</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>).</li><li><code>memory</code>: Initial memory vector (not present if <code>train_memory=false</code>).</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/minimalrnn_cell.jl#L2-L93">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.MultiplicativeLSTMCell" href="#LuxRecurrentLayers.MultiplicativeLSTMCell"><code>LuxRecurrentLayers.MultiplicativeLSTMCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MultiplicativeLSTMCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, train_memory=false,
    init_bias=nothing, init_recurrent_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32, init_memory=zeros32)</code></pre><p><a href="https://arxiv.org/abs/1609.07959">Multiplicative long short term memory cell</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{m}_t &amp;= (\mathbf{W}_{mx} \mathbf{x}_t) \circ (\mathbf{W}_{mh}
        \mathbf{h}_{t-1}), \\
    \hat{\mathbf{h}}_t &amp;= \mathbf{W}_{hx} \mathbf{x}_t + \mathbf{W}_{hm}
        \mathbf{m}_t, \\
    \mathbf{i}_t &amp;= \sigma(\mathbf{W}_{ix} \mathbf{x}_t + \mathbf{W}_{im}
        \mathbf{m}_t), \\
    \mathbf{o}_t &amp;= \sigma(\mathbf{W}_{ox} \mathbf{x}_t + \mathbf{W}_{om}
        \mathbf{m}_t), \\
    \mathbf{f}_t &amp;= \sigma(\mathbf{W}_{fx} \mathbf{x}_t + \mathbf{W}_{fm}
        \mathbf{m}_t), \\
    \mathbf{c}_t &amp;= \mathbf{f}_t \circ \mathbf{c}_{t-1} + \mathbf{i}_t \circ
        \tanh(\hat{\mathbf{h}}_t), \\
    \mathbf{h}_t &amp;= \tanh(\mathbf{c}_t) \circ \mathbf{o}_t.
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>train_memory</code>: Flag to set the initial memory state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_bias</code>: Initializer for recurrent bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_multiplicative_bias</code>: Initializer for multiplicative bias. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_multiplicative_weight</code>: Initializer for multiplicative weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>init_memory</code>: Initializer for memory. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>false</code> - Creates a hidden state using          <code>init_state</code>, hidden memory using <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>false</code> - Repeats <code>hidden_state</code> vector          from the parameters to match the shape of <code>x</code>, creates hidden memory using          <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1c: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>true</code> - Creates a hidden state using          <code>init_state</code>, repeats the memory vector from parameters to match the shape of          <code>x</code> and proceeds to Case 2.</li><li>Case 1d: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>true</code> - Repeats the hidden state and          memory vectors from the parameters to match the shape of  <code>x</code> and proceeds to          Case 2.</li><li>Case 2: Tuple <code>(x, (h, c))</code> is provided, then the output and a tuple containing the          updated hidden state and memory is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple Containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span> and new memory <span>$c_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated Weights to map from input space              <span>$\{ W_{if}, W_{ic}, W_{ii}, W_{io} \}$</span>.</li><li><code>weight_hh</code>: Concatenated weights to map from hidden space              <span>$\{ W_{hf}, W_{hc}, W_{hi}, W_{ho} \}$</span></li><li><code>weight_ph</code>: Concatenated weights to map from multiplicative space              <span>$\{ W_{ff}, W_{fc}, W_{fi} \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Concatenated Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_ph</code>: Concatenated Bias vector for the multiplicative-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li><li><code>memory</code>: Initial memory vector (not present if <code>train_memory=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/multiplicativelstm_cell.jl#L2-L118">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.MUT1Cell" href="#LuxRecurrentLayers.MUT1Cell"><code>LuxRecurrentLayers.MUT1Cell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MUT1Cell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, init_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32)</code></pre><p><a href="https://proceedings.mlr.press/v37/jozefowicz15.pdf">Mutated unit 1 cell</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    z &amp;= \sigma(W_z x_t + b_z), \\
    r &amp;= \sigma(W_r x_t + U_r h_t + b_r), \\
    h_{t+1} &amp;= \tanh(U_h (r \odot h_t) + \tanh(W_h x_t) + b_h) \odot z \\
        &amp;\quad + h_t \odot (1 - z).
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Weights to map from input space              <span>$\{W \}$</span>.</li><li><code>weight_hh</code>: Weights to map from hidden space              <span>$\{ w_h \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/mut_cell.jl#L2-L78">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.MUT2Cell" href="#LuxRecurrentLayers.MUT2Cell"><code>LuxRecurrentLayers.MUT2Cell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MUT2Cell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, init_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32)</code></pre><p><a href="https://proceedings.mlr.press/v37/jozefowicz15.pdf">Mutated unit 2 cell</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    z &amp;= \sigma(W_z x_t + U_z h_t + b_z), \\
    r &amp;= \sigma(x_t + U_r h_t + b_r), \\
    h_{t+1} &amp;= \tanh(U_h (r \odot h_t) + W_h x_t + b_h) \odot z \\
        &amp;\quad + h_t \odot (1 - z).
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Weights to map from input space              <span>$\{W \}$</span>.</li><li><code>weight_hh</code>: Weights to map from hidden space              <span>$\{ w_h \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/mut_cell.jl#L148-L224">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.MUT3Cell" href="#LuxRecurrentLayers.MUT3Cell"><code>LuxRecurrentLayers.MUT3Cell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MUT3Cell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, init_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32)</code></pre><p><a href="https://proceedings.mlr.press/v37/jozefowicz15.pdf">Mutated unit 2 cell</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    z &amp;= \sigma(W_z x_t + U_z h_t + b_z), \\
    r &amp;= \sigma(x_t + U_r h_t + b_r), \\
    h_{t+1} &amp;= \tanh(U_h (r \odot h_t) + W_h x_t + b_h) \odot z \\
        &amp;\quad + h_t \odot (1 - z).
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Weights to map from input space              <span>$\{W \}$</span>.</li><li><code>weight_hh</code>: Weights to map from hidden space              <span>$\{ w_h \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/mut_cell.jl#L293-L369">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.NASCell" href="#LuxRecurrentLayers.NASCell"><code>LuxRecurrentLayers.NASCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">NASCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, train_memory=false,
    init_bias=nothing, init_recurrent_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32, init_memory=zeros32)</code></pre><p><a href="https://arxiv.org/pdf/1611.01578">Neural Architecture Search unit</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
\text{First Layer Outputs:} &amp; \\
o_1 &amp;= \sigma(W_i^{(1)} x_t + W_h^{(1)} h_{t-1} + b^{(1)}), \\
o_2 &amp;= \text{ReLU}(W_i^{(2)} x_t + W_h^{(2)} h_{t-1} + b^{(2)}), \\
o_3 &amp;= \sigma(W_i^{(3)} x_t + W_h^{(3)} h_{t-1} + b^{(3)}), \\
o_4 &amp;= \text{ReLU}(W_i^{(4)} x_t \cdot W_h^{(4)} h_{t-1}), \\
o_5 &amp;= \tanh(W_i^{(5)} x_t + W_h^{(5)} h_{t-1} + b^{(5)}), \\
o_6 &amp;= \sigma(W_i^{(6)} x_t + W_h^{(6)} h_{t-1} + b^{(6)}), \\
o_7 &amp;= \tanh(W_i^{(7)} x_t + W_h^{(7)} h_{t-1} + b^{(7)}), \\
o_8 &amp;= \sigma(W_i^{(8)} x_t + W_h^{(8)} h_{t-1} + b^{(8)}). \\

\text{Second Layer Computations:} &amp; \\
l_1 &amp;= \tanh(o_1 \cdot o_2) \\
l_2 &amp;= \tanh(o_3 + o_4) \\
l_3 &amp;= \tanh(o_5 \cdot o_6) \\
l_4 &amp;= \sigma(o_7 + o_8) \\

\text{Inject Cell State:} &amp; \\
l_1 &amp;= \tanh(l_1 + c_{\text{state}}) \\

\text{Final Layer Computations:} &amp; \\
c_{\text{new}} &amp;= l_1 \cdot l_2 \\
l_5 &amp;= \tanh(l_3 + l_4) \\
h_{\text{new}} &amp;= \tanh(c_{\text{new}} \cdot l_5)
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>train_memory</code>: Flag to set the initial memory state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_bias</code>: Initializer for recurrent bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>init_memory</code>: Initializer for memory. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>false</code> - Creates a hidden state using          <code>init_state</code>, hidden memory using <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>false</code> - Repeats <code>hidden_state</code> vector          from the parameters to match the shape of <code>x</code>, creates hidden memory using          <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1c: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>true</code> - Creates a hidden state using          <code>init_state</code>, repeats the memory vector from parameters to match the shape of          <code>x</code> and proceeds to Case 2.</li><li>Case 1d: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>true</code> - Repeats the hidden state and          memory vectors from the parameters to match the shape of  <code>x</code> and proceeds to          Case 2.</li><li>Case 2: Tuple <code>(x, (h, c))</code> is provided, then the output and a tuple containing the          updated hidden state and memory is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple Containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span> and new memory <span>$c_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated Weights to map from input space              <span>$\{ W_{if}, W_{ic}, W_{ii}, W_{io} \}$</span>.</li><li><code>weight_hh</code>: Concatenated weights to map from hidden space              <span>$\{ W_{hf}, W_{hc}, W_{hi}, W_{ho} \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Concatenated Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li><li><code>memory</code>: Initial memory vector (not present if <code>train_memory=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/nas_cell.jl#L2-L116">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.NBRCell" href="#LuxRecurrentLayers.NBRCell"><code>LuxRecurrentLayers.NBRCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">NBRCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, init_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32)</code></pre><p><a href="https://doi.org/10.1371/journal.pone.0252676">Recurrently neuromodulated bistable recurrent cell</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{a}(t) &amp;= 1 + \tanh\left(\mathbf{W}_{ih}^{a} \mathbf{x}(t) +
        \mathbf{b}_{ih}^a + \mathbf{W}_{hh}^{a} \circ \mathbf{h}(t-1)+
        \mathbf{b}_{hh}^a \right) \\
    \mathbf{c}(t) &amp;= \sigma\left(\mathbf{W}_{ih}^{c} \mathbf{x}(t) +
        \mathbf{b}_{ih}^c + \mathbf{W}_{hh}^{c} \circ \mathbf{h}(t-1) +
        \mathbf{b}_{hh}^c \right)\\
    \mathbf{h}(t) &amp;= \mathbf{c}(t) \circ \mathbf{h}(t-1) + (1 - \mathbf{c}(t))
        \circ \tanh\left(\mathbf{W}_{ih}^{h} \mathbf{x}(t) + \mathbf{b}_{ih}^h +
        \mathbf{a}(t) \circ \mathbf{h}(t-1)\right) 
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for input to hidden bias <span>$\mathbf{b}_{ih}^a, \mathbf{b}_{ih}^c, \mathbf{b}_{ih}^h$</span>. Must be a tuple containing 3 functions, e.g., <code>(glorot_normal, kaiming_uniform)</code>. If a single function <code>fn</code> is provided, it is automatically expanded into a  3-element tuple (fn, fn). If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_recurrent_bias</code>: Initializer for hidden to hidden bias <span>$\mathbf{b}_{hh}^a, \mathbf{b}_{hh}^c$</span>. Must be a tuple containing 2 functions, e.g., <code>(glorot_normal, kaiming_uniform)</code>. If a single function <code>fn</code> is provided, it is automatically expanded into a  2-element tuple (fn, fn). If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for input to hidden weights <span>$\mathbf{W}_{ih}^a, \mathbf{W}_{ih}^c, \mathbf{W}_{ih}^h$</span>. Must be a tuple containing 3 functions, e.g., <code>(glorot_normal, kaiming_uniform)</code>. If a single function <code>fn</code> is provided, it is automatically expanded into a 3-element tuple (fn, fn). If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for input to hidden weights <span>$\mathbf{W}_{hh}^a, \mathbf{W}_{hh}^c$</span>. Must be a tuple containing 2 functions, e.g., <code>(glorot_normal, kaiming_uniform)</code>. If a single function <code>fn</code> is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to <code>nothing</code>, weights are initialized from a uniform distribution within <code>[-bound, bound]</code> where <code>bound = inv(sqrt(out_dims))</code>. Default is <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><pre><code class="nohighlight hljs">- `weight_ih`: Concatenated weights to map from input to the hidden state
             ``\{ \mathbf{W}_{ih}^a, \mathbf{W}_{ih}^c, \mathbf{W}_{ih}^h \}``
The initializers in `init_weight` are applied in the order they appear:
the first function is used for $\mathbf{W}_{ih}^a$, the second for $\mathbf{W}_{ih}^c$,
and the third for $\mathbf{W}_{ih}^h$.</code></pre><ul><li><code>weight_hh</code>: Weights to map the hidden state to the hidden state              <span>$\{ \mathbf{W}_{hh}^a, \mathbf{W}_{hh}^c \}$</span> The initializers in <code>init_weight</code> are applied in the order they appear: the first function is used for <span>$\mathbf{W}_{hh}^a$</span>, and the second for <span>$\mathbf{W}_{hh}^c$</span>.</li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)              <span>$\{ \mathbf{b}_{ih}^a, \mathbf{b}_{ih}^c, \mathbf{b}_{ih}^h \}$</span> The initializers in <code>init_bias</code> are applied in the order they appear: the first function is used for <span>$\mathbf{b}_{ih}^z$</span>, the second for <span>$\mathbf{b}_{ih}^c$</span>, and the third for <span>$\mathbf{b}_{ih}^h$</span>.</li><li><code>bias_hh</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)              <span>$\{ \mathbf{b}_{hh}^a, \mathbf{b}_{hh}^c \}$</span> The initializers in <code>init_bias</code> are applied in the order they appear: the first function is used for <span>$\mathbf{b}_{hh}^z$</span>, and the second for <span>$\mathbf{b}_{hh}^c$</span>.</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/br_cell.jl#L192-L304">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.PeepholeLSTMCell" href="#LuxRecurrentLayers.PeepholeLSTMCell"><code>LuxRecurrentLayers.PeepholeLSTMCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">PeepholeLSTMCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, train_memory=false,
    init_bias=nothing, init_recurrent_bias=nothing, init_peephole_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_peephole_weight=nothing, init_state=zeros32, init_memory=zeros32)</code></pre><p><a href="https://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf">Peephole long short term memory</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    z_t &amp;= \tanh(W_z x_t + U_z h_{t-1} + b_z), \\
    i_t &amp;= \sigma(W_i x_t + U_i h_{t-1} + p_i \odot c_{t-1} + b_i), \\
    f_t &amp;= \sigma(W_f x_t + U_f h_{t-1} + p_f \odot c_{t-1} + b_f), \\
    c_t &amp;= f_t \odot c_{t-1} + i_t \odot z_t, \\
    o_t &amp;= \sigma(W_o x_t + U_o h_{t-1} + p_o \odot c_t + b_o), \\
    h_t &amp;= o_t \odot \tanh(c_t).
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>train_memory</code>: Flag to set the initial memory state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_bias</code>: Initializer for recurrent bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_peephole_bias</code>: Initializer for peephole bias. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_peephole_weight</code>: Initializer for peephole weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>init_memory</code>: Initializer for memory. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>false</code> - Creates a hidden state using          <code>init_state</code>, hidden memory using <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>false</code> - Repeats <code>hidden_state</code> vector          from the parameters to match the shape of <code>x</code>, creates hidden memory using          <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1c: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>true</code> - Creates a hidden state using          <code>init_state</code>, repeats the memory vector from parameters to match the shape of          <code>x</code> and proceeds to Case 2.</li><li>Case 1d: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>true</code> - Repeats the hidden state and          memory vectors from the parameters to match the shape of  <code>x</code> and proceeds to          Case 2.</li><li>Case 2: Tuple <code>(x, (h, c))</code> is provided, then the output and a tuple containing the          updated hidden state and memory is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple Containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span> and new memory <span>$c_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated Weights to map from input space              <span>$\{ W_{if}, W_{ic}, W_{ii}, W_{io} \}$</span>.</li><li><code>weight_hh</code>: Concatenated weights to map from hidden space              <span>$\{ W_{hf}, W_{hc}, W_{hi}, W_{ho} \}$</span></li><li><code>weight_ph</code>: Concatenated weights to map from peephole space              <span>$\{ W_{ff}, W_{fc}, W_{fi} \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Concatenated Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_ph</code>: Concatenated Bias vector for the peephole-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li><li><code>memory</code>: Initial memory vector (not present if <code>train_memory=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/peepholelstm_cell.jl#L2-L111">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.RANCell" href="#LuxRecurrentLayers.RANCell"><code>LuxRecurrentLayers.RANCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">RANCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, train_memory=false,
    init_bias=nothing, init_recurrent_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32, init_memory=zeros32)</code></pre><p><a href="https://arxiv.org/pdf/1705.07393">Recurrent Additive Network cell</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
\tilde{c}_t &amp;= W_c x_t, \\
i_t         &amp;= \sigma(W_i x_t + U_i h_{t-1} + b_i), \\
f_t         &amp;= \sigma(W_f x_t + U_f h_{t-1} + b_f), \\
c_t         &amp;= i_t \odot \tilde{c}_t + f_t \odot c_{t-1}, \\
h_t         &amp;= g(c_t)
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>train_memory</code>: Flag to set the initial memory state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_bias</code>: Initializer for recurrent bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>init_memory</code>: Initializer for memory. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>false</code> - Creates a hidden state using          <code>init_state</code>, hidden memory using <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>false</code> - Repeats <code>hidden_state</code> vector          from the parameters to match the shape of <code>x</code>, creates hidden memory using          <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1c: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>true</code> - Creates a hidden state using          <code>init_state</code>, repeats the memory vector from parameters to match the shape of          <code>x</code> and proceeds to Case 2.</li><li>Case 1d: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>true</code> - Repeats the hidden state and          memory vectors from the parameters to match the shape of  <code>x</code> and proceeds to          Case 2.</li><li>Case 2: Tuple <code>(x, (h, c))</code> is provided, then the output and a tuple containing the          updated hidden state and memory is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple Containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span> and new memory <span>$c_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated Weights to map from input space              <span>$\{ W_{if}, W_{ic}, W_{ii}, W_{io} \}$</span>.</li><li><code>weight_hh</code>: Concatenated weights to map from hidden space              <span>$\{ W_{hf}, W_{hc}, W_{hi}, W_{ho} \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Concatenated Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li><li><code>memory</code>: Initial memory vector (not present if <code>train_memory=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/ran_cell.jl#L2-L98">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.SCRNCell" href="#LuxRecurrentLayers.SCRNCell"><code>LuxRecurrentLayers.SCRNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SCRNCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, train_memory=false,
    init_bias=nothing, init_weight=nothing, init_recurrent_weight=nothing,
    init_context_weight=nothing, init_state=zeros32, init_memory=zeros32)</code></pre><p><a href="https://arxiv.org/pdf/1412.7753">Structurally contraint recurrent unit</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
s_t &amp;= (1 - \alpha) W_s x_t + \alpha s_{t-1}, \\
h_t &amp;= \sigma(W_h s_t + U_h h_{t-1} + b_h), \\
y_t &amp;= f(U_y h_t + W_y s_t)
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>train_memory</code>: Flag to set the initial memory state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_context_weight</code>: Initializer for context weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>init_memory</code>: Initializer for memory. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>false</code> - Creates a hidden state using          <code>init_state</code>, hidden memory using <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>false</code> - Repeats <code>hidden_state</code> vector          from the parameters to match the shape of <code>x</code>, creates hidden memory using          <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1c: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>true</code> - Creates a hidden state using          <code>init_state</code>, repeats the memory vector from parameters to match the shape of          <code>x</code> and proceeds to Case 2.</li><li>Case 1d: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>true</code> - Repeats the hidden state and          memory vectors from the parameters to match the shape of  <code>x</code> and proceeds to          Case 2.</li><li>Case 2: Tuple <code>(x, (h, c))</code> is provided, then the output and a tuple containing the          updated hidden state and memory is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple Containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span> and new memory <span>$c_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated Weights to map from input space              <span>$\{ W_{if}, W_{ic} \}$</span>.</li><li><code>weight_hh</code>: Concatenated Weights to map from hidden space              <span>$\{ W_{hf}, W_{hc} \}$</span></li><li><code>weight_hh</code>: Concatenated Weights to map from context space              <span>$\{ W_{cf}, W_{cc} \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Concatenated Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>alpha</code>: Initial context strength.</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li><li><code>memory</code>: Initial memory vector (not present if <code>train_memory=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/scrn_cell.jl#L3-L99">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.SGRNCell" href="#LuxRecurrentLayers.SGRNCell"><code>LuxRecurrentLayers.SGRNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SGRNCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, init_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32)</code></pre><p><a href="https://doi.org/10.1049/gtd2.12056">Simple gated recurrent network</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{f}_t &amp;= \sigma(\mathbf{W} \mathbf{x}_t + \mathbf{U} \mathbf{h}_{t-1} +
        \mathbf{b}), \\
    \mathbf{i}_t &amp;= 1 - \mathbf{f}_t, \\
    \mathbf{h}_t &amp;= \tanh\left(\mathbf{i}_t \circ (\mathbf{W} \mathbf{x}_t) +
        \mathbf{f}_t \circ \mathbf{h}_{t-1}\right).
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Weights to map from input space              <span>$\{W \}$</span>.</li><li><code>weight_hh</code>: Weights to map from hidden space              <span>$\{ w_h \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/sgrn_cell.jl#L2-L79">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.STARCell" href="#LuxRecurrentLayers.STARCell"><code>LuxRecurrentLayers.STARCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">STARCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, init_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32)</code></pre><p><a href="https://arxiv.org/abs/1911.11033">Stackable recurrent cell</a>.</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>train_memory</code>: Flag to set the initial memory state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>init_memory</code>: Initializer for memory. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated Weights to map from input space              <span>$\{ W, W_{\theta},  W_{\eta} \}$</span>.</li><li><code>weight_hh</code>: Concatenated Weights to map from hidden space              <span>$\{ W_{\theta}, W_{\eta} \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/star_cell.jl#L2-L72">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.TGRUCell" href="#LuxRecurrentLayers.TGRUCell"><code>LuxRecurrentLayers.TGRUCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">TGRUCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, train_memory=false,
    init_bias=nothing, init_recurrent_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32, init_memory=zeros32)</code></pre><p><a href="https://arxiv.org/abs/1602.02218">Strongly typed gated recurrent unit</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    z_t &amp;= \mathbf{V}_z \mathbf{x}_{t-1} + \mathbf{W}_z \mathbf{x}_t + \mathbf{b}_z \\
    f_t &amp;= \sigma (\mathbf{V}_f \mathbf{x}_{t-1} + \mathbf{W}_f \mathbf{x}_t +
        \mathbf{b}_f) \\
    o_t &amp;= \tau (\mathbf{V}_o \mathbf{x}_{t-1} + \mathbf{W}_o \mathbf{x}_t + \mathbf{b}_o) \\
    h_t &amp;= f_t \odot h_{t-1} + z_t \odot o_t
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>train_memory</code>: Flag to set the initial memory state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_bias</code>: Initializer for recurrent bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>init_memory</code>: Initializer for memory. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>false</code> - Creates a hidden state using          <code>init_state</code>, hidden memory using <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>false</code> - Repeats <code>hidden_state</code> vector          from the parameters to match the shape of <code>x</code>, creates hidden memory using          <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1c: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>true</code> - Creates a hidden state using          <code>init_state</code>, repeats the memory vector from parameters to match the shape of          <code>x</code> and proceeds to Case 2.</li><li>Case 1d: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>true</code> - Repeats the hidden state and          memory vectors from the parameters to match the shape of  <code>x</code> and proceeds to          Case 2.</li><li>Case 2: Tuple <code>(x, (h, c))</code> is provided, then the output and a tuple containing the          updated hidden state and memory is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple Containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span> and new memory <span>$c_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated Weights to map from input space              <span>$\{ W_{if}, W_{ic}, W_{ii}, W_{io} \}$</span>.</li><li><code>weight_hh</code>: Concatenated weights to map from hidden space              <span>$\{ W_{hf}, W_{hc}, W_{hi}, W_{ho} \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Concatenated Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_ph</code>: Concatenated Bias vector for the memory-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li><li><code>memory</code>: Initial memory vector (not present if <code>train_memory=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/trnn_cell.jl#L149-L248">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.TLSTMCell" href="#LuxRecurrentLayers.TLSTMCell"><code>LuxRecurrentLayers.TLSTMCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">TLSTMCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, train_memory=false,
    init_bias=nothing, init_recurrent_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32, init_memory=zeros32)</code></pre><p><a href="https://arxiv.org/abs/1602.02218">Strongly typed long short term memory cell</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    z_t &amp;= \mathbf{V}_z \mathbf{x}_{t-1} + \mathbf{W}_z \mathbf{x}_t + \mathbf{b}_z \\
    f_t &amp;= \sigma (\mathbf{V}_f \mathbf{x}_{t-1} + \mathbf{W}_f \mathbf{x}_t +
        \mathbf{b}_f) \\
    o_t &amp;= \tau (\mathbf{V}_o \mathbf{x}_{t-1} + \mathbf{W}_o \mathbf{x}_t + \mathbf{b}_o) \\
    c_t &amp;= f_t \odot c_{t-1} + (1 - f_t) \odot z_t \\
    h_t &amp;= c_t \odot o_t
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>train_memory</code>: Flag to set the initial memory state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_bias</code>: Initializer for recurrent bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>init_memory</code>: Initializer for memory. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>false</code> - Creates a hidden state using          <code>init_state</code>, hidden memory using <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>false</code> - Repeats <code>hidden_state</code> vector          from the parameters to match the shape of <code>x</code>, creates hidden memory using          <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1c: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>true</code> - Creates a hidden state using          <code>init_state</code>, repeats the memory vector from parameters to match the shape of          <code>x</code> and proceeds to Case 2.</li><li>Case 1d: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>true</code> - Repeats the hidden state and          memory vectors from the parameters to match the shape of  <code>x</code> and proceeds to          Case 2.</li><li>Case 2: Tuple <code>(x, (h, c))</code> is provided, then the output and a tuple containing the          updated hidden state and memory is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple Containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span> and new memory <span>$c_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated Weights to map from input space              <span>$\{ W_{if}, W_{ic}, W_{ii}, W_{io} \}$</span>.</li><li><code>weight_hh</code>: Concatenated weights to map from hidden space              <span>$\{ W_{hf}, W_{hc}, W_{hi}, W_{ho} \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Concatenated Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li><li><code>memory</code>: Initial memory vector (not present if <code>train_memory=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/trnn_cell.jl#L323-L421">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.TRNNCell" href="#LuxRecurrentLayers.TRNNCell"><code>LuxRecurrentLayers.TRNNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">TRNNCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, init_bias=nothing,
    init_weight=nothing,
    init_state=zeros32)</code></pre><p><a href="https://arxiv.org/abs/1602.02218">Strongly typed recurrent unit</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    z_t &amp;= \mathbf{W} x_t \\
    f_t &amp;= \sigma (\mathbf{V} x_t + b) \\
    h_t &amp;= f_t \odot h_{t-1} + (1 - f_t) \odot z_t
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>train_memory</code>: Flag to set the initial memory state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>init_memory</code>: Initializer for memory. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated Weights to map from input space              <span>$\{ W, W_{\theta},  W_{\eta} \}$</span>.</li><li><code>weight_hh</code>: Concatenated Weights to map from hidden space              <span>$\{ W_{\theta}, W_{\eta} \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/trnn_cell.jl#L2-L81">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.UnICORNNCell" href="#LuxRecurrentLayers.UnICORNNCell"><code>LuxRecurrentLayers.UnICORNNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">UnICORNNCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, train_memory=false,
    init_bias=nothing, init_recurrent_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32, init_memory=zeros32,
    dt=1.0, alpha=0.0)</code></pre><p><a href="https://arxiv.org/abs/2103.05487">Undamped independent controlled oscillatory recurrent neural unit</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    y_n &amp;= y_{n-1} + \Delta t \, \hat{\sigma}(c) \odot z_n, \\
    z_n &amp;= z_{n-1} - \Delta t \, \hat{\sigma}(c) \odot \left[ 
        \sigma \left( w \odot y_{n-1} + V y_{n-1} + b \right) + 
        \alpha y_{n-1} \right].
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>train_memory</code>: Flag to set the initial memory state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_bias</code>: Initializer for recurrent bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>init_memory</code>: Initializer for memory. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>false</code> - Creates a hidden state using          <code>init_state</code>, hidden memory using <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>false</code> - Repeats <code>hidden_state</code> vector          from the parameters to match the shape of <code>x</code>, creates hidden memory using          <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1c: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>true</code> - Creates a hidden state using          <code>init_state</code>, repeats the memory vector from parameters to match the shape of          <code>x</code> and proceeds to Case 2.</li><li>Case 1d: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>true</code> - Repeats the hidden state and          memory vectors from the parameters to match the shape of  <code>x</code> and proceeds to          Case 2.</li><li>Case 2: Tuple <code>(x, (h, c))</code> is provided, then the output and a tuple containing the          updated hidden state and memory is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple Containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span> and new memory <span>$c_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated Weights to map from input space              <span>$\{ W_{if}, W_{ic}, W_{ii}, W_{io} \}$</span>.</li><li><code>weight_hh</code>: Concatenated weights to map from hidden space              <span>$\{ W_{hf}, W_{hc}, W_{hi}, W_{ho} \}$</span>`</li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Concatenated Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li><li><code>memory</code>: Initial memory vector (not present if <code>train_memory=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/unicornn_cell.jl#L2-L98">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.WMCLSTMCell" href="#LuxRecurrentLayers.WMCLSTMCell"><code>LuxRecurrentLayers.WMCLSTMCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">WMCLSTMCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, train_memory=false,
    init_bias=nothing, init_recurrent_bias=nothing, init_memory_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_memory_weight=nothing, init_state=zeros32, init_memory=zeros32)</code></pre><p><a href="https://arxiv.org/abs/2109.00020">Long short term memory cell with working memory connections</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{i}_t &amp;= \sigma\left(\mathbf{W}_{ix} \mathbf{x}_t + \mathbf{W}_{ih}
        \mathbf{h}_{t-1} + \tanh(\mathbf{W}_{ic} \mathbf{c}_{t-1}) +
        \mathbf{b}_i\right), \\
    \mathbf{f}_t &amp;= \sigma\left(\mathbf{W}_{fx} \mathbf{x}_t + \mathbf{W}_{fh}
        \mathbf{h}_{t-1} + \tanh(\mathbf{W}_{fc} \mathbf{c}_{t-1}) +
        \mathbf{b}_f\right), \\
    \mathbf{o}_t &amp;= \sigma\left(\mathbf{W}_{ox} \mathbf{x}_t + \mathbf{W}_{oh}
        \mathbf{h}_{t-1} + \tanh(\mathbf{W}_{oc} \mathbf{c}_t) + \mathbf{b}_o\right), \\
    \mathbf{c}_t &amp;= \mathbf{f}_t \circ \mathbf{c}_{t-1} + \mathbf{i}_t \circ
        \sigma_c(\mathbf{W}_{c} \mathbf{x}_t + \mathbf{b}_c), \\
    \mathbf{h}_t &amp;= \mathbf{o}_t \circ \sigma_h(\mathbf{c}_t).
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>train_memory</code>: Flag to set the initial memory state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_bias</code>: Initializer for recurrent bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_memory_bias</code>: Initializer for memory bias. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_memory_weight</code>: Initializer for memory weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>init_memory</code>: Initializer for memory. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>false</code> - Creates a hidden state using          <code>init_state</code>, hidden memory using <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>false</code> - Repeats <code>hidden_state</code> vector          from the parameters to match the shape of <code>x</code>, creates hidden memory using          <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1c: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>true</code> - Creates a hidden state using          <code>init_state</code>, repeats the memory vector from parameters to match the shape of          <code>x</code> and proceeds to Case 2.</li><li>Case 1d: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>true</code> - Repeats the hidden state and          memory vectors from the parameters to match the shape of  <code>x</code> and proceeds to          Case 2.</li><li>Case 2: Tuple <code>(x, (h, c))</code> is provided, then the output and a tuple containing the          updated hidden state and memory is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple Containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span> and new memory <span>$c_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated Weights to map from input space              <span>$\{ W_{if}, W_{ic}, W_{ii}, W_{io} \}$</span>.</li><li><code>weight_hh</code>: Concatenated weights to map from hidden space              <span>$\{ W_{hf}, W_{hc}, W_{hi}, W_{ho} \}$</span></li><li><code>weight_ph</code>: Concatenated weights to map from memory space              <span>$\{ W_{ff}, W_{fc}, W_{fi} \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Concatenated Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_ph</code>: Concatenated Bias vector for the memory-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li><li><code>memory</code>: Initial memory vector (not present if <code>train_memory=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/1205aff874e05a34e7fbe844374ccc3e6028ee8b/src/cells/wmclstm_cell.jl#L2-L118">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">« Home</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.1 on <span class="colophon-date" title="Thursday 5 June 2025 09:07">Thursday 5 June 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
