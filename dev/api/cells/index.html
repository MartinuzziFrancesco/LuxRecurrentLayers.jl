<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Cells · LuxRecurrentLayers.jl</title><meta name="title" content="Cells · LuxRecurrentLayers.jl"/><meta property="og:title" content="Cells · LuxRecurrentLayers.jl"/><meta property="twitter:title" content="Cells · LuxRecurrentLayers.jl"/><meta name="description" content="Documentation for LuxRecurrentLayers.jl."/><meta property="og:description" content="Documentation for LuxRecurrentLayers.jl."/><meta property="twitter:description" content="Documentation for LuxRecurrentLayers.jl."/><meta property="og:url" content="https://MartinuzziFrancesco.github.io/LuxRecurrentLayers.jl/api/cells/"/><meta property="twitter:url" content="https://MartinuzziFrancesco.github.io/LuxRecurrentLayers.jl/api/cells/"/><link rel="canonical" href="https://MartinuzziFrancesco.github.io/LuxRecurrentLayers.jl/api/cells/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="LuxRecurrentLayers.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">LuxRecurrentLayers.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">API Documentation</span><ul><li class="is-active"><a class="tocitem" href>Cells</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API Documentation</a></li><li class="is-active"><a href>Cells</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Cells</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/main/docs/src/api/cells.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Cells"><a class="docs-heading-anchor" href="#Cells">Cells</a><a id="Cells-1"></a><a class="docs-heading-anchor-permalink" href="#Cells" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.AntisymmetricRNNCell" href="#LuxRecurrentLayers.AntisymmetricRNNCell"><code>LuxRecurrentLayers.AntisymmetricRNNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">AntisymmetricRNNCell(in_dims =&gt; out_dims, [activation];
    use_bias=true, train_state=false, init_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32, epsilon=1.0, gamma=0.0)</code></pre><p><a href="https://arxiv.org/abs/1902.09689">Antisymmetric recurrent cell</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[h_t = h_{t-1} + \epsilon \tanh \left( (W_h - W_h^T - \gamma I) h_{t-1} + V_h x_t + b_h \right),\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li><li><code>activation</code>: activation function. Default is <code>tanh</code></li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>train_memory</code>: Flag to set the initial memory state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_context_weight</code>: Initializer for context weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>init_memory</code>: Initializer for memory. Default set to <code>zeros32</code>.</li><li><code>epsilon</code>: step size. Default is 1.0.</li><li><code>gamma</code>: strength of diffusion. Default is 0.0.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Maps the input to the hidden state.</li><li><code>weight_hh</code>: Maps the hidden state to the hidden state.</li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/45f18afd7dcd00320003a031575038ef3dea87dc/src/cells/antisymmetricrnn_cell.jl#L2-L83">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.ATRCell" href="#LuxRecurrentLayers.ATRCell"><code>LuxRecurrentLayers.ATRCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ATRCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, init_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32)</code></pre><p><a href="https://arxiv.org/abs/1810.12546">Addition-subtraction twin-gated recurrent cell</a>.</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>train_memory</code>: Flag to set the initial memory state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>init_memory</code>: Initializer for memory. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated Weights to map from input space              <span>$\{ W, W_{\theta},  W_{\eta} \}$</span>.</li><li><code>weight_hh</code>: Concatenated Weights to map from hidden space              <span>$\{ W_{\theta}, W_{\eta} \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/45f18afd7dcd00320003a031575038ef3dea87dc/src/cells/atr_cell.jl#L2-L72">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.BRCell" href="#LuxRecurrentLayers.BRCell"><code>LuxRecurrentLayers.BRCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">BRCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, init_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32)</code></pre><p><a href="https://doi.org/10.1371/journal.pone.0252676">Bistable recurrent cell</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{h}_t &amp;= \mathbf{c}_t \circ \mathbf{h}_{t-1} + (1 - \mathbf{c}_t)
        \circ \tanh\left(\mathbf{U}_x \mathbf{x}_t + \mathbf{a}_t \circ
        \mathbf{h}_{t-1}\right), \\
    \mathbf{a}_t &amp;= 1 + \tanh\left(\mathbf{U}_a \mathbf{x}_t +
        \mathbf{w}_a \circ \mathbf{h}_{t-1}\right), \\
    \mathbf{c}_t &amp;= \sigma\left(\mathbf{U}_c \mathbf{x}_t + \mathbf{w}_c \circ
        \mathbf{h}_{t-1}\right).
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated Weights to map from input space              <span>$\{ U_x, U_a, U_c \}$</span>.</li><li><code>weight_hh</code>: Concatenated Weights to map from hidden space              <span>$\{ a, w_a, w_c \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/45f18afd7dcd00320003a031575038ef3dea87dc/src/cells/br_cell.jl#L2-L81">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.CFNCell" href="#LuxRecurrentLayers.CFNCell"><code>LuxRecurrentLayers.CFNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">CFNCell(in_dims =&gt; out_dims, [activation];
    use_bias=true, train_state=false, init_bias=nothing,
    init_recurrent_bias=nothing, init_weight=nothing,
    init_recurrent_weight=nothing, init_state=zeros32,
    epsilon=1.0, gamma=0.0)</code></pre><p><a href="https://arxiv.org/abs/1612.06212">Chaos free network unit</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    h_t &amp;= \theta_t \odot \tanh(h_{t-1}) + \eta_t \odot \tanh(W x_t), \\
    \theta_t &amp;:= \sigma (U_\theta h_{t-1} + V_\theta x_t + b_\theta), \\
    \eta_t &amp;:= \sigma (U_\eta h_{t-1} + V_\eta x_t + b_\eta).
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li><li><code>activation</code>: activation function. Default is <code>tanh</code></li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>train_memory</code>: Flag to set the initial memory state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_bias</code>: Initializer for recurrent bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_context_weight</code>: Initializer for context weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>init_memory</code>: Initializer for memory. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated Weights to map from input space              <span>$\{ W, W_{\theta},  W_{\eta} \}$</span>.</li><li><code>weight_hh</code>: Concatenated Weights to map from hidden space              <span>$\{ W_{\theta}, W_{\eta} \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/45f18afd7dcd00320003a031575038ef3dea87dc/src/cells/cfn_cell.jl#L2-L93">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.coRNNCell" href="#LuxRecurrentLayers.coRNNCell"><code>LuxRecurrentLayers.coRNNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">coRNNCell(input_size =&gt; hidden_size, [dt];
    gamma=0.0, epsilon=0.0,
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    bias = true)</code></pre><p><a href="https://arxiv.org/abs/2010.00951">Coupled oscillatory recurrent neural unit</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
\mathbf{y}_n &amp;= y_{n-1} + \Delta t \mathbf{z}_n, \\
\mathbf{z}_n &amp;= z_{n-1} + \Delta t \sigma \left( \mathbf{W} y_{n-1} +
    \mathcal{W} z_{n-1} + \mathbf{V} u_n + \mathbf{b} \right) -
    \Delta t \gamma y_{n-1} - \Delta t \epsilon \mathbf{z}_n,
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer</li><li><code>dt</code>: time step. Default is 1.0.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>gamma</code>: damping for state. Default is 0.0.</li><li><code>epsilon</code>: damping for candidate state. Default is 0.0.</li><li><code>init_kernel</code>: initializer for the input to hidden weights</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights</li><li><code>bias</code>: include a bias or not. Default is <code>true</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/45f18afd7dcd00320003a031575038ef3dea87dc/src/cells/cornn_cell.jl#L2-L36">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.FastGRNNCell" href="#LuxRecurrentLayers.FastGRNNCell"><code>LuxRecurrentLayers.FastGRNNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">FastGRNNCell(input_size =&gt; hidden_size, [activation];
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    bias = true)</code></pre><p><a href="https://arxiv.org/abs/1901.02358">Fast gated recurrent neural network cell</a>.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li><li><code>activation</code>: the activation function, defaults to <code>tanh_fast</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
z_t &amp;= \sigma(W x_t + U h_{t-1} + b_z), \\
\tilde{h}_t &amp;= \tanh(W x_t + U h_{t-1} + b_h), \\
h_t &amp;= \big((\zeta (1 - z_t) + \nu) \odot \tilde{h}_t\big) + z_t \odot h_{t-1}
\end{aligned}\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/45f18afd7dcd00320003a031575038ef3dea87dc/src/cells/fastrnn_cell.jl#L95-L125">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.FastRNNCell" href="#LuxRecurrentLayers.FastRNNCell"><code>LuxRecurrentLayers.FastRNNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">FastRNNCell(input_size =&gt; hidden_size, [activation];
    init_kernel = glorot_uniform,
    init_recurrent_kernel = glorot_uniform,
    bias = true)</code></pre><p><a href="https://arxiv.org/abs/1901.02358">Fast recurrent neural network cell</a>.</p><p><strong>Arguments</strong></p><ul><li><code>input_size =&gt; hidden_size</code>: input and inner dimension of the layer.</li><li><code>activation</code>: the activation function, defaults to <code>tanh_fast</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_kernel</code>: initializer for the input to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>init_recurrent_kernel</code>: initializer for the hidden to hidden weights.   Default is <code>glorot_uniform</code>.</li><li><code>bias</code>: include a bias or not. Default is <code>true</code>.</li></ul><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
\tilde{h}_t &amp;= \sigma(W_h x_t + U_h h_{t-1} + b), \\
h_t &amp;= \alpha \tilde{h}_t + \beta h_{t-1}
\end{aligned}\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/45f18afd7dcd00320003a031575038ef3dea87dc/src/cells/fastrnn_cell.jl#L2-L31">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.IndRNNCell" href="#LuxRecurrentLayers.IndRNNCell"><code>LuxRecurrentLayers.IndRNNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">IndRNNCell(in_dims =&gt; out_dims, [activation];
    use_bias=true, train_state=false, init_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32)</code></pre><p><a href="https://arxiv.org/pdf/1803.04831">Independently recurrent cell</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\mathbf{h}_{t} = \sigma(\mathbf{W} \mathbf{x}_t + \mathbf{u} \odot \mathbf{h}_{t-1} +
    \mathbf{b})\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li><li>&#39;activation&#39;: Activation function. Defaults to <code>tanh_fast</code></li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Weights to map from input space              <span>$\{W \}$</span>.</li><li><code>weight_hh</code>: Weights to map from hidden space              <span>$\{ w_h \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/45f18afd7dcd00320003a031575038ef3dea87dc/src/cells/indrnn_cell.jl#L2-L75">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.JANETCell" href="#LuxRecurrentLayers.JANETCell"><code>LuxRecurrentLayers.JANETCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">JANETCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, train_memory=false,
    init_bias=nothing, init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32, init_memory=zeros32, beta=1.0)</code></pre><p><a href="https://arxiv.org/abs/1804.04849">Just another network unit</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{s}_t &amp;= \mathbf{U}_f \mathbf{h}_{t-1} + \mathbf{W}_f \mathbf{x}_t + \mathbf{b}_f \\
    \tilde{\mathbf{c}}_t &amp;= \tanh (\mathbf{U}_c \mathbf{h}_{t-1} + \mathbf{W}_c \mathbf{x}_t + \mathbf{b}_c) \\
    \mathbf{c}_t &amp;= \sigma(\mathbf{s}_t) \odot \mathbf{c}_{t-1} + (1 - \sigma (\mathbf{s}_t - \beta)) \odot \tilde{\mathbf{c}}_t \\
    \mathbf{h}_t &amp;= \mathbf{c}_t.
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>train_memory</code>: Flag to set the initial memory state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>init_memory</code>: Initializer for memory. Default set to <code>zeros32</code>.</li><li><code>beta</code>: Control parameter over the input data flow. Default is 1.0.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>false</code> - Creates a hidden state using          <code>init_state</code>, hidden memory using <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>false</code> - Repeats <code>hidden_state</code> vector          from the parameters to match the shape of <code>x</code>, creates hidden memory using          <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1c: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>true</code> - Creates a hidden state using          <code>init_state</code>, repeats the memory vector from parameters to match the shape of          <code>x</code> and proceeds to Case 2.</li><li>Case 1d: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>true</code> - Repeats the hidden state and          memory vectors from the parameters to match the shape of  <code>x</code> and proceeds to          Case 2.</li><li>Case 2: Tuple <code>(x, (h, c))</code> is provided, then the output and a tuple containing the          updated hidden state and memory is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple Containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span> and new memory <span>$c_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated Weights to map from input space              <span>$\{ W_{if}, W_{ic} \}$</span>.</li><li><code>weight_hh</code>: Concatenated Weights to map from hidden space              <span>$\{ W_{hf}, W_{hc} \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Concatenated Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li><li><code>memory</code>: Initial memory vector (not present if <code>train_memory=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/45f18afd7dcd00320003a031575038ef3dea87dc/src/cells/janet_cell.jl#L2-L94">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.LEMCell" href="#LuxRecurrentLayers.LEMCell"><code>LuxRecurrentLayers.LEMCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">LEMCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, train_memory=false,
    init_bias=nothing, init_weight=nothing, init_recurrent_weight=nothing,
    init_context_weight=nothing, init_state=zeros32, init_memory=zeros32)</code></pre><p><a href="https://arxiv.org/pdf/2110.04744">Long expressive memory unit</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
\boldsymbol{\Delta t_n} &amp;= \Delta \hat{t} \hat{\sigma}
    (W_1 y_{n-1} + V_1 u_n + b_1) \\
\overline{\boldsymbol{\Delta t_n}} &amp;= \Delta \hat{t}
    \hat{\sigma} (W_2 y_{n-1} + V_2 u_n + b_2) \\
z_n &amp;= (1 - \boldsymbol{\Delta t_n}) \odot z_{n-1} +
    \boldsymbol{\Delta t_n} \odot \sigma (W_z y_{n-1} + V_z u_n + b_z) \\
y_n &amp;= (1 - \boldsymbol{\Delta t_n}) \odot y_{n-1} +
    \boldsymbol{\Delta t_n} \odot \sigma (W_y z_n + V_y u_n + b_y)
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>train_memory</code>: Flag to set the initial memory state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_context_weight</code>: Initializer for context weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>init_memory</code>: Initializer for memory. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>false</code> - Creates a hidden state using          <code>init_state</code>, hidden memory using <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>false</code> - Repeats <code>hidden_state</code> vector          from the parameters to match the shape of <code>x</code>, creates hidden memory using          <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1c: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>true</code> - Creates a hidden state using          <code>init_state</code>, repeats the memory vector from parameters to match the shape of          <code>x</code> and proceeds to Case 2.</li><li>Case 1d: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>true</code> - Repeats the hidden state and          memory vectors from the parameters to match the shape of  <code>x</code> and proceeds to          Case 2.</li><li>Case 2: Tuple <code>(x, (h, c))</code> is provided, then the output and a tuple containing the          updated hidden state and memory is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple Containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span> and new memory <span>$c_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated Weights to map from input space              <span>$\{ W_{if}, W_{ic} \}$</span>.</li><li><code>weight_hh</code>: Concatenated Weights to map from hidden space              <span>$\{ W_{hf}, W_{hc} \}$</span></li><li><code>weight_hh</code>: Concatenated Weights to map from context space              <span>$\{ W_{cf}, W_{cc} \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Concatenated Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>alpha</code>: Initial context strength.</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li><li><code>memory</code>: Initial memory vector (not present if <code>train_memory=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/45f18afd7dcd00320003a031575038ef3dea87dc/src/cells/lem_cell.jl#L3-L104">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.LightRUCell" href="#LuxRecurrentLayers.LightRUCell"><code>LuxRecurrentLayers.LightRUCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">LightRUCell(in_dims =&gt; out_dims, [activation];
    use_bias=true, train_state=false, init_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32)</code></pre><p><a href="https://www.mdpi.com/2079-9292/13/16/3204">Light recurrent unit</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \tilde{h}_t &amp;= \tanh(W_h x_t), \\
    f_t         &amp;= \delta(W_f x_t + U_f h_{t-1} + b_f), \\
    h_t         &amp;= (1 - f_t) \odot h_{t-1} + f_t \odot \tilde{h}_t.
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Weights to map from input space              <span>$\{W \}$</span>.</li><li><code>weight_hh</code>: Weights to map from hidden space              <span>$\{ w_h \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/45f18afd7dcd00320003a031575038ef3dea87dc/src/cells/lightru_cell.jl#L2-L77">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.LiGRUCell" href="#LuxRecurrentLayers.LiGRUCell"><code>LuxRecurrentLayers.LiGRUCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">LiGRUCell(in_dims =&gt; out_dims, [activation];
    use_bias=true, train_state=false, init_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32)</code></pre><p><a href="https://arxiv.org/pdf/1803.10225">Light gated recurrent unit</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    z_t &amp;= \sigma(W_z x_t + U_z h_{t-1}), \\
    \tilde{h}_t &amp;= \text{ReLU}(W_h x_t + U_h h_{t-1}), \\
    h_t &amp;= z_t \odot h_{t-1} + (1 - z_t) \odot \tilde{h}_t
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Weights to map from input space              <span>$\{W \}$</span>.</li><li><code>weight_hh</code>: Weights to map from hidden space              <span>$\{ w_h \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/45f18afd7dcd00320003a031575038ef3dea87dc/src/cells/ligru_cell.jl#L2-L77">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.MGUCell" href="#LuxRecurrentLayers.MGUCell"><code>LuxRecurrentLayers.MGUCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MGUCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, init_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32)</code></pre><p><a href="https://arxiv.org/pdf/1603.09420">Minimal gated unit</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    f_t         &amp;= \sigma(W_f x_t + U_f h_{t-1} + b_f), \\
    \tilde{h}_t &amp;= \tanh(W_h x_t + U_h (f_t \odot h_{t-1}) + b_h), \\
    h_t         &amp;= (1 - f_t) \odot h_{t-1} + f_t \odot \tilde{h}_t
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Weights to map from input space              <span>$\{W \}$</span>.</li><li><code>weight_hh</code>: Weights to map from hidden space              <span>$\{ w_h \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/45f18afd7dcd00320003a031575038ef3dea87dc/src/cells/mgu_cell.jl#L2-L77">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.MinimalRNNCell" href="#LuxRecurrentLayers.MinimalRNNCell"><code>LuxRecurrentLayers.MinimalRNNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MinimalRNNCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, init_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32,)</code></pre><p><a href="https://arxiv.org/abs/1711.06788">Minimal recurrent neural network unit</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{z}_t &amp;= \Phi(\mathbf{x}_t) = \tanh(\mathbf{W}_x \mathbf{x}_t +
        \mathbf{b}_z), \\
    \mathbf{u}_t &amp;= \sigma(\mathbf{U}_h \mathbf{h}_{t-1} + \mathbf{U}_z \mathbf{z}_t +
        \mathbf{b}_u), \\
    \mathbf{h}_t &amp;= \mathbf{u}_t \circ \mathbf{h}_{t-1} + (1 - \mathbf{u}_t) \circ
        \mathbf{z}_t.
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>use_bias</code>: Set to false to deactivate bias</li><li><code>train_state</code>: Trainable initial hidden state can be activated by setting this to <code>true</code></li><li><code>train_memory</code>: Trainable initial memory can be activated by setting this to <code>true</code></li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>.</li><li><code>init_state</code>: Initializer for hidden state</li><li><code>init_memory</code>: Initializer for memory</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>false</code> - Creates a hidden state using          <code>init_state</code>, hidden memory using <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>false</code> - Repeats <code>hidden_state</code> vector          from the parameters to match the shape of <code>x</code>, creates hidden memory using          <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1c: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>true</code> - Creates a hidden state using          <code>init_state</code>, repeats the memory vector from parameters to match the shape of          <code>x</code> and proceeds to Case 2.</li><li>Case 1d: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>true</code> - Repeats the hidden state and          memory vectors from the parameters to match the shape of  <code>x</code> and proceeds to          Case 2.</li><li>Case 2: Tuple <code>(x, (h, c))</code> is provided, then the output and a tuple containing the          updated hidden state and memory is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple Containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span> and new memory <span>$c_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Weights to map from input space.</li><li><code>weight_hh</code>: Weights to map from hidden space.</li><li><code>weight_mm</code>: Weights to map from memory space.</li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>).</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>).</li><li><code>bias_hh</code>: Bias vector for the mnemrory-memory connection (not present if <code>use_bias=false</code>).</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>).</li><li><code>memory</code>: Initial memory vector (not present if <code>train_memory=false</code>).</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/45f18afd7dcd00320003a031575038ef3dea87dc/src/cells/minimalrnn_cell.jl#L2-L93">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.MUT1Cell" href="#LuxRecurrentLayers.MUT1Cell"><code>LuxRecurrentLayers.MUT1Cell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MUT1Cell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, init_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32)</code></pre><p><a href="https://proceedings.mlr.press/v37/jozefowicz15.pdf">Mutated unit 1 cell</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    z &amp;= \sigma(W_z x_t + b_z), \\
    r &amp;= \sigma(W_r x_t + U_r h_t + b_r), \\
    h_{t+1} &amp;= \tanh(U_h (r \odot h_t) + \tanh(W_h x_t) + b_h) \odot z \\
        &amp;\quad + h_t \odot (1 - z).
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Weights to map from input space              <span>$\{W \}$</span>.</li><li><code>weight_hh</code>: Weights to map from hidden space              <span>$\{ w_h \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/45f18afd7dcd00320003a031575038ef3dea87dc/src/cells/mut_cell.jl#L2-L78">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.MUT2Cell" href="#LuxRecurrentLayers.MUT2Cell"><code>LuxRecurrentLayers.MUT2Cell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MUT2Cell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, init_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32)</code></pre><p><a href="https://proceedings.mlr.press/v37/jozefowicz15.pdf">Mutated unit 2 cell</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    z &amp;= \sigma(W_z x_t + U_z h_t + b_z), \\
    r &amp;= \sigma(x_t + U_r h_t + b_r), \\
    h_{t+1} &amp;= \tanh(U_h (r \odot h_t) + W_h x_t + b_h) \odot z \\
        &amp;\quad + h_t \odot (1 - z).
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Weights to map from input space              <span>$\{W \}$</span>.</li><li><code>weight_hh</code>: Weights to map from hidden space              <span>$\{ w_h \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/45f18afd7dcd00320003a031575038ef3dea87dc/src/cells/mut_cell.jl#L148-L224">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.MUT3Cell" href="#LuxRecurrentLayers.MUT3Cell"><code>LuxRecurrentLayers.MUT3Cell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MUT3Cell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, init_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32)</code></pre><p><a href="https://proceedings.mlr.press/v37/jozefowicz15.pdf">Mutated unit 2 cell</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    z &amp;= \sigma(W_z x_t + U_z h_t + b_z), \\
    r &amp;= \sigma(x_t + U_r h_t + b_r), \\
    h_{t+1} &amp;= \tanh(U_h (r \odot h_t) + W_h x_t + b_h) \odot z \\
        &amp;\quad + h_t \odot (1 - z).
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Weights to map from input space              <span>$\{W \}$</span>.</li><li><code>weight_hh</code>: Weights to map from hidden space              <span>$\{ w_h \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/45f18afd7dcd00320003a031575038ef3dea87dc/src/cells/mut_cell.jl#L293-L369">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.NASCell" href="#LuxRecurrentLayers.NASCell"><code>LuxRecurrentLayers.NASCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">NASCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, train_memory=false,
    init_bias=nothing, init_recurrent_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32, init_memory=zeros32)</code></pre><p><a href="https://arxiv.org/pdf/1611.01578">Neural Architecture Search unit</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
\text{First Layer Outputs:} &amp; \\
o_1 &amp;= \sigma(W_i^{(1)} x_t + W_h^{(1)} h_{t-1} + b^{(1)}), \\
o_2 &amp;= \text{ReLU}(W_i^{(2)} x_t + W_h^{(2)} h_{t-1} + b^{(2)}), \\
o_3 &amp;= \sigma(W_i^{(3)} x_t + W_h^{(3)} h_{t-1} + b^{(3)}), \\
o_4 &amp;= \text{ReLU}(W_i^{(4)} x_t \cdot W_h^{(4)} h_{t-1}), \\
o_5 &amp;= \tanh(W_i^{(5)} x_t + W_h^{(5)} h_{t-1} + b^{(5)}), \\
o_6 &amp;= \sigma(W_i^{(6)} x_t + W_h^{(6)} h_{t-1} + b^{(6)}), \\
o_7 &amp;= \tanh(W_i^{(7)} x_t + W_h^{(7)} h_{t-1} + b^{(7)}), \\
o_8 &amp;= \sigma(W_i^{(8)} x_t + W_h^{(8)} h_{t-1} + b^{(8)}). \\

\text{Second Layer Computations:} &amp; \\
l_1 &amp;= \tanh(o_1 \cdot o_2) \\
l_2 &amp;= \tanh(o_3 + o_4) \\
l_3 &amp;= \tanh(o_5 \cdot o_6) \\
l_4 &amp;= \sigma(o_7 + o_8) \\

\text{Inject Cell State:} &amp; \\
l_1 &amp;= \tanh(l_1 + c_{\text{state}}) \\

\text{Final Layer Computations:} &amp; \\
c_{\text{new}} &amp;= l_1 \cdot l_2 \\
l_5 &amp;= \tanh(l_3 + l_4) \\
h_{\text{new}} &amp;= \tanh(c_{\text{new}} \cdot l_5)
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>train_memory</code>: Flag to set the initial memory state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_bias</code>: Initializer for recurrent bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>init_memory</code>: Initializer for memory. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>false</code> - Creates a hidden state using          <code>init_state</code>, hidden memory using <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>false</code> - Repeats <code>hidden_state</code> vector          from the parameters to match the shape of <code>x</code>, creates hidden memory using          <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1c: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>true</code> - Creates a hidden state using          <code>init_state</code>, repeats the memory vector from parameters to match the shape of          <code>x</code> and proceeds to Case 2.</li><li>Case 1d: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>true</code> - Repeats the hidden state and          memory vectors from the parameters to match the shape of  <code>x</code> and proceeds to          Case 2.</li><li>Case 2: Tuple <code>(x, (h, c))</code> is provided, then the output and a tuple containing the          updated hidden state and memory is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple Containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span> and new memory <span>$c_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated Weights to map from input space              <span>$\{ W_{if}, W_{ic}, W_{ii}, W_{io} \}$</span>.</li><li><code>weight_hh</code>: Concatenated weights to map from hidden space              <span>$\{ W_{hf}, W_{hc}, W_{hi}, W_{ho} \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Concatenated Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li><li><code>memory</code>: Initial memory vector (not present if <code>train_memory=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/45f18afd7dcd00320003a031575038ef3dea87dc/src/cells/nas_cell.jl#L2-L116">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.PeepholeLSTMCell" href="#LuxRecurrentLayers.PeepholeLSTMCell"><code>LuxRecurrentLayers.PeepholeLSTMCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">PeepholeLSTMCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, train_memory=false,
    init_bias=nothing, init_recurrent_bias=nothing, init_peephole_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_peephole_weight=nothing, init_state=zeros32, init_memory=zeros32)</code></pre><p><a href="https://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf">Peephole long short term memory</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    z_t &amp;= \tanh(W_z x_t + U_z h_{t-1} + b_z), \\
    i_t &amp;= \sigma(W_i x_t + U_i h_{t-1} + p_i \odot c_{t-1} + b_i), \\
    f_t &amp;= \sigma(W_f x_t + U_f h_{t-1} + p_f \odot c_{t-1} + b_f), \\
    c_t &amp;= f_t \odot c_{t-1} + i_t \odot z_t, \\
    o_t &amp;= \sigma(W_o x_t + U_o h_{t-1} + p_o \odot c_t + b_o), \\
    h_t &amp;= o_t \odot \tanh(c_t).
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>train_memory</code>: Flag to set the initial memory state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_bias</code>: Initializer for recurrent bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_peephole_bias</code>: Initializer for peephole bias. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_peephole_weight</code>: Initializer for peephole weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>init_memory</code>: Initializer for memory. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>false</code> - Creates a hidden state using          <code>init_state</code>, hidden memory using <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>false</code> - Repeats <code>hidden_state</code> vector          from the parameters to match the shape of <code>x</code>, creates hidden memory using          <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1c: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>true</code> - Creates a hidden state using          <code>init_state</code>, repeats the memory vector from parameters to match the shape of          <code>x</code> and proceeds to Case 2.</li><li>Case 1d: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>true</code> - Repeats the hidden state and          memory vectors from the parameters to match the shape of  <code>x</code> and proceeds to          Case 2.</li><li>Case 2: Tuple <code>(x, (h, c))</code> is provided, then the output and a tuple containing the          updated hidden state and memory is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple Containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span> and new memory <span>$c_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated Weights to map from input space              <span>$\{ W_{if}, W_{ic}, W_{ii}, W_{io} \}$</span>.</li><li><code>weight_hh</code>: Concatenated weights to map from hidden space              <span>$\{ W_{hf}, W_{hc}, W_{hi}, W_{ho} \}$</span></li><li><code>weight_ph</code>: Concatenated weights to map from peephole space              <span>$\{ W_{ff}, W_{fc}, W_{fi} \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Concatenated Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_ph</code>: Concatenated Bias vector for the peephole-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li><li><code>memory</code>: Initial memory vector (not present if <code>train_memory=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/45f18afd7dcd00320003a031575038ef3dea87dc/src/cells/peepholelstm_cell.jl#L2-L111">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.SCRNCell" href="#LuxRecurrentLayers.SCRNCell"><code>LuxRecurrentLayers.SCRNCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SCRNCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, train_memory=false,
    init_bias=nothing, init_weight=nothing, init_recurrent_weight=nothing,
    init_context_weight=nothing, init_state=zeros32, init_memory=zeros32)</code></pre><p><a href="https://arxiv.org/pdf/1412.7753">Structurally contraint recurrent unit</a>.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
s_t &amp;= (1 - \alpha) W_s x_t + \alpha s_{t-1}, \\
h_t &amp;= \sigma(W_h s_t + U_h h_{t-1} + b_h), \\
y_t &amp;= f(U_y h_t + W_y s_t)
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>train_memory</code>: Flag to set the initial memory state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_context_weight</code>: Initializer for context weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>init_memory</code>: Initializer for memory. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>false</code> - Creates a hidden state using          <code>init_state</code>, hidden memory using <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>false</code> - Repeats <code>hidden_state</code> vector          from the parameters to match the shape of <code>x</code>, creates hidden memory using          <code>init_memory</code> and proceeds to Case 2.</li><li>Case 1c: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code>, <code>train_memory</code> is set to <code>true</code> - Creates a hidden state using          <code>init_state</code>, repeats the memory vector from parameters to match the shape of          <code>x</code> and proceeds to Case 2.</li><li>Case 1d: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code>, <code>train_memory</code> is set to <code>true</code> - Repeats the hidden state and          memory vectors from the parameters to match the shape of  <code>x</code> and proceeds to          Case 2.</li><li>Case 2: Tuple <code>(x, (h, c))</code> is provided, then the output and a tuple containing the          updated hidden state and memory is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple Containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span> and new memory <span>$c_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated Weights to map from input space              <span>$\{ W_{if}, W_{ic} \}$</span>.</li><li><code>weight_hh</code>: Concatenated Weights to map from hidden space              <span>$\{ W_{hf}, W_{hc} \}$</span></li><li><code>weight_hh</code>: Concatenated Weights to map from context space              <span>$\{ W_{cf}, W_{cc} \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Concatenated Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>alpha</code>: Initial context strength.</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li><li><code>memory</code>: Initial memory vector (not present if <code>train_memory=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/45f18afd7dcd00320003a031575038ef3dea87dc/src/cells/scrn_cell.jl#L3-L99">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxRecurrentLayers.STARCell" href="#LuxRecurrentLayers.STARCell"><code>LuxRecurrentLayers.STARCell</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">STARCell(in_dims =&gt; out_dims;
    use_bias=true, train_state=false, init_bias=nothing,
    init_weight=nothing, init_recurrent_weight=nothing,
    init_state=zeros32)</code></pre><p><a href="https://arxiv.org/abs/1911.11033">Stackable recurrent cell</a>.</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input Dimension</li><li><code>out_dims</code>: Output (Hidden State &amp; Memory) Dimension</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>use_bias</code>: Flag to use bias in the computation. Default set to <code>true</code>.</li><li><code>train_state</code>: Flag to set the initial hidden state as trainable. Default set to <code>false</code>.</li><li><code>train_memory</code>: Flag to set the initial memory state as trainable. Default set to <code>false</code>.</li><li><code>init_bias</code>: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_weight</code>: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_recurrent_weight</code>: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If <code>nothing</code>, then we use uniform distribution with bounds <code>-bound</code> and <code>bound</code> where <code>bound = inv(sqrt(out_dims))</code>. Default set to <code>nothing</code>.</li><li><code>init_state</code>: Initializer for hidden state. Default set to <code>zeros32</code>.</li><li><code>init_memory</code>: Initializer for memory. Default set to <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li>Case 1a: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>false</code> - Creates a hidden state using <code>init_state</code> and proceeds to Case 2.</li><li>Case 1b: Only a single input <code>x</code> of shape <code>(in_dims, batch_size)</code>, <code>train_state</code> is set          to <code>true</code> - Repeats <code>hidden_state</code> from parameters to match the shape of <code>x</code>          and proceeds to Case 2.</li><li>Case 2: Tuple <code>(x, (h, ))</code> is provided, then the output and a tuple containing the         updated hidden state is returned.</li></ul><p><strong>Returns</strong></p><ul><li><p>Tuple containing</p><ul><li>Output <span>$h_{new}$</span> of shape <code>(out_dims, batch_size)</code></li><li>Tuple containing new hidden state <span>$h_{new}$</span></li></ul></li><li><p>Updated model state</p></li></ul><p><strong>Parameters</strong></p><ul><li><code>weight_ih</code>: Concatenated Weights to map from input space              <span>$\{ W, W_{\theta},  W_{\eta} \}$</span>.</li><li><code>weight_hh</code>: Concatenated Weights to map from hidden space              <span>$\{ W_{\theta}, W_{\eta} \}$</span></li><li><code>bias_ih</code>: Bias vector for the input-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>bias_hh</code>: Bias vector for the hidden-hidden connection (not present if <code>use_bias=false</code>)</li><li><code>hidden_state</code>: Initial hidden state vector (not present if <code>train_state=false</code>)</li></ul><p><strong>States</strong></p><ul><li><code>rng</code>: Controls the randomness (if any) in the initial state generation</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/MartinuzziFrancesco/LuxRecurrentLayers.jl/blob/45f18afd7dcd00320003a031575038ef3dea87dc/src/cells/star_cell.jl#L2-L72">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">« Home</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.1 on <span class="colophon-date" title="Monday 26 May 2025 08:14">Monday 26 May 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
