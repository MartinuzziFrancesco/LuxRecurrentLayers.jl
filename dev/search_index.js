var documenterSearchIndex = {"docs":
[{"location":"api/cells/#Cells","page":"Cells","title":"Cells","text":"","category":"section"},{"location":"api/cells/","page":"Cells","title":"Cells","text":"    AntisymmetricRNNCell\n    ATRCell\n    BRCell\n    CFNCell\n    coRNNCell\n    FastGRNNCell\n    FastRNNCell\n    IndRNNCell\n    JANETCell\n    LEMCell\n    LightRUCell\n    LiGRUCell\n    MinimalRNNCell\n    PeepholeLSTMCell\n    SCRNCell\n    STARCell","category":"page"},{"location":"api/cells/#LuxRecurrentLayers.AntisymmetricRNNCell","page":"Cells","title":"LuxRecurrentLayers.AntisymmetricRNNCell","text":"AntisymmetricRNNCell(in_dims => out_dims, [activation];\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32, epsilon=1.0, gamma=0.0)\n\nAntisymmetric recurrent cell.\n\nEquations\n\nh_t = h_t-1 + epsilon tanh left( (W_h - W_h^T - gamma I) h_t-1 + V_h x_t + b_h right)\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\nactivation: activation function. Default is tanh\n\nKeyword arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_context_weight: Initializer for context weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\nepsilon: step size. Default is 1.0.\ngamma: strength of diffusion. Default is 0.0.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Maps the input to the hidden state.\nweight_hh: Maps the hidden state to the hidden state.\nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.ATRCell","page":"Cells","title":"LuxRecurrentLayers.ATRCell","text":"ATRCell(in_dims => out_dims;\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nAddition-subtraction twin-gated recurrent cell.\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W W_theta  W_eta .\nweight_hh: Concatenated Weights to map from hidden space               W_theta W_eta \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.BRCell","page":"Cells","title":"LuxRecurrentLayers.BRCell","text":"BRCell(in_dims => out_dims;\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nBistable recurrent cell.\n\nEquations\n\nbeginaligned\n    mathbfh_t = mathbfc_t circ mathbfh_t-1 + (1 - mathbfc_t)\n        circ tanhleft(mathbfU_x mathbfx_t + mathbfa_t circ\n        mathbfh_t-1right) \n    mathbfa_t = 1 + tanhleft(mathbfU_a mathbfx_t +\n        mathbfw_a circ mathbfh_t-1right) \n    mathbfc_t = sigmaleft(mathbfU_c mathbfx_t + mathbfw_c circ\n        mathbfh_t-1right)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               U_x U_a U_c .\nweight_hh: Concatenated Weights to map from hidden space               a w_a w_c \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.CFNCell","page":"Cells","title":"LuxRecurrentLayers.CFNCell","text":"CFNCell(in_dims => out_dims, [activation];\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_recurrent_bias=nothing, init_weight=nothing,\n    init_recurrent_weight=nothing, init_state=zeros32,\n    epsilon=1.0, gamma=0.0)\n\nChaos free network unit.\n\nEquations\n\nbeginaligned\n    h_t = theta_t odot tanh(h_t-1) + eta_t odot tanh(W x_t) \n    theta_t = sigma (U_theta h_t-1 + V_theta x_t + b_theta) \n    eta_t = sigma (U_eta h_t-1 + V_eta x_t + b_eta)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\nactivation: activation function. Default is tanh\n\nKeyword arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_bias: Initializer for recurrent bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_context_weight: Initializer for context weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W W_theta  W_eta .\nweight_hh: Concatenated Weights to map from hidden space               W_theta W_eta \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.coRNNCell","page":"Cells","title":"LuxRecurrentLayers.coRNNCell","text":"coRNNCell(input_size => hidden_size, [dt];\n    gamma=0.0, epsilon=0.0,\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nCoupled oscillatory recurrent neural unit.\n\nEquations\n\nbeginaligned\nmathbfy_n = y_n-1 + Delta t mathbfz_n \nmathbfz_n = z_n-1 + Delta t sigma left( mathbfW y_n-1 +\n    mathcalW z_n-1 + mathbfV u_n + mathbfb right) -\n    Delta t gamma y_n-1 - Delta t epsilon mathbfz_n\nendaligned\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer\ndt: time step. Default is 1.0.\n\nKeyword arguments\n\ngamma: damping for state. Default is 0.0.\nepsilon: damping for candidate state. Default is 0.0.\ninit_kernel: initializer for the input to hidden weights\ninit_recurrent_kernel: initializer for the hidden to hidden weights\nbias: include a bias or not. Default is true\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.FastGRNNCell","page":"Cells","title":"LuxRecurrentLayers.FastGRNNCell","text":"FastGRNNCell(input_size => hidden_size, [activation];\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nFast gated recurrent neural network cell.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\nactivation: the activation function, defaults to tanh_fast.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\nz_t = sigma(W x_t + U h_t-1 + b_z) \ntildeh_t = tanh(W x_t + U h_t-1 + b_h) \nh_t = big((zeta (1 - z_t) + nu) odot tildeh_tbig) + z_t odot h_t-1\nendaligned\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.FastRNNCell","page":"Cells","title":"LuxRecurrentLayers.FastRNNCell","text":"FastRNNCell(input_size => hidden_size, [activation];\n    init_kernel = glorot_uniform,\n    init_recurrent_kernel = glorot_uniform,\n    bias = true)\n\nFast recurrent neural network cell.\n\nArguments\n\ninput_size => hidden_size: input and inner dimension of the layer.\nactivation: the activation function, defaults to tanh_fast.\n\nKeyword arguments\n\ninit_kernel: initializer for the input to hidden weights.   Default is glorot_uniform.\ninit_recurrent_kernel: initializer for the hidden to hidden weights.   Default is glorot_uniform.\nbias: include a bias or not. Default is true.\n\nEquations\n\nbeginaligned\ntildeh_t = sigma(W_h x_t + U_h h_t-1 + b) \nh_t = alpha tildeh_t + beta h_t-1\nendaligned\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.IndRNNCell","page":"Cells","title":"LuxRecurrentLayers.IndRNNCell","text":"IndRNNCell(in_dims => out_dims, [activation];\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nIndependently recurrent cell.\n\nEquations\n\nmathbfh_t = sigma(mathbfW mathbfx_t + mathbfu odot mathbfh_t-1 +\n    mathbfb)\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n'activation': Activation function. Defaults to tanh_fast\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Weights to map from input space              W .\nweight_hh: Weights to map from hidden space               w_h \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.JANETCell","page":"Cells","title":"LuxRecurrentLayers.JANETCell","text":"JANETCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32, init_memory=zeros32, beta=1.0)\n\nJust another network unit.\n\nEquations\n\nbeginaligned\n    mathbfs_t = mathbfU_f mathbfh_t-1 + mathbfW_f mathbfx_t + mathbfb_f \n    tildemathbfc_t = tanh (mathbfU_c mathbfh_t-1 + mathbfW_c mathbfx_t + mathbfb_c) \n    mathbfc_t = sigma(mathbfs_t) odot mathbfc_t-1 + (1 - sigma (mathbfs_t - beta)) odot tildemathbfc_t \n    mathbfh_t = mathbfc_t\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\nbeta: Control parameter over the input data flow. Default is 1.0.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W_if W_ic .\nweight_hh: Concatenated Weights to map from hidden space               W_hf W_hc \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Concatenated Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.LEMCell","page":"Cells","title":"LuxRecurrentLayers.LEMCell","text":"LEMCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_weight=nothing, init_recurrent_weight=nothing,\n    init_context_weight=nothing, init_state=zeros32, init_memory=zeros32)\n\nLong expressive memory unit.\n\nEquations\n\nbeginaligned\nboldsymbolDelta t_n = Delta hatt hatsigma\n    (W_1 y_n-1 + V_1 u_n + b_1) \noverlineboldsymbolDelta t_n = Delta hatt\n    hatsigma (W_2 y_n-1 + V_2 u_n + b_2) \nz_n = (1 - boldsymbolDelta t_n) odot z_n-1 +\n    boldsymbolDelta t_n odot sigma (W_z y_n-1 + V_z u_n + b_z) \ny_n = (1 - boldsymbolDelta t_n) odot y_n-1 +\n    boldsymbolDelta t_n odot sigma (W_y z_n + V_y u_n + b_y)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_context_weight: Initializer for context weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W_if W_ic .\nweight_hh: Concatenated Weights to map from hidden space               W_hf W_hc \nweight_hh: Concatenated Weights to map from context space               W_cf W_cc \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Concatenated Bias vector for the hidden-hidden connection (not present if use_bias=false)\nalpha: Initial context strength.\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.LightRUCell","page":"Cells","title":"LuxRecurrentLayers.LightRUCell","text":"LightRUCell(in_dims => out_dims, [activation];\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nLight recurrent unit.\n\nEquations\n\nbeginaligned\n    tildeh_t = tanh(W_h x_t) \n    f_t         = delta(W_f x_t + U_f h_t-1 + b_f) \n    h_t         = (1 - f_t) odot h_t-1 + f_t odot tildeh_t\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Weights to map from input space              W .\nweight_hh: Weights to map from hidden space               w_h \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.LiGRUCell","page":"Cells","title":"LuxRecurrentLayers.LiGRUCell","text":"LiGRUCell(in_dims => out_dims, [activation];\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nLight gated recurrent unit.\n\nEquations\n\nbeginaligned\n    z_t = sigma(W_z x_t + U_z h_t-1) \n    tildeh_t = textReLU(W_h x_t + U_h h_t-1) \n    h_t = z_t odot h_t-1 + (1 - z_t) odot tildeh_t\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Weights to map from input space              W .\nweight_hh: Weights to map from hidden space               w_h \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.MinimalRNNCell","page":"Cells","title":"LuxRecurrentLayers.MinimalRNNCell","text":"MinimalRNNCell(in_dims => out_dims;\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32,)\n\nMinimal recurrent neural network unit.\n\nEquations\n\nbeginaligned\n    mathbfz_t = Phi(mathbfx_t) = tanh(mathbfW_x mathbfx_t +\n        mathbfb_z) \n    mathbfu_t = sigma(mathbfU_h mathbfh_t-1 + mathbfU_z mathbfz_t +\n        mathbfb_u) \n    mathbfh_t = mathbfu_t circ mathbfh_t-1 + (1 - mathbfu_t) circ\n        mathbfz_t\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword arguments\n\nuse_bias: Set to false to deactivate bias\ntrain_state: Trainable initial hidden state can be activated by setting this to true\ntrain_memory: Trainable initial memory can be activated by setting this to true\ninit_bias: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)).\ninit_weight: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)).\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)).\ninit_state: Initializer for hidden state\ninit_memory: Initializer for memory\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Weights to map from input space.\nweight_hh: Weights to map from hidden space.\nweight_mm: Weights to map from memory space.\nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false).\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false).\nbias_hh: Bias vector for the mnemrory-memory connection (not present if use_bias=false).\nhidden_state: Initial hidden state vector (not present if train_state=false).\nmemory: Initial memory vector (not present if train_memory=false).\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.PeepholeLSTMCell","page":"Cells","title":"LuxRecurrentLayers.PeepholeLSTMCell","text":"PeepholeLSTMCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing, init_peephole_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_peephole_weight=nothing, init_state=zeros32, init_memory=zeros32)\n\nPeephole long short term memory.\n\nEquations\n\nbeginaligned\n    z_t = tanh(W_z x_t + U_z h_t-1 + b_z) \n    i_t = sigma(W_i x_t + U_i h_t-1 + p_i odot c_t-1 + b_i) \n    f_t = sigma(W_f x_t + U_f h_t-1 + p_f odot c_t-1 + b_f) \n    c_t = f_t odot c_t-1 + i_t odot z_t \n    o_t = sigma(W_o x_t + U_o h_t-1 + p_o odot c_t + b_o) \n    h_t = o_t odot tanh(c_t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_bias: Initializer for recurrent bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_peephole_bias: Initializer for peephole bias. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_peephole_weight: Initializer for peephole weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W_if W_ic W_ii W_io .\nweight_hh: Concatenated weights to map from hidden space               W_hf W_hc W_hi W_ho \nweight_ph: Concatenated weights to map from peephole space               W_ff W_fc W_fi \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Concatenated Bias vector for the hidden-hidden connection (not present if use_bias=false)\nbias_ph: Concatenated Bias vector for the peephole-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.SCRNCell","page":"Cells","title":"LuxRecurrentLayers.SCRNCell","text":"SCRNCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_weight=nothing, init_recurrent_weight=nothing,\n    init_context_weight=nothing, init_state=zeros32, init_memory=zeros32)\n\nStructurally contraint recurrent unit.\n\nEquations\n\nbeginaligned\ns_t = (1 - alpha) W_s x_t + alpha s_t-1 \nh_t = sigma(W_h s_t + U_h h_t-1 + b_h) \ny_t = f(U_y h_t + W_y s_t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_context_weight: Initializer for context weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W_if W_ic .\nweight_hh: Concatenated Weights to map from hidden space               W_hf W_hc \nweight_hh: Concatenated Weights to map from context space               W_cf W_cc \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Concatenated Bias vector for the hidden-hidden connection (not present if use_bias=false)\nalpha: Initial context strength.\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.STARCell","page":"Cells","title":"LuxRecurrentLayers.STARCell","text":"STARCell(in_dims => out_dims;\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nStackable recurrent cell.\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W W_theta  W_eta .\nweight_hh: Concatenated Weights to map from hidden space               W_theta W_eta \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = LuxRecurrentLayers","category":"page"},{"location":"#LuxRecurrentLayers","page":"Home","title":"LuxRecurrentLayers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for LuxRecurrentLayers.","category":"page"}]
}
