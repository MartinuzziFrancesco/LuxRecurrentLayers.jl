var documenterSearchIndex = {"docs":
[{"location":"api/cells/mgucell/#MGUCell","page":"MGUCell","title":"MGUCell","text":"","category":"section"},{"location":"api/cells/mgucell/","page":"MGUCell","title":"MGUCell","text":"    MGUCell","category":"page"},{"location":"api/cells/mgucell/#LuxRecurrentLayers.MGUCell","page":"MGUCell","title":"LuxRecurrentLayers.MGUCell","text":"MGUCell(in_dims => out_dims;\n    use_bias=true, train_state=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nMinimal gated unit.\n\nEquations\n\nbeginaligned\n    mathbff(t) = sigmaleft( \n        mathbfW_ih^f mathbfx(t) + mathbfb_ih^f + \n        mathbfW_hh^f mathbfh(t-1) + mathbfb_hh^f right) \n    tildemathbfh(t) = tanhleft( \n        mathbfW_ih^h mathbfx(t) + mathbfb_ih^h + \n        mathbfW_hh^h left( mathbff(t) circ mathbfh(t-1) right) +\n        mathbfb_hh^h right) \n    mathbfh(t) = left(1 - mathbff(t)right) circ mathbfh(t-1) +\n        mathbff(t) circ tildemathbfh(t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for input-to-hidden biases   mathbfb_ih^f mathbfb_ih^h.   Must be a tuple of 2 functions. If a single function is provided, it is expanded to 2 copies. If set to nothing, biases are initialized from a uniform distribution in [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden-to-hidden biases   mathbfb_hh^f mathbfb_hh^h.   Must be a tuple of 2 functions. If a single function is provided, it is expanded to 2 copies. If set to nothing, biases are initialized from a uniform distribution in [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input-to-hidden weights   mathbfW_ih^f mathbfW_ih^h.   Must be a tuple of 2 functions. If a single function is provided, it is expanded to 2 copies. If set to nothing, weights are initialized from a uniform distribution in [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for hidden-to-hidden weights   mathbfW_hh^f mathbfW_hh^h.   Must be a tuple of 2 functions. If a single function is provided, it is expanded to 2 copies. If set to nothing, weights are initialized from a uniform distribution in [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Input-to-hidden weights    mathbfW_ih^f mathbfW_ih^h    The functions from init_weight are applied in order:   the first initializes mathbfW_ih^f, the second mathbfW_ih^h.\nweight_hh: Hidden-to-hidden weights    mathbfW_hh^f mathbfW_hh^h    The functions from init_recurrent_weight are applied in order:   the first initializes mathbfW_hh^f, the second mathbfW_hh^h.\nbias_ih: Input-to-hidden biases (if use_bias=true)    mathbfb_ih^f mathbfb_ih^h    The functions from init_bias are applied in order:   the first initializes mathbfb_ih^f, the second mathbfb_ih^h.\nbias_hh: Hidden-to-hidden biases (if use_bias=true)    mathbfb_hh^f mathbfb_hh^h    The functions from init_recurrent_bias are applied in order:   the first initializes mathbfb_hh^f, the second mathbfb_hh^h.\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/rancell/#RANCell","page":"RANCell","title":"RANCell","text":"","category":"section"},{"location":"api/cells/rancell/","page":"RANCell","title":"RANCell","text":"    RANCell","category":"page"},{"location":"api/cells/rancell/#LuxRecurrentLayers.RANCell","page":"RANCell","title":"LuxRecurrentLayers.RANCell","text":"RANCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32, init_memory=zeros32)\n\nRecurrent Additive Network cell.\n\nEquations\n\nbeginaligned\ntildec_t = W_c x_t \ni_t         = sigma(W_i x_t + U_i h_t-1 + b_i) \nf_t         = sigma(W_f x_t + U_f h_t-1 + b_f) \nc_t         = i_t odot tildec_t + f_t odot c_t-1 \nh_t         = g(c_t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_bias: Initializer for recurrent bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W_if W_ic W_ii W_io .\nweight_hh: Concatenated weights to map from hidden space               W_hf W_hc W_hi W_ho \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Concatenated Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/tlstmcell/#TLSTMCell","page":"TLSTMCell","title":"TLSTMCell","text":"","category":"section"},{"location":"api/cells/tlstmcell/","page":"TLSTMCell","title":"TLSTMCell","text":"    TLSTMCell","category":"page"},{"location":"api/cells/tlstmcell/#LuxRecurrentLayers.TLSTMCell","page":"TLSTMCell","title":"LuxRecurrentLayers.TLSTMCell","text":"TLSTMCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32, init_memory=zeros32)\n\nStrongly typed long short term memory cell.\n\nEquations\n\nbeginaligned\n    z_t = mathbfV_z mathbfx_t-1 + mathbfW_z mathbfx_t + mathbfb_z \n    f_t = sigma (mathbfV_f mathbfx_t-1 + mathbfW_f mathbfx_t +\n        mathbfb_f) \n    o_t = tau (mathbfV_o mathbfx_t-1 + mathbfW_o mathbfx_t + mathbfb_o) \n    c_t = f_t odot c_t-1 + (1 - f_t) odot z_t \n    h_t = c_t odot o_t\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_bias: Initializer for recurrent bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W_if W_ic W_ii W_io .\nweight_hh: Concatenated weights to map from hidden space               W_hf W_hc W_hi W_ho \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Concatenated Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/mut3cell/#MUT3Cell","page":"MUT3Cell","title":"MUT3Cell","text":"","category":"section"},{"location":"api/cells/mut3cell/","page":"MUT3Cell","title":"MUT3Cell","text":"    MUT3Cell","category":"page"},{"location":"api/cells/mut3cell/#LuxRecurrentLayers.MUT3Cell","page":"MUT3Cell","title":"LuxRecurrentLayers.MUT3Cell","text":"MUT3Cell(in_dims => out_dims;\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nMutated unit 2 cell.\n\nEquations\n\nbeginaligned\n    z = sigma(W_z x_t + U_z h_t + b_z) \n    r = sigma(x_t + U_r h_t + b_r) \n    h_t+1 = tanh(U_h (r odot h_t) + W_h x_t + b_h) odot z \n        quad + h_t odot (1 - z)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Weights to map from input space              W .\nweight_hh: Weights to map from hidden space               w_h \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/brcell/#BRCell","page":"BRCell","title":"BRCell","text":"","category":"section"},{"location":"api/cells/brcell/","page":"BRCell","title":"BRCell","text":"    BRCell","category":"page"},{"location":"api/cells/brcell/#LuxRecurrentLayers.BRCell","page":"BRCell","title":"LuxRecurrentLayers.BRCell","text":"BRCell(in_dims => out_dims;\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nBistable recurrent cell.\n\nEquations\n\nbeginaligned\n    mathbfa(t) = 1 + tanhleft(mathbfW_ih^a mathbfx(t) +\n        mathbfb_ih^a + mathbfw_hh^a circ mathbfh(t-1) +\n        mathbfb_hh^a right) \n    mathbfc(t) = sigmaleft(mathbfW_ih^c mathbfx(t) +\n        mathbfb_ih^c + mathbfw_hh^c circ mathbfh(t-1) +\n        mathbfb_hh^c right)\n    mathbfh(t) = mathbfc(t) circ mathbfh(t-1) + (1 - mathbfc(t))\n        circ tanhleft(mathbfW_ih^h mathbfx(t) + mathbfb_ih^h +\n        mathbfa(t) circ mathbfh(t-1)right)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for input to hidden bias mathbfb_ih^a mathbfb_ih^c mathbfb_ih^h. Must be a tuple containing 3 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a  3-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden to hidden bias mathbfb_hh^a mathbfb_hh^c. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a  2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input to hidden weights mathbfW_ih^a mathbfW_ih^c mathbfW_ih^h. Must be a tuple containing 3 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 3-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for input to hidden weights mathbfw_hh^a mathbfw_hh^c. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights to map from input to the hidden state               mathbfW_ih^a mathbfW_ih^c mathbfW_ih^h  The initializers in init_weight are applied in the order they appear: the first function is used for mathbfW_ih^a, the second for mathbfW_ih^c, and the third for mathbfW_ih^h.\nweight_hh: Weights to map the hidden state to the hidden state               mathbfw_hh^a mathbfw_hh^c  The initializers in init_weight are applied in the order they appear: the first function is used for mathbfw_hh^a, and the second for mathbfw_hh^c.\nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)               mathbfb_ih^a mathbfb_ih^c mathbfb_ih^h  The initializers in init_bias are applied in the order they appear: the first function is used for mathbfb_ih^z, the second for mathbfb_ih^c, and the third for mathbfb_ih^h.\nbias_hh: Bias vector for the input-hidden connection (not present if use_bias=false)               mathbfb_hh^a mathbfb_hh^c  The initializers in init_bias are applied in the order they appear: the first function is used for mathbfb_hh^z, and the second for mathbfb_hh^c.\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/lightrucell/#LightRUCell","page":"LightRUCell","title":"LightRUCell","text":"","category":"section"},{"location":"api/cells/lightrucell/","page":"LightRUCell","title":"LightRUCell","text":"    LightRUCell","category":"page"},{"location":"api/cells/lightrucell/#LuxRecurrentLayers.LightRUCell","page":"LightRUCell","title":"LuxRecurrentLayers.LightRUCell","text":"LightRUCell(in_dims => out_dims, [activation];\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nLight recurrent unit.\n\nEquations\n\nbeginaligned\n    tildemathbfh(t) = tanhleft( mathbfW_ih^h mathbfx(t) +\n        mathbfb_ih^h right) \n    mathbff(t) = deltaleft( mathbfW_ih^f mathbfx(t) +\n        mathbfb_ih^f + mathbfW_hh^f mathbfh(t-1) +\n        mathbfb_hh^f right) \n    mathbfh(t) = (1 - mathbff(t)) circ mathbfh(t-1) + mathbff(t)\n        circ tildemathbfh(t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for input-to-hidden biases   mathbfb_ih^h mathbfb_ih^f.   Must be a tuple of 2 functions. If a single function is passed, it is expanded to 2 copies. If set to nothing, each bias is initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden-to-hidden bias   mathbfb_hh^f. Must be a single function. If set to nothing, initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input-to-hidden weights   mathbfW_ih^h mathbfW_ih^f. Must be a tuple of 2 functions. If a single function is passed, it is expanded to 2 copies. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for hidden-to-hidden weight   mathbfW_hh^f.   Must be a single function. If set to nothing, initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Input-to-hidden weights    mathbfW_ih^h mathbfW_ih^f    The functions from init_weight are applied in order:   the first initializes mathbfW_ih^h, the second mathbfW_ih^f.\nweight_hh: Hidden-to-hidden weight    mathbfW_hh^f    Initialized via init_recurrent_weight.\nbias_ih: Input-to-hidden biases (if use_bias=true)    mathbfb_ih^h mathbfb_ih^f    The functions from init_bias are applied in order:   the first initializes mathbfb_ih^h, the second mathbfb_ih^f.\nbias_hh: Hidden-to-hidden bias (if use_bias=true)    mathbfb_hh^f    Initialized via init_recurrent_bias.\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/sgrncell/#SGRNCell","page":"SGRNCell","title":"SGRNCell","text":"","category":"section"},{"location":"api/cells/sgrncell/","page":"SGRNCell","title":"SGRNCell","text":"    SGRNCell","category":"page"},{"location":"api/cells/sgrncell/#LuxRecurrentLayers.SGRNCell","page":"SGRNCell","title":"LuxRecurrentLayers.SGRNCell","text":"SGRNCell(in_dims => out_dims;\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nSimple gated recurrent network.\n\nEquations\n\nbeginaligned\n    mathbff_t = sigma(mathbfW mathbfx_t + mathbfU mathbfh_t-1 +\n        mathbfb) \n    mathbfi_t = 1 - mathbff_t \n    mathbfh_t = tanhleft(mathbfi_t circ (mathbfW mathbfx_t) +\n        mathbff_t circ mathbfh_t-1right)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Weights to map from input space              W .\nweight_hh: Weights to map from hidden space               w_h \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/janetcell/#JANETCell","page":"JANETCell","title":"JANETCell","text":"","category":"section"},{"location":"api/cells/janetcell/","page":"JANETCell","title":"JANETCell","text":"    JANETCell","category":"page"},{"location":"api/cells/janetcell/#LuxRecurrentLayers.JANETCell","page":"JANETCell","title":"LuxRecurrentLayers.JANETCell","text":"JANETCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32, init_memory=zeros32, beta=1.0)\n\nJust another network unit.\n\nEquations\n\nbeginaligned\n    mathbfs(t) = mathbfW_ih^f mathbfx(t) + mathbfb_ih^f +\n        mathbfW_hh^f mathbfh(t-1) + mathbfb_hh^f \n    tildemathbfc(t) = tanhleft( mathbfW_ih^c mathbfx(t) +\n        mathbfb_ih^c + mathbfW_hh^c mathbfh(t-1) +\n        mathbfb_hh^c right) \n    mathbfc(t) = sigma(mathbfs(t)) circ mathbfc(t-1) + left(1 -\n        sigma(mathbfs(t) - beta)right) circ tildemathbfc(t) \n    mathbfh(t) = mathbfc(t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for input-to-hidden biases mathbfb_ih^f and mathbfb_ih^c. Must be a tuple of 2 functions, e.g., (glorot_uniform, kaiming_uniform). If a single function fn is provided, it is expanded to (fn, fn). If set to nothing, each bias is initialized from a uniform distribution  within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden-to-hidden biases mathbfb_hh^f and mathbfb_hh^c. Must be a tuple of 2 functions, e.g., (glorot_uniform, kaiming_uniform). If a single function fn is provided, it is expanded to (fn, fn). If set to nothing, each bias is initialized from a uniform distribution  within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input-to-hidden weights mathbfW_ih^f and mathbfW_ih^c. Must be a tuple of 2 functions, e.g., (glorot_uniform, kaiming_uniform). If a single function fn is provided, it is expanded to (fn, fn). If set to nothing, each weight is initialized from a uniform distribution  within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for hidden-to-hidden weights mathbfW_hh^f and mathbfW_hh^c. Must be a tuple of 2 functions, e.g., (glorot_uniform, kaiming_uniform). If a single function fn is provided, it is expanded to (fn, fn). If set to nothing, each weight is initialized from a uniform distribution  within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\nbeta: Control parameter over the input data flow. Default is 1.0.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights mapping from input to hidden units    mathbfW_ih^f mathbfW_ih^c    The functions provided in init_weight are applied in order:   the first function initializes mathbfW_ih^f, the second initializes mathbfW_ih^c.\nweight_hh: Concatenated weights mapping from hidden state to hidden units    mathbfW_hh^f mathbfW_hh^c    The functions provided in init_recurrent_weight are applied in order:   the first function initializes mathbfW_hh^f, the second initializes mathbfW_hh^c.\nbias_ih: Concatenated input-to-hidden bias vectors (if use_bias=true)    mathbfb_ih^f mathbfb_ih^c    The functions provided in init_bias are applied in order:   the first function initializes mathbfb_ih^f, the second initializes mathbfb_ih^c.\nbias_hh: Concatenated hidden-to-hidden bias vectors (if use_bias=true)    mathbfb_hh^f mathbfb_hh^c    The functions provided in init_recurrent_bias are applied in order:   the first function initializes mathbfb_hh^f, the second initializes mathbfb_hh^c.\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/ligrucell/#LiGRUCell","page":"LiGRUCell","title":"LiGRUCell","text":"","category":"section"},{"location":"api/cells/ligrucell/","page":"LiGRUCell","title":"LiGRUCell","text":"    LiGRUCell","category":"page"},{"location":"api/cells/ligrucell/#LuxRecurrentLayers.LiGRUCell","page":"LiGRUCell","title":"LuxRecurrentLayers.LiGRUCell","text":"LiGRUCell(in_dims => out_dims, [activation];\n    use_bias=true, train_state=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nLight gated recurrent unit.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = sigmaleft( \n        mathbfW_ih^z mathbfx(t) + mathbfb_ih^z + \n        mathbfW_hh^z mathbfh(t-1) + mathbfb_hh^z right) \n    tildemathbfh(t) = textReLUleft( \n        mathbfW_ih^h mathbfx(t) + mathbfb_ih^h + \n        mathbfW_hh^h mathbfh(t-1) + mathbfb_hh^h right) \n    mathbfh(t) = mathbfz(t) circ mathbfh(t-1) + \n        left(1 - mathbfz(t)right) circ tildemathbfh(t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for input-to-hidden biases   mathbfb_ih^z mathbfb_ih^h.   Must be a tuple of 2 functions. If a single function is passed, it is expanded to 2 copies. If set to nothing, each bias is initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden-to-hidden biases   mathbfb_hh^z mathbfb_hh^h.   Must be a tuple of 2 functions. If a single function is passed, it is expanded to 2 copies. If set to nothing, each bias is initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input-to-hidden weights   mathbfW_ih^z mathbfW_ih^h.   Must be a tuple of 2 functions. If a single function is passed, it is expanded to 2 copies. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for hidden-to-hidden weights   mathbfW_hh^z mathbfW_hh^h.   Must be a tuple of 2 functions. If a single function is passed, it is expanded to 2 copies. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Input-to-hidden weights    mathbfW_ih^z mathbfW_ih^h    The functions from init_weight are applied in order:   the first initializes mathbfW_ih^z, the second mathbfW_ih^h.\nweight_hh: Hidden-to-hidden weights    mathbfW_hh^z mathbfW_hh^h    The functions from init_recurrent_weight are applied in order:   the first initializes mathbfW_hh^z, the second mathbfW_hh^h.\nbias_ih: Input-to-hidden biases (if use_bias=true)    mathbfb_ih^z mathbfb_ih^h    The functions from init_bias are applied in order:   the first initializes mathbfb_ih^z, the second mathbfb_ih^h.\nbias_hh: Hidden-to-hidden biases (if use_bias=true)    mathbfb_hh^z mathbfb_hh^h    The functions from init_recurrent_bias are applied in order:   the first initializes mathbfb_hh^z, the second mathbfb_hh^h.\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/atrcell/#ATRCell","page":"ATRCell","title":"ATRCell","text":"","category":"section"},{"location":"api/cells/atrcell/","page":"ATRCell","title":"ATRCell","text":"    ATRCell","category":"page"},{"location":"api/cells/atrcell/#LuxRecurrentLayers.ATRCell","page":"ATRCell","title":"LuxRecurrentLayers.ATRCell","text":"ATRCell(in_dims => out_dims;\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_recurrent_bias=nothing, init_weight=nothing,\n    init_recurrent_weight=nothing, init_state=zeros32)\n\nAddition-subtraction twin-gated recurrent cell.\n\nEquations\n\nbeginaligned\n    mathbfp(t) = mathbfW_ih mathbfx(t) + mathbfb_ih \n    mathbfq(t) = mathbfW_ih mathbfh(t-1) + mathbfb_hh \n    mathbfi(t) = sigma(mathbfp(t) + mathbfq(t)) \n    mathbff(t) = sigma(mathbfp(t) - mathbfq(t)) \n    mathbfh(t) = mathbfi(t) circ mathbfp(t) + mathbff(t)\n        circ mathbfh(t-1)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for input to hidden bias mathbfb_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden to hidden bias mathbfb_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input to hidden weights mathbfW_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for recurrent weight mathbfW_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights to map from input to the hidden state mathbfW_ih.\nweight_hh: Concatenated weights to map from hidden to the hidden state mathbfW_hh.\nbias_ih: Bias vector for the input-hidden connection (not present if   use_bias=false) mathbfb_ih.\nbias_hh: Bias vector for the hidden-hidden connection (not present if   use_bias=false) mathbfb_hh.\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/peepholelstmcell/#PeepholeLSTMCell","page":"PeepholeLSTMCell","title":"PeepholeLSTMCell","text":"","category":"section"},{"location":"api/cells/peepholelstmcell/","page":"PeepholeLSTMCell","title":"PeepholeLSTMCell","text":"    PeepholeLSTMCell","category":"page"},{"location":"api/cells/peepholelstmcell/#LuxRecurrentLayers.PeepholeLSTMCell","page":"PeepholeLSTMCell","title":"LuxRecurrentLayers.PeepholeLSTMCell","text":"PeepholeLSTMCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing, init_peephole_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_peephole_weight=nothing, init_state=zeros32, init_memory=zeros32)\n\nPeephole long short term memory.\n\nEquations\n\nbeginaligned\n    z_t = tanh(W_z x_t + U_z h_t-1 + b_z) \n    i_t = sigma(W_i x_t + U_i h_t-1 + p_i odot c_t-1 + b_i) \n    f_t = sigma(W_f x_t + U_f h_t-1 + p_f odot c_t-1 + b_f) \n    c_t = f_t odot c_t-1 + i_t odot z_t \n    o_t = sigma(W_o x_t + U_o h_t-1 + p_o odot c_t + b_o) \n    h_t = o_t odot tanh(c_t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_bias: Initializer for recurrent bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_peephole_bias: Initializer for peephole bias. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_peephole_weight: Initializer for peephole weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W_if W_ic W_ii W_io .\nweight_hh: Concatenated weights to map from hidden space               W_hf W_hc W_hi W_ho \nweight_ph: Concatenated weights to map from peephole space               W_ff W_fc W_fi \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Concatenated Bias vector for the hidden-hidden connection (not present if use_bias=false)\nbias_ph: Concatenated Bias vector for the peephole-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/indrnncell/#IndRNNCell","page":"IndRNNCell","title":"IndRNNCell","text":"","category":"section"},{"location":"api/cells/indrnncell/","page":"IndRNNCell","title":"IndRNNCell","text":"    IndRNNCell","category":"page"},{"location":"api/cells/indrnncell/#LuxRecurrentLayers.IndRNNCell","page":"IndRNNCell","title":"LuxRecurrentLayers.IndRNNCell","text":"IndRNNCell(in_dims => out_dims, [activation];\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nIndependently recurrent cell.\n\nEquations\n\nbeginequation\n    mathbfh(t) = sigmaleft( mathbfW_ih mathbfx(t) + mathbfb_ih +\n        mathbfw_hh circ mathbfh(t-1) + mathbfb_hh right)\nendequation\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n'activation': Activation function. Defaults to tanh_fast\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for bias mathbfb_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for bias mathbfb_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input to hidden weight mathbfW_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for hidden to hidden weight mathbfw_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Weights to map from input space mathbfW_ih.\nweight_hh: Weights to map from hidden space mathbfW_hh.\nbias_ih: Bias vector for the input-hidden connection mathbfb_ih (not present if use_bias=false)\nbias_hh: Bias vector for the hidden-hidden connection mathbfb_hh (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/fastgrnncell/#FastGRNNCell","page":"FastGRNNCell","title":"FastGRNNCell","text":"","category":"section"},{"location":"api/cells/fastgrnncell/","page":"FastGRNNCell","title":"FastGRNNCell","text":"    FastGRNNCell","category":"page"},{"location":"api/cells/fastgrnncell/#LuxRecurrentLayers.FastGRNNCell","page":"FastGRNNCell","title":"LuxRecurrentLayers.FastGRNNCell","text":"FastGRNNCell(input_size => hidden_size, [activation];\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_recurrent_bias=nothing, init_weight=nothing,\n    init_recurrent_weight=nothing, init_state=zeros32,\n    init_zeta=1.0, init_nu=4.0)\n\nFast gated recurrent neural network cell.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = sigmaleft( mathbfW_ih mathbfx(t) +\n        mathbfb_ih^z + mathbfW_hh mathbfh(t-1) +\n        mathbfb_hh^z right) \n    tildemathbfh(t) = tanhleft( mathbfW_ih mathbfx(t) +\n        mathbfb_ih^h + mathbfW_hh mathbfh(t-1) +\n        mathbfb_hh^h right) \n    mathbfh(t) = left( left( zeta (1 - mathbfz(t)) + nu right)\n        circ tildemathbfh(t) right) + mathbfz(t) circ\n        mathbfh(t-1)\nendaligned\n\nArguments\n\nin_dims: Input dimension\nout_dims: Output (Hidden State & Memory) dimension\nactivation: activation function. Default is tanh\n\nKeyword arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for input to hidden bias mathbfb_ih^z mathbfb_ih^h. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden to hidden bias mathbfb_hh^z mathbfb_hh^h. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for weight mathbfW_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for recurrent weight mathbfW_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_zeta: initializer for the zeta learnable parameter. Default is 1.0.\ninit_nu: initializer for the nu learnable parameter. Default is 4.0.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights to map from input to the hidden state mathbfW_ih.\nweight_hh: Concatenated weights to map from hidden to the hidden state mathbfW_hh.\nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)               mathbfb_ih^z mathbfb_ih^h  The initializers in init_bias are applied in the order they appear: the first function is used for mathbfb_ih^z, and the second for mathbfb_ih^h.\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)               mathbfb_hh^z mathbfb_hh^h  The initializers in init_bias are applied in the order they appear: the first function is used for mathbfb_hh^z, and the second for mathbfb_hh^h.\nhidden_state: Initial hidden state vector (not present if train_state=false)\nzeta: Learnable scalar to modulate candidate state.\nnu: Learnable scalar to modulate previous state.\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/gatedantisymmetricrnncell/#GatedAntisymmetricRNNCell","page":"GatedAntisymmetricRNNCell","title":"GatedAntisymmetricRNNCell","text":"","category":"section"},{"location":"api/cells/gatedantisymmetricrnncell/","page":"GatedAntisymmetricRNNCell","title":"GatedAntisymmetricRNNCell","text":"    GatedAntisymmetricRNNCell","category":"page"},{"location":"api/cells/gatedantisymmetricrnncell/#LuxRecurrentLayers.GatedAntisymmetricRNNCell","page":"GatedAntisymmetricRNNCell","title":"LuxRecurrentLayers.GatedAntisymmetricRNNCell","text":"GatedAntisymmetricRNNCell(in_dims => out_dims, [activation];\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_recurrent_bias=nothing, init_weight=nothing,\n    init_recurrent_weight=nothing, init_state=zeros32,\n    epsilon=1.0, gamma=0.0)\n\nAntisymmetric recurrent cell with gating.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = sigmaleft( \n        (mathbfW_hh - mathbfW_hh^top - gamma cdot mathbfI)\n        mathbfh(t-1) + mathbfb_hh + mathbfW_ih^z mathbfx(t)\n        + mathbfb_ih^z right) \n    mathbfh(t) = mathbfh(t-1) + epsilon cdot mathbfz(t) circ\n        tanhleft( (mathbfW_hh - mathbfW_hh^top - gamma cdot\n        mathbfI) mathbfh(t-1) + mathbfb_hh + mathbfW_ih^x\n        mathbfx(t) + mathbfb_ih^h right)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\nactivation: activation function. Default is tanh\n\nKeyword arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for input to hidden bias mathbfb_ih^z mathbfb_ih^h. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden to hidden bias mathbfb_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input to hidden weights mathbfW_ih^z mathbfW_ih^x. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for recurrent weight mathbfW_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\nepsilon: step size. Default is 1.0.\ngamma: strength of diffusion. Default is 0.0.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights to map from input to the hidden state.               mathbfW_ih^z mathbfW_ih^h  The initializers in init_weight are applied in the order they appear: the first function is used for mathbfW_ih^z, and the second for mathbfW_ih^h.\nweight_hh: Weights to map the hidden state to the hidden state mathbfW_hh.\nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)               mathbfb_ih^z mathbfb_ih^h  The initializers in init_bias are applied in the order they appear: the first function is used for mathbfb_ih^z, and the second for mathbfb_ih^h.\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false) mathbfb_hh\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/cfncell/#CFNCell","page":"CFNCell","title":"CFNCell","text":"","category":"section"},{"location":"api/cells/cfncell/","page":"CFNCell","title":"CFNCell","text":"    CFNCell","category":"page"},{"location":"api/cells/cfncell/#LuxRecurrentLayers.CFNCell","page":"CFNCell","title":"LuxRecurrentLayers.CFNCell","text":"CFNCell(in_dims => out_dims, [activation];\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_recurrent_bias=nothing, init_weight=nothing,\n    init_recurrent_weight=nothing, init_state=zeros32)\n\nChaos free network unit.\n\nEquations\n\nbeginaligned\n    boldsymboltheta(t) = sigmaleft(mathbfW_ih^theta mathbfx(t)\n        + mathbfb_ih^theta + mathbfW_hh^theta mathbfh(t-1) +\n        mathbfb_hh^thetaright) \n    boldsymboleta(t) = sigmaleft(mathbfW_ih^eta mathbfx(t) +\n        mathbfb_ih^eta + mathbfW_hh^eta mathbfh(t-1) + \n        mathbfb_hh^eta right) \n    mathbfh(t) = boldsymboltheta(t) circ tanh(mathbfh(t-1)) +\n        boldsymboleta(t) circ tanh(mathbfW_ih^h mathbfx(t) +\n        mathbfb_ih^h)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\nactivation: activation function. Default is tanh\n\nKeyword arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for input to hidden bias mathbfb_ih^theta mathbfb_ih^eta mathbfb_ih^h. Must be a tuple containing 3 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 3-element tuple (fn, fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden to hidden bias mathbfb_hh^theta mathbfb_hh^eta. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input to hidden weights mathbfW_ih^theta mathbfW_ih^eta mathbfW_ih^h. Must be a tuple containing 3 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 3-element tuple (fn, fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for input to hidden weights mathbfW_hh^theta mathbfW_hh^eta. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights to map from input to the hidden state.               mathbfW_ih^theta mathbfW_ih^eta mathbfW_ih^h  The initializers in init_weight are applied in the order they appear: the first function is used for mathbfW_ih^theta, the second for mathbfW_ih^eta, and the third for mathbfW_ih^h.\nweight_hh: Concatenated weights to map from hidden to hidden state.               mathbfW_hh^theta mathbfW_hh^eta  The initializers in init_recurrent_weight are applied in the order they appear: the first function is used for mathbfW_hh^theta, and the second for mathbfW_hh^eta.\nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)               mathbfb_ih^theta mathbfb_ih^eta mathbfb_ih^h  The initializers in init_bias are applied in the order they appear: the first function is used for mathbfb_ih^theta, the second for   mathbfb_ih^eta, and the third for mathbfb_ih^h.\nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)               mathbfb_hh^theta mathbfb_hh^eta  The initializers in init_recurrent_bias are applied in the order they appear: the first function is used for mathbfb_hh^theta, and the second for   mathbfb_hh^eta.\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/cornncell/#coRNNCell","page":"coRNNCell","title":"coRNNCell","text":"","category":"section"},{"location":"api/cells/cornncell/","page":"coRNNCell","title":"coRNNCell","text":"    coRNNCell","category":"page"},{"location":"api/cells/cornncell/#LuxRecurrentLayers.coRNNCell","page":"coRNNCell","title":"LuxRecurrentLayers.coRNNCell","text":"coRNNCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing, init_cell_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_cell_weight=nothing, init_state=zeros32, init_memory=zeros32,\n    gamma=0.0, epsilon=0.0, dt=1.0)\n\nCoupled oscillatory recurrent neural unit.\n\nEquations\n\nbeginaligned\n    mathbfc(t) = mathbfc(t-1) + Delta t  sigmaleft( \n        mathbfW_ih mathbfx(t) + mathbfb_ih + \n        mathbfW_hh mathbfh(t-1) + mathbfb_hh + \n        mathbfW_ch mathbfc(t-1) + mathbfb_ch right) \n        - Delta t  gamma  mathbfh(t-1) - Delta t  epsilon \n        mathbfc(t) \n    mathbfh(t) = mathbfh(t-1) + Delta t  mathbfc(t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for input to hidden bias mathbfb_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden to hidden bias mathbfb_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_cell_bias: Initializer for cell to hidden bias mathbfb_ch. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input to hidden weight mathbfW_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for hidden to hidden weight mathbfW_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_cell_weight: Initializer for cell to hidden weight mathbfW_ch. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\ndt: time step. Default is 1.0.\ngamma: Damping for state. Default is 0.0.\nepsilon: Damping for candidate state. Default is 0.0.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Weights to map the input to the hidden state mathbfW_ih.\nweight_hh: Weights to map the hidden state to the hidden state mathbfW_hh.\nweight_ch: Weights to map the cell state to the hidden state mathbfW_ch.\nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false) mathbfb_ih\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false) mathbfb_hh\nbias_ch: Bias vector for the cell-hidden connection (not present if use_bias=false) mathbfb_ch\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/fastrnncell/#FastRNNCell","page":"FastRNNCell","title":"FastRNNCell","text":"","category":"section"},{"location":"api/cells/fastrnncell/","page":"FastRNNCell","title":"FastRNNCell","text":"    FastRNNCell","category":"page"},{"location":"api/cells/fastrnncell/#LuxRecurrentLayers.FastRNNCell","page":"FastRNNCell","title":"LuxRecurrentLayers.FastRNNCell","text":"FastRNNCell(in_dims => out_dims, [activation];\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_recurrent_bias=nothing, init_weight=nothing,\n    init_recurrent_weight=nothing, init_state=zeros32,\n    init_alpha=-3.0, init_beta=3.0)\n\nFast recurrent neural network cell.\n\nEquations\n\nbeginaligned\n    tildemathbfh(t) = sigmaleft( mathbfW_ih mathbfx(t) +\n        mathbfb_ih + mathbfW_hh mathbfh(t-1) + mathbfb_hh\n        right) \n    mathbfh(t) = alpha  tildemathbfh(t) + beta \n        mathbfh(t-1)\nendaligned\n\nArguments\n\nin_dims: Input dimension\nout_dims: Output (Hidden State & Memory) dimension\nactivation: activation function. Default is tanh\n\nKeyword arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for bias mathbfb_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden to hidden bias mathbfb_hh^z mathbfb_hh^h. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for weight mathbfW_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for recurrent weight mathbfW_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_alpha: initializer for the alpha learnable parameter. Default is -3.0.\ninit_beta: initializer for the beta learnable parameter. Default is 3.0.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights to map from input to the hidden state mathbfW_ih.\nweight_hh: Concatenated weights to map from hidden to the hidden state mathbfW_hh.\nbias_ih: Bias vector for the input-hidden connection (not present if   use_bias=false) mathbfb_ih.\nbias_hh: Bias vector for the hidden-hidden connection (not present if   use_bias=false) mathbfb_hh.\nhidden_state: Initial hidden state vector (not present if train_state=false)\nalpha: Learnable scalar to modulate candidate state.\nbeta: Learnable scalar to modulate previous state.\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/multiplicativelstmcell/#MinimalRNNCell","page":"MinimalRNNCell","title":"MinimalRNNCell","text":"","category":"section"},{"location":"api/cells/multiplicativelstmcell/","page":"MinimalRNNCell","title":"MinimalRNNCell","text":"    MinimalRNNCell","category":"page"},{"location":"api/cells/multiplicativelstmcell/#LuxRecurrentLayers.MinimalRNNCell","page":"MinimalRNNCell","title":"LuxRecurrentLayers.MinimalRNNCell","text":"MinimalRNNCell(in_dims => out_dims;\n    use_bias=true, train_state=false,\n    init_encoder_bias=nothing, init_recurrent_bias=nothing,\n    init_memory_bias=nothing, init_encoder_weight=nothing,\n    init_recurrent_weight=nothing, init_memory_weight=nothing,\n    init_state=zeros32,)\n\nMinimal recurrent neural network unit.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = tanhleft( mathbfW_ih^z mathbfx(t) +\n        mathbfb_ih^z right) \n    mathbfu(t) = sigmaleft( mathbfW_hh^u mathbfh(t-1) +\n        mathbfW_zh^u mathbfz(t) + mathbfb_hh^u right) \n    mathbfh(t) = mathbfu(t) circ mathbfh(t-1) + left(1 -\n        mathbfu(t)right) circ mathbfz(t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword arguments\n\nuse_bias: Set to false to deactivate bias\ntrain_state: Trainable initial hidden state can be activated by setting this to true\ntrain_memory: Trainable initial memory can be activated by setting this to true\ninit_encoder_bias: Initializer for encoder bias mathbfb_ih^z.   Must be a single function. If nothing, initialized from a uniform distribution in [-bound, bound] where bound = inv(sqrt(out_dims)).\ninit_recurrent_bias: Initializer for recurrent bias mathbfb_hh^u.   Must be a single function. If nothing, initialized from a uniform distribution in   [-bound, bound] where bound = inv(sqrt(out_dims)).\ninit_memory_bias: Initializer for memory bias mathbfb_zh^u.   Must be a single function. If nothing, initialized from a uniform distribution in   [-bound, bound] where bound = inv(sqrt(out_dims)).\ninit_encoder_weight: Initializer for encoder weight mathbfW_ih^z.   Must be a single function. If nothing, initialized from a uniform distribution in   [-bound, bound] where bound = inv(sqrt(out_dims)).\ninit_recurrent_weight: Initializer for recurrent weight mathbfW_hh^u.   Must be a single function. If nothing, initialized from a uniform distribution in   [-bound, bound] where bound = inv(sqrt(out_dims)).\ninit_memory_weight: Initializer for memory weight mathbfW_zh^u.   Must be a single function. If nothing, initialized from a uniform distribution in   [-bound, bound] where bound = inv(sqrt(out_dims)).\ninit_state: Initializer for hidden state\ninit_memory: Initializer for memory\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Encoder weights    mathbfW_ih^z   \nweight_hh: Recurrent weights    mathbfW_hh^u   \nweight_mm: Memory weights    mathbfW_zh^u   \nbias_ih: Encoder bias (if use_bias=true)    mathbfb_ih^z   \nbias_hh: Recurrent bias (if use_bias=true)    mathbfb_hh^u   \nbias_mm: Memory bias (if use_bias=true)    mathbfb_zh^u   \nhidden_state: Initial hidden state vector mathbfh(0)   (not present if train_state=false).\nmemory: Initial memory vector mathbfc(0)   (not present if train_memory=false).\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/wmclstmcell/#WMCLSTMCell","page":"WMCLSTMCell","title":"WMCLSTMCell","text":"","category":"section"},{"location":"api/cells/wmclstmcell/","page":"WMCLSTMCell","title":"WMCLSTMCell","text":"    WMCLSTMCell","category":"page"},{"location":"api/cells/wmclstmcell/#LuxRecurrentLayers.WMCLSTMCell","page":"WMCLSTMCell","title":"LuxRecurrentLayers.WMCLSTMCell","text":"WMCLSTMCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing, init_memory_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_memory_weight=nothing, init_state=zeros32, init_memory=zeros32)\n\nLong short term memory cell with working memory connections.\n\nEquations\n\nbeginaligned\n    mathbfi_t = sigmaleft(mathbfW_ix mathbfx_t + mathbfW_ih\n        mathbfh_t-1 + tanh(mathbfW_ic mathbfc_t-1) +\n        mathbfb_iright) \n    mathbff_t = sigmaleft(mathbfW_fx mathbfx_t + mathbfW_fh\n        mathbfh_t-1 + tanh(mathbfW_fc mathbfc_t-1) +\n        mathbfb_fright) \n    mathbfo_t = sigmaleft(mathbfW_ox mathbfx_t + mathbfW_oh\n        mathbfh_t-1 + tanh(mathbfW_oc mathbfc_t) + mathbfb_oright) \n    mathbfc_t = mathbff_t circ mathbfc_t-1 + mathbfi_t circ\n        sigma_c(mathbfW_c mathbfx_t + mathbfb_c) \n    mathbfh_t = mathbfo_t circ sigma_h(mathbfc_t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_bias: Initializer for recurrent bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_memory_bias: Initializer for memory bias. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_memory_weight: Initializer for memory weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W_if W_ic W_ii W_io .\nweight_hh: Concatenated weights to map from hidden space               W_hf W_hc W_hi W_ho \nweight_ph: Concatenated weights to map from memory space               W_ff W_fc W_fi \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Concatenated Bias vector for the hidden-hidden connection (not present if use_bias=false)\nbias_ph: Concatenated Bias vector for the memory-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/minimalrnncell/#MultiplicativeLSTMCell","page":"MultiplicativeLSTMCell","title":"MultiplicativeLSTMCell","text":"","category":"section"},{"location":"api/cells/minimalrnncell/","page":"MultiplicativeLSTMCell","title":"MultiplicativeLSTMCell","text":"    MultiplicativeLSTMCell","category":"page"},{"location":"api/cells/minimalrnncell/#LuxRecurrentLayers.MultiplicativeLSTMCell","page":"MultiplicativeLSTMCell","title":"LuxRecurrentLayers.MultiplicativeLSTMCell","text":"MultiplicativeLSTMCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_multiplicative_bias=nothing, init_weight=nothing,\n    init_recurrent_weight=nothing, init_multiplicative_weight=nothing,\n    init_state=zeros32, init_memory=zeros32)\n\nMultiplicative long short term memory cell.\n\nEquations\n\nbeginaligned\n    mathbfm(t) = left( mathbfW_ih^m mathbfx(t) + mathbfb_ih^m\n        right) circ left( mathbfW_hh^m mathbfh(t-1) +\n        mathbfb_hh^m right) \n    hatmathbfh(t) = mathbfW_ih^h mathbfx(t) + mathbfb_ih^h\n        + mathbfW_mh^h mathbfm(t) + mathbfb_mh^h \n    mathbfi(t) = sigmaleft( mathbfW_ih^i mathbfx(t) +\n        mathbfb_ih^i + mathbfW_mh^i mathbfm(t) +\n        mathbfb_mh^i right) \n    mathbfo(t) = sigmaleft( mathbfW_ih^o mathbfx(t) +\n        mathbfb_ih^o + mathbfW_mh^o mathbfm(t) +\n        mathbfb_mh^o right) \n    mathbff(t) = sigmaleft( mathbfW_ih^f mathbfx(t) +\n        mathbfb_ih^f + mathbfW_mh^f mathbfm(t) +\n        mathbfb_mh^f right) \n    mathbfc(t) = mathbff(t) circ mathbfc(t-1) + mathbfi(t)\n        circ tanhleft( hatmathbfh(t) right) \n    mathbfh(t) = tanhleft( mathbfc(t) right) circ mathbfo(t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for input-to-hidden biases   mathbfb_ih^m mathbfb_ih^h mathbfb_ih^i mathbfb_ih^o mathbfb_ih^f.   Must be a tuple containing 5 functions. If a single value is passed, it is copied into a 5-element tuple.   If set to nothing, weights are initialized from a uniform distribution within [-bound, bound]   where bound = inv(sqrt(out_dims)).   The functions are applied in order:   the first initializes mathbfb_ih^m, the second mathbfb_ih^h, the third mathbfb_ih^i,   the fourth mathbfb_ih^o, and the fifth mathbfb_ih^f.\ninit_recurrent_bias: Initializer for hidden-to-hidden biases   mathbfb_hh^m.   Must be a tuple containing 1 function. If a single value is passed, it is used directly.   If set to nothing, weights are initialized from a uniform distribution within [-bound, bound]   where bound = inv(sqrt(out_dims)).  \ninit_multiplicative_bias: Initializer for multiplicative-to-hidden biases   mathbfb_mh^h mathbfb_mh^i mathbfb_mh^o mathbfb_mh^f.   Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4-element tuple.   If set to nothing, weights are initialized from a uniform distribution within [-bound, bound]   where bound = inv(sqrt(out_dims)).   The functions are applied in order:   the first initializes mathbfb_mh^h, the second mathbfb_mh^i,   the third mathbfb_mh^o, and the fourth mathbfb_mh^f.\ninit_weight: Initializer for input-to-hidden weights   mathbfW_ih^m mathbfW_ih^h mathbfW_ih^i mathbfW_ih^o mathbfW_ih^f.   Must be a tuple containing 5 functions. If a single value is passed, it is copied into a 5-element tuple.   If set to nothing, weights are initialized from a uniform distribution within [-bound, bound]   where bound = inv(sqrt(out_dims)).   The functions are applied in order:   the first initializes mathbfW_ih^m, the second mathbfW_ih^h,   the third mathbfW_ih^i, the fourth mathbfW_ih^o, and the fifth mathbfW_ih^f.\ninit_recurrent_weight: Initializer for hidden-to-hidden weights   mathbfW_hh^m.   Must be a tuple containing 1 function. If a single value is passed, it is used directly.   If set to nothing, weights are initialized from a uniform distribution within [-bound, bound]   where bound = inv(sqrt(out_dims)).  \ninit_multiplicative_weight: Initializer for multiplicative-to-hidden weights   mathbfW_mh^h mathbfW_mh^i mathbfW_mh^o mathbfW_mh^f.   Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4-element tuple.   If set to nothing, weights are initialized from a uniform distribution within [-bound, bound]   where bound = inv(sqrt(out_dims)).   The functions are applied in order:   the first initializes mathbfW_mh^h, the second mathbfW_mh^i,   the third mathbfW_mh^o, and the fourth mathbfW_mh^f.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Input-to-hidden weights    mathbfW_ih^m mathbfW_ih^h mathbfW_ih^i mathbfW_ih^o mathbfW_ih^f   \nweight_hh: Hidden-to-hidden weights    mathbfW_hh^m   \nweight_mh: Multiplicative-to-hidden weights    mathbfW_mh^h mathbfW_mh^i mathbfW_mh^o mathbfW_mh^f   \nbias_ih: Input-to-hidden biases (if use_bias=true)    mathbfb_ih^m mathbfb_ih^h mathbfb_ih^i mathbfb_ih^o mathbfb_ih^f   \nbias_hh: Hidden-to-hidden biases (if use_bias=true)    mathbfb_hh^m   \nbias_mh: Multiplicative-to-hidden biases (if use_bias=true)    mathbfb_mh^h mathbfb_mh^i mathbfb_mh^o mathbfb_mh^f   \nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/scrncell/#SCRNCell","page":"SCRNCell","title":"SCRNCell","text":"","category":"section"},{"location":"api/cells/scrncell/","page":"SCRNCell","title":"SCRNCell","text":"    SCRNCell","category":"page"},{"location":"api/cells/scrncell/#LuxRecurrentLayers.SCRNCell","page":"SCRNCell","title":"LuxRecurrentLayers.SCRNCell","text":"SCRNCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_weight=nothing, init_recurrent_weight=nothing,\n    init_context_weight=nothing, init_state=zeros32, init_memory=zeros32)\n\nStructurally contraint recurrent unit.\n\nEquations\n\nbeginaligned\ns_t = (1 - alpha) W_s x_t + alpha s_t-1 \nh_t = sigma(W_h s_t + U_h h_t-1 + b_h) \ny_t = f(U_y h_t + W_y s_t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_context_weight: Initializer for context weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W_if W_ic .\nweight_hh: Concatenated Weights to map from hidden space               W_hf W_hc \nweight_hh: Concatenated Weights to map from context space               W_cf W_cc \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Concatenated Bias vector for the hidden-hidden connection (not present if use_bias=false)\nalpha: Initial context strength.\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/antisymmetricrnncell/#AntysimmetricRNNCell","page":"AntysimmetricRNNCell","title":"AntysimmetricRNNCell","text":"","category":"section"},{"location":"api/cells/antisymmetricrnncell/","page":"AntysimmetricRNNCell","title":"AntysimmetricRNNCell","text":"    AntisymmetricRNNCell","category":"page"},{"location":"api/cells/antisymmetricrnncell/#LuxRecurrentLayers.AntisymmetricRNNCell","page":"AntysimmetricRNNCell","title":"LuxRecurrentLayers.AntisymmetricRNNCell","text":"AntisymmetricRNNCell(in_dims => out_dims, [activation];\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_recurrent_bias=nothing, init_weight=nothing,\n    init_recurrent_weight=nothing, init_state=zeros32,\n    epsilon=1.0, gamma=0.0)\n\nAntisymmetric recurrent cell.\n\nEquations\n\nbeginequation\n    mathbfh(t) = mathbfh(t-1) + epsilon cdot tanhleft(\n        mathbfW_ih mathbfx(t) + mathbfb_ih +\n        (mathbfW_hh - mathbfW_hh^top - gamma cdot mathbfI)\n        mathbfh(t-1) + mathbfb_hh right)\nendequation\n\nArguments\n\nin_dims: Input dimension\nout_dims: Output (Hidden State & Memory) dimension\nactivation: activation function. Default is tanh\n\nKeyword arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for bias mathbfb_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_bias: Initializer for recurrent bias mathbfb_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for weight mathbfW_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for recurrent weight mathbfW_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\nepsilon: step size epsilon. Default is 1.0.\ngamma: strength of diffusion gamma. Default is 0.0.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights to map from input to the hidden state mathbfW_ih.\nweight_hh: Concatenated weights to map from hidden to the hidden state mathbfW_hh.\nbias_ih: Bias vector for the input-hidden connection (not present if   use_bias=false) mathbfb_ih.\nbias_hh: Bias vector for the hidden-hidden connection (not present if   use_bias=false) mathbfb_hh.\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/mut2cell/#MUT2Cell","page":"MUT2Cell","title":"MUT2Cell","text":"","category":"section"},{"location":"api/cells/mut2cell/","page":"MUT2Cell","title":"MUT2Cell","text":"    MUT2Cell","category":"page"},{"location":"api/cells/mut2cell/#LuxRecurrentLayers.MUT2Cell","page":"MUT2Cell","title":"LuxRecurrentLayers.MUT2Cell","text":"MUT2Cell(in_dims => out_dims;\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nMutated unit 2 cell.\n\nEquations\n\nbeginaligned\n    z = sigma(W_z x_t + U_z h_t + b_z) \n    r = sigma(x_t + U_r h_t + b_r) \n    h_t+1 = tanh(U_h (r odot h_t) + W_h x_t + b_h) odot z \n        quad + h_t odot (1 - z)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Weights to map from input space              W .\nweight_hh: Weights to map from hidden space               w_h \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/trnncell/#TRNNCell","page":"TRNNCell","title":"TRNNCell","text":"","category":"section"},{"location":"api/cells/trnncell/","page":"TRNNCell","title":"TRNNCell","text":"    TRNNCell","category":"page"},{"location":"api/cells/trnncell/#LuxRecurrentLayers.TRNNCell","page":"TRNNCell","title":"LuxRecurrentLayers.TRNNCell","text":"TRNNCell(in_dims => out_dims;\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing,\n    init_state=zeros32)\n\nStrongly typed recurrent unit.\n\nEquations\n\nbeginaligned\n    z_t = mathbfW x_t \n    f_t = sigma (mathbfV x_t + b) \n    h_t = f_t odot h_t-1 + (1 - f_t) odot z_t\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W W_theta  W_eta .\nweight_hh: Concatenated Weights to map from hidden space               W_theta W_eta \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#Cells","page":"Cells","title":"Cells","text":"","category":"section"},{"location":"api/cells/","page":"Cells","title":"Cells","text":"This module provides several recurrent layers. In this page you can access their API documentation from this list:","category":"page"},{"location":"api/cells/","page":"Cells","title":"Cells","text":"AntisymmetricRNNCell\nATRCell\nBRCell\nCFNCell\ncoRNNCell\nFastGRNNCell\nFastRNNCell\nGatedAntisymmetricRNNCell\nIndRNNCell\nJANETCell\nLEMCell\nLightRUCell\nLiGRUCell\nMGUCell\nMinimalRNNCell\nMultiplicativeLSTMCell\nMUT1Cell\nMUT2Cell\nMUT3Cell\nNASCell\nNBRCell\nPeepholeLSTMCell\nRANCell\nSCRNCell\nSGRNCell\nSTARCell\nTGRUCell\nTLSTMCell\nTRNNCell\nUnICORNNCell\nWMCLSTMCell","category":"page"},{"location":"api/cells/lemcell/#LEMCell","page":"LEMCell","title":"LEMCell","text":"","category":"section"},{"location":"api/cells/lemcell/","page":"LEMCell","title":"LEMCell","text":"    LEMCell","category":"page"},{"location":"api/cells/lemcell/#LuxRecurrentLayers.LEMCell","page":"LEMCell","title":"LuxRecurrentLayers.LEMCell","text":"LEMCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32, init_memory=zeros32, dt=1.0)\n\nLong expressive memory unit.\n\nEquations\n\nbeginaligned\n    boldsymbolDelta t(t) = Delta t cdot hatsigma left( \n        mathbfW_ih^1 mathbfx(t) + mathbfb_ih^1 + \n        mathbfW_hh^1 mathbfh(t-1) + mathbfb_hh^1 right) \n    overlineboldsymbolDelta t(t) = Delta t cdot hatsigma left( \n        mathbfW_ih^2 mathbfx(t) + mathbfb_ih^2 + \n        mathbfW_hh^2 mathbfh(t-1) + mathbfb_hh^2 right) \n    mathbfc(t) = left(1 - boldsymbolDelta t(t)right) circ mathbfc(t-1) + \n        boldsymbolDelta t(t) circ sigmaleft( \n        mathbfW_ih^c mathbfx(t) + mathbfb_ih^c + \n        mathbfW_hh^c mathbfh(t-1) + mathbfb_hh^c right) \n    mathbfh(t) = left(1 - boldsymbolDelta t(t)right) circ mathbfh(t-1) + \n        boldsymbolDelta t(t) circ sigmaleft( \n        mathbfW_ih^h mathbfx(t) + mathbfb_ih^h + \n        mathbfW_ch mathbfc(t) + mathbfb_ch right)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for input-to-hidden biases mathbfb_ih^1 mathbfb_ih^2 mathbfb_ih^c mathbfb_ih^h. Must be a tuple of 4 functions, e.g., (glorot_uniform, kaiming_uniform, lecun_normal, zeros). If a single function is passed, it is expanded to a 4-element tuple. If set to nothing, biases are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden-to-hidden biases mathbfb_hh^1 mathbfb_hh^2 mathbfb_hh^c mathbfb_hh^h. Must be a tuple of 3 functions. If a single function is passed, it is expanded to a 3-element tuple. If set to nothing, biases are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_cell_bias: Initializer for bias mathbfb_ch. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input-to-hidden weights mathbfW_ih^1 mathbfW_ih^2 mathbfW_ih^c mathbfW_ih^h. Must be a tuple of 4 functions. If a single function is passed, it is expanded to a 4-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for hidden-to-hidden weights mathbfW_hh^1 mathbfW_hh^2 mathbfW_hh^c mathbfW_hh^h. Must be a tuple of 3 functions. If a single function is passed, it is expanded to a 3-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_cell_weight: Initializer for input to hidden weight mathbfW_ch. If set to\nnothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\ndt: timestep. Defaul is 1.0.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights mapping from input to internal units    mathbfW_ih^1 mathbfW_ih^2 mathbfW_ih^c mathbfW_ih^h    The functions provided in init_weight are applied in order:   the first initializes mathbfW_ih^1, the second mathbfW_ih^2, the third mathbfW_ih^c, and the fourth mathbfW_ih^h.\nweight_hh: Concatenated weights mapping from hidden state to internal units    mathbfW_hh^1 mathbfW_hh^2 mathbfW_hh^c    The functions provided in init_recurrent_weight are applied in order:   the first initializes mathbfW_hh^1, the second mathbfW_hh^2, and the third mathbfW_hh^c.\nweight_ch: Weights to map from cell space mathbfW_ch.\nbias_ih: Concatenated input-to-hidden bias vectors (not present if use_bias=false)    mathbfb_ih^1 mathbfb_ih^2 mathbfb_ih^c mathbfb_ih^h    The functions provided in init_bias are applied in order:   the first initializes mathbfb_ih^1, the second mathbfb_ih^2, the third mathbfb_ih^c, and the fourth mathbfb_ih^h.\nbias_hh: Concatenated hidden-to-hidden bias vectors (not present if use_bias=false)    mathbfb_hh^1 mathbfb_hh^2 mathbfb_hh^c    The functions provided in init_recurrent_bias are applied in order:   the first initializes mathbfb_hh^1, the second mathbfb_hh^2, and the third mathbfb_hh^c.\nbias_ch: Bias vector for the cell-hidden connection mathbfb_ch (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/nbrcell/#NBRCell","page":"NBRCell","title":"NBRCell","text":"","category":"section"},{"location":"api/cells/nbrcell/","page":"NBRCell","title":"NBRCell","text":"    NBRCell","category":"page"},{"location":"api/cells/nbrcell/#LuxRecurrentLayers.NBRCell","page":"NBRCell","title":"LuxRecurrentLayers.NBRCell","text":"NBRCell(in_dims => out_dims;\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nRecurrently neuromodulated bistable recurrent cell.\n\nEquations\n\nbeginaligned\n    mathbfa(t) = 1 + tanhleft(mathbfW_ih^a mathbfx(t) +\n        mathbfb_ih^a + mathbfW_hh^a circ mathbfh(t-1)+\n        mathbfb_hh^a right) \n    mathbfc(t) = sigmaleft(mathbfW_ih^c mathbfx(t) +\n        mathbfb_ih^c + mathbfW_hh^c circ mathbfh(t-1) +\n        mathbfb_hh^c right)\n    mathbfh(t) = mathbfc(t) circ mathbfh(t-1) + (1 - mathbfc(t))\n        circ tanhleft(mathbfW_ih^h mathbfx(t) + mathbfb_ih^h +\n        mathbfa(t) circ mathbfh(t-1)right) \nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for input to hidden bias mathbfb_ih^a mathbfb_ih^c mathbfb_ih^h. Must be a tuple containing 3 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a  3-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden to hidden bias mathbfb_hh^a mathbfb_hh^c. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a  2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input to hidden weights mathbfW_ih^a mathbfW_ih^c mathbfW_ih^h. Must be a tuple containing 3 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 3-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for input to hidden weights mathbfW_hh^a mathbfW_hh^c. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\n- `weight_ih`: Concatenated weights to map from input to the hidden state\n             ``\\{ \\mathbf{W}_{ih}^a, \\mathbf{W}_{ih}^c, \\mathbf{W}_{ih}^h \\}``\nThe initializers in `init_weight` are applied in the order they appear:\nthe first function is used for $\\mathbf{W}_{ih}^a$, the second for $\\mathbf{W}_{ih}^c$,\nand the third for $\\mathbf{W}_{ih}^h$.\n\nweight_hh: Weights to map the hidden state to the hidden state               mathbfW_hh^a mathbfW_hh^c  The initializers in init_weight are applied in the order they appear: the first function is used for mathbfW_hh^a, and the second for mathbfW_hh^c.\nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)               mathbfb_ih^a mathbfb_ih^c mathbfb_ih^h  The initializers in init_bias are applied in the order they appear: the first function is used for mathbfb_ih^z, the second for mathbfb_ih^c, and the third for mathbfb_ih^h.\nbias_hh: Bias vector for the input-hidden connection (not present if use_bias=false)               mathbfb_hh^a mathbfb_hh^c  The initializers in init_bias are applied in the order they appear: the first function is used for mathbfb_hh^z, and the second for mathbfb_hh^c.\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/starcell/#STARCell","page":"STARCell","title":"STARCell","text":"","category":"section"},{"location":"api/cells/starcell/","page":"STARCell","title":"STARCell","text":"    STARCell","category":"page"},{"location":"api/cells/starcell/#LuxRecurrentLayers.STARCell","page":"STARCell","title":"LuxRecurrentLayers.STARCell","text":"STARCell(in_dims => out_dims;\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nStackable recurrent cell.\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W W_theta  W_eta .\nweight_hh: Concatenated Weights to map from hidden space               W_theta W_eta \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/nascell/#NASCell","page":"NASCell","title":"NASCell","text":"","category":"section"},{"location":"api/cells/nascell/","page":"NASCell","title":"NASCell","text":"    NASCell","category":"page"},{"location":"api/cells/nascell/#LuxRecurrentLayers.NASCell","page":"NASCell","title":"LuxRecurrentLayers.NASCell","text":"NASCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32, init_memory=zeros32)\n\nNeural Architecture Search unit.\n\nEquations\n\nbeginaligned\ntextFirst Layer Outputs  \no_1 = sigma(W_i^(1) x_t + W_h^(1) h_t-1 + b^(1)) \no_2 = textReLU(W_i^(2) x_t + W_h^(2) h_t-1 + b^(2)) \no_3 = sigma(W_i^(3) x_t + W_h^(3) h_t-1 + b^(3)) \no_4 = textReLU(W_i^(4) x_t cdot W_h^(4) h_t-1) \no_5 = tanh(W_i^(5) x_t + W_h^(5) h_t-1 + b^(5)) \no_6 = sigma(W_i^(6) x_t + W_h^(6) h_t-1 + b^(6)) \no_7 = tanh(W_i^(7) x_t + W_h^(7) h_t-1 + b^(7)) \no_8 = sigma(W_i^(8) x_t + W_h^(8) h_t-1 + b^(8)) \n\ntextSecond Layer Computations  \nl_1 = tanh(o_1 cdot o_2) \nl_2 = tanh(o_3 + o_4) \nl_3 = tanh(o_5 cdot o_6) \nl_4 = sigma(o_7 + o_8) \n\ntextInject Cell State  \nl_1 = tanh(l_1 + c_textstate) \n\ntextFinal Layer Computations  \nc_textnew = l_1 cdot l_2 \nl_5 = tanh(l_3 + l_4) \nh_textnew = tanh(c_textnew cdot l_5)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_bias: Initializer for recurrent bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W_if W_ic W_ii W_io .\nweight_hh: Concatenated weights to map from hidden space               W_hf W_hc W_hi W_ho \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Concatenated Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/tgrucell/#TGRUCell","page":"TGRUCell","title":"TGRUCell","text":"","category":"section"},{"location":"api/cells/tgrucell/","page":"TGRUCell","title":"TGRUCell","text":"    TGRUCell","category":"page"},{"location":"api/cells/tgrucell/#LuxRecurrentLayers.TGRUCell","page":"TGRUCell","title":"LuxRecurrentLayers.TGRUCell","text":"TGRUCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32, init_memory=zeros32)\n\nStrongly typed gated recurrent unit.\n\nEquations\n\nbeginaligned\n    z_t = mathbfV_z mathbfx_t-1 + mathbfW_z mathbfx_t + mathbfb_z \n    f_t = sigma (mathbfV_f mathbfx_t-1 + mathbfW_f mathbfx_t +\n        mathbfb_f) \n    o_t = tau (mathbfV_o mathbfx_t-1 + mathbfW_o mathbfx_t + mathbfb_o) \n    h_t = f_t odot h_t-1 + z_t odot o_t\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_bias: Initializer for recurrent bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W_if W_ic W_ii W_io .\nweight_hh: Concatenated weights to map from hidden space               W_hf W_hc W_hi W_ho \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Concatenated Bias vector for the hidden-hidden connection (not present if use_bias=false)\nbias_ph: Concatenated Bias vector for the memory-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/unicornncell/#UnICORNNCell","page":"UnICORNNCell","title":"UnICORNNCell","text":"","category":"section"},{"location":"api/cells/unicornncell/","page":"UnICORNNCell","title":"UnICORNNCell","text":"    UnICORNNCell","category":"page"},{"location":"api/cells/unicornncell/#LuxRecurrentLayers.UnICORNNCell","page":"UnICORNNCell","title":"LuxRecurrentLayers.UnICORNNCell","text":"UnICORNNCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32, init_memory=zeros32,\n    dt=1.0, alpha=0.0)\n\nUndamped independent controlled oscillatory recurrent neural unit.\n\nEquations\n\nbeginaligned\n    y_n = y_n-1 + Delta t  hatsigma(c) odot z_n \n    z_n = z_n-1 - Delta t  hatsigma(c) odot left \n        sigma left( w odot y_n-1 + V y_n-1 + b right) + \n        alpha y_n-1 right\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_bias: Initializer for recurrent bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W_if W_ic W_ii W_io .\nweight_hh: Concatenated weights to map from hidden space               W_hf W_hc W_hi W_ho `\nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Concatenated Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = LuxRecurrentLayers","category":"page"},{"location":"#LuxRecurrentLayers","page":"Home","title":"LuxRecurrentLayers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for LuxRecurrentLayers.","category":"page"},{"location":"api/cells/mut1cell/#MUT1Cell","page":"MUT1Cell","title":"MUT1Cell","text":"","category":"section"},{"location":"api/cells/mut1cell/","page":"MUT1Cell","title":"MUT1Cell","text":"    MUT1Cell","category":"page"},{"location":"api/cells/mut1cell/#LuxRecurrentLayers.MUT1Cell","page":"MUT1Cell","title":"LuxRecurrentLayers.MUT1Cell","text":"MUT1Cell(in_dims => out_dims;\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nMutated unit 1 cell.\n\nEquations\n\nbeginaligned\n    z = sigma(W_z x_t + b_z) \n    r = sigma(W_r x_t + U_r h_t + b_r) \n    h_t+1 = tanh(U_h (r odot h_t) + tanh(W_h x_t) + b_h) odot z \n        quad + h_t odot (1 - z)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Weights to map from input space              W .\nweight_hh: Weights to map from hidden space               w_h \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"}]
}
