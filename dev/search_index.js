var documenterSearchIndex = {"docs":
[{"location":"api/cells/#Cells","page":"Cells","title":"Cells","text":"","category":"section"},{"location":"api/cells/","page":"Cells","title":"Cells","text":"    AntisymmetricRNNCell\n    ATRCell\n    BRCell\n    CFNCell\n    coRNNCell\n    FastGRNNCell\n    FastRNNCell\n    GatedAntisymmetricRNNCell\n    IndRNNCell\n    JANETCell\n    LEMCell\n    LightRUCell\n    LiGRUCell\n    MGUCell\n    MinimalRNNCell\n    MultiplicativeLSTMCell\n    MUT1Cell\n    MUT2Cell\n    MUT3Cell\n    NASCell\n    NBRCell\n    PeepholeLSTMCell\n    RANCell\n    SCRNCell\n    SGRNCell\n    STARCell\n    TGRUCell\n    TLSTMCell\n    TRNNCell\n    UnICORNNCell\n    WMCLSTMCell","category":"page"},{"location":"api/cells/#LuxRecurrentLayers.AntisymmetricRNNCell","page":"Cells","title":"LuxRecurrentLayers.AntisymmetricRNNCell","text":"AntisymmetricRNNCell(in_dims => out_dims, [activation];\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_recurrent_bias=nothing, init_weight=nothing,\n    init_recurrent_weight=nothing, init_state=zeros32,\n    epsilon=1.0, gamma=0.0)\n\nAntisymmetric recurrent cell.\n\nEquations\n\nbeginequation\n    mathbfh(t) = mathbfh(t-1) + epsilon cdot tanhleft(\n        mathbfW_ih mathbfx(t) + mathbfb_ih +\n        (mathbfW_hh - mathbfW_hh^top - gamma cdot mathbfI)\n        mathbfh(t-1) + mathbfb_hh right)\nendequation\n\nArguments\n\nin_dims: Input dimension\nout_dims: Output (Hidden State & Memory) dimension\nactivation: activation function. Default is tanh\n\nKeyword arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for bias mathbfb_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_bias: Initializer for recurrent bias mathbfb_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for weight mathbfW_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for recurrent weight mathbfW_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\nepsilon: step size epsilon. Default is 1.0.\ngamma: strength of diffusion gamma. Default is 0.0.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights to map from input to the hidden state mathbfW_ih.\nweight_hh: Concatenated weights to map from hidden to the hidden state mathbfW_hh.\nbias_ih: Bias vector for the input-hidden connection (not present if   use_bias=false) mathbfb_ih.\nbias_hh: Bias vector for the hidden-hidden connection (not present if   use_bias=false) mathbfb_hh.\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.ATRCell","page":"Cells","title":"LuxRecurrentLayers.ATRCell","text":"ATRCell(in_dims => out_dims;\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_recurrent_bias=nothing, init_weight=nothing,\n    init_recurrent_weight=nothing, init_state=zeros32)\n\nAddition-subtraction twin-gated recurrent cell.\n\nEquations\n\nbeginaligned\n    mathbfp(t) = mathbfW_ih mathbfx(t) + mathbfb_ih \n    mathbfq(t) = mathbfW_ih mathbfh(t-1) + mathbfb_hh \n    mathbfi(t) = sigma(mathbfp(t) + mathbfq(t)) \n    mathbff(t) = sigma(mathbfp(t) - mathbfq(t)) \n    mathbfh(t) = mathbfi(t) circ mathbfp(t) + mathbff(t)\n        circ mathbfh(t-1)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for input to hidden bias mathbfb_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden to hidden bias mathbfb_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input to hidden weights mathbfW_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for recurrent weight mathbfW_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights to map from input to the hidden state mathbfW_ih.\nweight_hh: Concatenated weights to map from hidden to the hidden state mathbfW_hh.\nbias_ih: Bias vector for the input-hidden connection (not present if   use_bias=false) mathbfb_ih.\nbias_hh: Bias vector for the hidden-hidden connection (not present if   use_bias=false) mathbfb_hh.\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.BRCell","page":"Cells","title":"LuxRecurrentLayers.BRCell","text":"BRCell(in_dims => out_dims;\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nBistable recurrent cell.\n\nEquations\n\nbeginaligned\n    mathbfa(t) = 1 + tanhleft(mathbfW_ih^a mathbfx(t) +\n        mathbfb_ih^a + mathbfw_hh^a circ mathbfh(t-1) +\n        mathbfb_hh^a right) \n    mathbfc(t) = sigmaleft(mathbfW_ih^c mathbfx(t) +\n        mathbfb_ih^c + mathbfw_hh^c circ mathbfh(t-1) +\n        mathbfb_hh^c right)\n    mathbfh(t) = mathbfc(t) circ mathbfh(t-1) + (1 - mathbfc(t))\n        circ tanhleft(mathbfW_ih^h mathbfx(t) + mathbfb_ih^h +\n        mathbfa(t) circ mathbfh(t-1)right)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for input to hidden bias mathbfb_ih^a mathbfb_ih^c mathbfb_ih^h. Must be a tuple containing 3 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a  3-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden to hidden bias mathbfb_hh^a mathbfb_hh^c. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a  2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input to hidden weights mathbfW_ih^a mathbfW_ih^c mathbfW_ih^h. Must be a tuple containing 3 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 3-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for input to hidden weights mathbfw_hh^a mathbfw_hh^c. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights to map from input to the hidden state               mathbfW_ih^a mathbfW_ih^c mathbfW_ih^h  The initializers in init_weight are applied in the order they appear: the first function is used for mathbfW_ih^a, the second for mathbfW_ih^c, and the third for mathbfW_ih^h.\nweight_hh: Weights to map the hidden state to the hidden state               mathbfw_hh^a mathbfw_hh^c  The initializers in init_weight are applied in the order they appear: the first function is used for mathbfw_hh^a, and the second for mathbfw_hh^c.\nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)               mathbfb_ih^a mathbfb_ih^c mathbfb_ih^h  The initializers in init_bias are applied in the order they appear: the first function is used for mathbfb_ih^z, the second for mathbfb_ih^c, and the third for mathbfb_ih^h.\nbias_hh: Bias vector for the input-hidden connection (not present if use_bias=false)               mathbfb_hh^a mathbfb_hh^c  The initializers in init_bias are applied in the order they appear: the first function is used for mathbfb_hh^z, and the second for mathbfb_hh^c.\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.CFNCell","page":"Cells","title":"LuxRecurrentLayers.CFNCell","text":"CFNCell(in_dims => out_dims, [activation];\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_recurrent_bias=nothing, init_weight=nothing,\n    init_recurrent_weight=nothing, init_state=zeros32)\n\nChaos free network unit.\n\nEquations\n\nbeginaligned\n    boldsymboltheta(t) = sigmaleft(mathbfW_ih^theta mathbfx(t)\n        + mathbfb_ih^theta + mathbfW_hh^theta mathbfh(t-1) +\n        mathbfb_hh^thetaright) \n    boldsymboleta(t) = sigmaleft(mathbfW_ih^eta mathbfx(t) +\n        mathbfb_ih^eta + mathbfW_hh^eta mathbfh(t-1) + \n        mathbfb_hh^eta right) \n    mathbfh(t) = boldsymboltheta(t) circ tanh(mathbfh(t-1)) +\n        boldsymboleta(t) circ tanh(mathbfW_ih^h mathbfx(t) +\n        mathbfb_ih^h)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\nactivation: activation function. Default is tanh\n\nKeyword arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for input to hidden bias mathbfb_ih^theta mathbfb_ih^eta mathbfb_ih^h. Must be a tuple containing 3 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 3-element tuple (fn, fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden to hidden bias mathbfb_hh^theta mathbfb_hh^eta. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input to hidden weights mathbfW_ih^theta mathbfW_ih^eta mathbfW_ih^h. Must be a tuple containing 3 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 3-element tuple (fn, fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for input to hidden weights mathbfW_hh^theta mathbfW_hh^eta. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights to map from input to the hidden state.               mathbfW_ih^theta mathbfW_ih^eta mathbfW_ih^h  The initializers in init_weight are applied in the order they appear: the first function is used for mathbfW_ih^theta, the second for mathbfW_ih^eta, and the third for mathbfW_ih^h.\nweight_hh: Concatenated weights to map from hidden to hidden state.               mathbfW_hh^theta mathbfW_hh^eta  The initializers in init_recurrent_weight are applied in the order they appear: the first function is used for mathbfW_hh^theta, and the second for mathbfW_hh^eta.\nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)               mathbfb_ih^theta mathbfb_ih^eta mathbfb_ih^h  The initializers in init_bias are applied in the order they appear: the first function is used for mathbfb_ih^theta, the second for   mathbfb_ih^eta, and the third for mathbfb_ih^h.\nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)               mathbfb_hh^theta mathbfb_hh^eta  The initializers in init_recurrent_bias are applied in the order they appear: the first function is used for mathbfb_hh^theta, and the second for   mathbfb_hh^eta.\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.coRNNCell","page":"Cells","title":"LuxRecurrentLayers.coRNNCell","text":"coRNNCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing, init_cell_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_cell_weight=nothing, init_state=zeros32, init_memory=zeros32)\n    gamma=0.0, epsilon=0.0, dt=1.0)\n\nCoupled oscillatory recurrent neural unit.\n\nEquations\n\nbeginaligned\n    mathbfc(t) = mathbfc(t-1) + Delta t  sigmaleft( \n        mathbfW_ih mathbfx(t) + mathbfb_ih + \n        mathbfW_hh mathbfh(t-1) + mathbfb_hh + \n        mathbfW_ch mathbfc(t-1) + mathbfb_ch right) \n        - Delta t  gamma  mathbfh(t-1) - Delta t  epsilon \n        mathbfc(t) \n    mathbfh(t) = mathbfh(t-1) + Delta t  mathbfc(t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for input to hidden bias mathbfb_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden to hidden bias mathbfb_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_cell_bias: Initializer for cell to hidden bias mathbfb_ch. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input to hidden weight mathbfW_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for hidden to hidden weight mathbfW_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_cell_weight: Initializer for cell to hidden weight mathbfW_ch. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\ndt: time step. Default is 1.0.\ngamma: Damping for state. Default is 0.0.\nepsilon: Damping for candidate state. Default is 0.0.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Weights to map the input to the hidden state mathbfW_ih.\nweight_hh: Weights to map the hidden state to the hidden state mathbfW_hh.\nweight_ch: Weights to map the cell state to the hidden state mathbfW_ch.\nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false) mathbfb_ih\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false) mathbfb_hh\nbias_ch: Bias vector for the cell-hidden connection (not present if use_bias=false) mathbfb_ch\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.FastGRNNCell","page":"Cells","title":"LuxRecurrentLayers.FastGRNNCell","text":"FastGRNNCell(input_size => hidden_size, [activation];\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_recurrent_bias=nothing, init_weight=nothing,\n    init_recurrent_weight=nothing, init_state=zeros32,\n    init_zeta=1.0, init_nu=4.0)\n\nFast gated recurrent neural network cell.\n\nArguments\n\nin_dims: Input dimension\nout_dims: Output (Hidden State & Memory) dimension\nactivation: activation function. Default is tanh\n\nKeyword arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for input to hidden bias mathbfb_ih^z mathbfb_ih^h. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden to hidden bias mathbfb_hh^z mathbfb_hh^h. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for weight mathbfW_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for recurrent weight mathbfW_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_zeta: initializer for the zeta learnable parameter. Default is 1.0.\ninit_nu: initializer for the nu learnable parameter. Default is 4.0.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = sigmaleft( mathbfW_ih mathbfx(t) +\n        mathbfb_ih^z + mathbfW_hh mathbfh(t-1) +\n        mathbfb_hh^z right) \n    tildemathbfh(t) = tanhleft( mathbfW_ih mathbfx(t) +\n        mathbfb_ih^h + mathbfW_hh mathbfh(t-1) +\n        mathbfb_hh^h right) \n    mathbfh(t) = left( left( zeta (1 - mathbfz(t)) + nu right)\n        circ tildemathbfh(t) right) + mathbfz(t) circ\n        mathbfh(t-1)\nendaligned\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights to map from input to the hidden state mathbfW_ih.\nweight_hh: Concatenated weights to map from hidden to the hidden state mathbfW_hh.\nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)               mathbfb_ih^z mathbfb_ih^h  The initializers in init_bias are applied in the order they appear: the first function is used for mathbfb_ih^z, and the second for mathbfb_ih^h.\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)               mathbfb_hh^z mathbfb_hh^h  The initializers in init_bias are applied in the order they appear: the first function is used for mathbfb_hh^z, and the second for mathbfb_hh^h.\nhidden_state: Initial hidden state vector (not present if train_state=false)\nzeta: Learnable scalar to modulate candidate state.\nnu: Learnable scalar to modulate previous state.\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.FastRNNCell","page":"Cells","title":"LuxRecurrentLayers.FastRNNCell","text":"FastRNNCell(in_dims => out_dims, [activation];\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_recurrent_bias=nothing, init_weight=nothing,\n    init_recurrent_weight=nothing, init_state=zeros32,\n    init_alpha=-3.0, init_beta=3.0)\n\nFast recurrent neural network cell.\n\nArguments\n\nin_dims: Input dimension\nout_dims: Output (Hidden State & Memory) dimension\nactivation: activation function. Default is tanh\n\nKeyword arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for bias mathbfb_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden to hidden bias mathbfb_hh^z mathbfb_hh^h. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for weight mathbfW_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for recurrent weight mathbfW_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_alpha: initializer for the alpha learnable parameter. Default is -3.0.\ninit_beta: initializer for the beta learnable parameter. Default is 3.0.\n\nEquations\n\nbeginaligned\n    tildemathbfh(t) = sigmaleft( mathbfW_ih mathbfx(t) +\n        mathbfb_ih + mathbfW_hh mathbfh(t-1) + mathbfb_hh\n        right) \n    mathbfh(t) = alpha  tildemathbfh(t) + beta \n        mathbfh(t-1)\nendaligned\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights to map from input to the hidden state mathbfW_ih.\nweight_hh: Concatenated weights to map from hidden to the hidden state mathbfW_hh.\nbias_ih: Bias vector for the input-hidden connection (not present if   use_bias=false) mathbfb_ih.\nbias_hh: Bias vector for the hidden-hidden connection (not present if   use_bias=false) mathbfb_hh.\nhidden_state: Initial hidden state vector (not present if train_state=false)\nalpha: Learnable scalar to modulate candidate state.\nbeta: Learnable scalar to modulate previous state.\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.GatedAntisymmetricRNNCell","page":"Cells","title":"LuxRecurrentLayers.GatedAntisymmetricRNNCell","text":"GatedAntisymmetricRNNCell(in_dims => out_dims, [activation];\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_recurrent_bias=nothing, init_weight=nothing,\n    init_recurrent_weight=nothing, init_state=zeros32,\n    epsilon=1.0, gamma=0.0)\n\nAntisymmetric recurrent cell with gating.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = sigmaleft( \n        (mathbfW_hh - mathbfW_hh^top - gamma cdot mathbfI)\n        mathbfh(t-1) + mathbfb_hh + mathbfW_ih^z mathbfx(t)\n        + mathbfb_ih^z right) \n    mathbfh(t) = mathbfh(t-1) + epsilon cdot mathbfz(t) circ\n        tanhleft( (mathbfW_hh - mathbfW_hh^top - gamma cdot\n        mathbfI) mathbfh(t-1) + mathbfb_hh + mathbfW_ih^x\n        mathbfx(t) + mathbfb_ih^h right)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\nactivation: activation function. Default is tanh\n\nKeyword arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for input to hidden bias mathbfb_ih^z mathbfb_ih^h. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden to hidden bias mathbfb_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input to hidden weights mathbfW_ih^z mathbfW_ih^x. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for recurrent weight mathbfW_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\nepsilon: step size. Default is 1.0.\ngamma: strength of diffusion. Default is 0.0.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights to map from input to the hidden state.               mathbfW_ih^z mathbfW_ih^h  The initializers in init_weight are applied in the order they appear: the first function is used for mathbfW_ih^z, and the second for mathbfW_ih^h.\nweight_hh: Weights to map the hidden state to the hidden state mathbfW_hh.\nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)               mathbfb_ih^z mathbfb_ih^h  The initializers in init_bias are applied in the order they appear: the first function is used for mathbfb_ih^z, and the second for mathbfb_ih^h.\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false) mathbfb_hh\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.IndRNNCell","page":"Cells","title":"LuxRecurrentLayers.IndRNNCell","text":"IndRNNCell(in_dims => out_dims, [activation];\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nIndependently recurrent cell.\n\nEquations\n\nmathbfh_t = sigma(mathbfW mathbfx_t + mathbfu odot mathbfh_t-1 +\n    mathbfb)\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n'activation': Activation function. Defaults to tanh_fast\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Weights to map from input space              W .\nweight_hh: Weights to map from hidden space               w_h \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.JANETCell","page":"Cells","title":"LuxRecurrentLayers.JANETCell","text":"JANETCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32, init_memory=zeros32, beta=1.0)\n\nJust another network unit.\n\nEquations\n\nbeginaligned\n    mathbfs_t = mathbfU_f mathbfh_t-1 + mathbfW_f mathbfx_t + mathbfb_f \n    tildemathbfc_t = tanh (mathbfU_c mathbfh_t-1 + mathbfW_c mathbfx_t + mathbfb_c) \n    mathbfc_t = sigma(mathbfs_t) odot mathbfc_t-1 + (1 - sigma (mathbfs_t - beta)) odot tildemathbfc_t \n    mathbfh_t = mathbfc_t\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\nbeta: Control parameter over the input data flow. Default is 1.0.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W_if W_ic .\nweight_hh: Concatenated Weights to map from hidden space               W_hf W_hc \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Concatenated Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.LEMCell","page":"Cells","title":"LuxRecurrentLayers.LEMCell","text":"LEMCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_weight=nothing, init_recurrent_weight=nothing,\n    init_context_weight=nothing, init_state=zeros32, init_memory=zeros32)\n\nLong expressive memory unit.\n\nEquations\n\nbeginaligned\nboldsymbolDelta t_n = Delta hatt hatsigma\n    (W_1 y_n-1 + V_1 u_n + b_1) \noverlineboldsymbolDelta t_n = Delta hatt\n    hatsigma (W_2 y_n-1 + V_2 u_n + b_2) \nz_n = (1 - boldsymbolDelta t_n) odot z_n-1 +\n    boldsymbolDelta t_n odot sigma (W_z y_n-1 + V_z u_n + b_z) \ny_n = (1 - boldsymbolDelta t_n) odot y_n-1 +\n    boldsymbolDelta t_n odot sigma (W_y z_n + V_y u_n + b_y)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_context_weight: Initializer for context weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W_if W_ic .\nweight_hh: Concatenated Weights to map from hidden space               W_hf W_hc \nweight_hh: Concatenated Weights to map from context space               W_cf W_cc \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Concatenated Bias vector for the hidden-hidden connection (not present if use_bias=false)\nalpha: Initial context strength.\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.LightRUCell","page":"Cells","title":"LuxRecurrentLayers.LightRUCell","text":"LightRUCell(in_dims => out_dims, [activation];\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nLight recurrent unit.\n\nEquations\n\nbeginaligned\n    tildeh_t = tanh(W_h x_t) \n    f_t         = delta(W_f x_t + U_f h_t-1 + b_f) \n    h_t         = (1 - f_t) odot h_t-1 + f_t odot tildeh_t\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Weights to map from input space              W .\nweight_hh: Weights to map from hidden space               w_h \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.LiGRUCell","page":"Cells","title":"LuxRecurrentLayers.LiGRUCell","text":"LiGRUCell(in_dims => out_dims, [activation];\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nLight gated recurrent unit.\n\nEquations\n\nbeginaligned\n    z_t = sigma(W_z x_t + U_z h_t-1) \n    tildeh_t = textReLU(W_h x_t + U_h h_t-1) \n    h_t = z_t odot h_t-1 + (1 - z_t) odot tildeh_t\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Weights to map from input space              W .\nweight_hh: Weights to map from hidden space               w_h \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.MGUCell","page":"Cells","title":"LuxRecurrentLayers.MGUCell","text":"MGUCell(in_dims => out_dims;\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nMinimal gated unit.\n\nEquations\n\nbeginaligned\n    f_t         = sigma(W_f x_t + U_f h_t-1 + b_f) \n    tildeh_t = tanh(W_h x_t + U_h (f_t odot h_t-1) + b_h) \n    h_t         = (1 - f_t) odot h_t-1 + f_t odot tildeh_t\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Weights to map from input space              W .\nweight_hh: Weights to map from hidden space               w_h \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.MinimalRNNCell","page":"Cells","title":"LuxRecurrentLayers.MinimalRNNCell","text":"MinimalRNNCell(in_dims => out_dims;\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32,)\n\nMinimal recurrent neural network unit.\n\nEquations\n\nbeginaligned\n    mathbfz_t = Phi(mathbfx_t) = tanh(mathbfW_x mathbfx_t +\n        mathbfb_z) \n    mathbfu_t = sigma(mathbfU_h mathbfh_t-1 + mathbfU_z mathbfz_t +\n        mathbfb_u) \n    mathbfh_t = mathbfu_t circ mathbfh_t-1 + (1 - mathbfu_t) circ\n        mathbfz_t\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword arguments\n\nuse_bias: Set to false to deactivate bias\ntrain_state: Trainable initial hidden state can be activated by setting this to true\ntrain_memory: Trainable initial memory can be activated by setting this to true\ninit_bias: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)).\ninit_weight: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)).\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)).\ninit_state: Initializer for hidden state\ninit_memory: Initializer for memory\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Weights to map from input space.\nweight_hh: Weights to map from hidden space.\nweight_mm: Weights to map from memory space.\nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false).\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false).\nbias_hh: Bias vector for the mnemrory-memory connection (not present if use_bias=false).\nhidden_state: Initial hidden state vector (not present if train_state=false).\nmemory: Initial memory vector (not present if train_memory=false).\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.MultiplicativeLSTMCell","page":"Cells","title":"LuxRecurrentLayers.MultiplicativeLSTMCell","text":"MultiplicativeLSTMCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32, init_memory=zeros32)\n\nMultiplicative long short term memory cell.\n\nEquations\n\nbeginaligned\n    mathbfm_t = (mathbfW_mx mathbfx_t) circ (mathbfW_mh\n        mathbfh_t-1) \n    hatmathbfh_t = mathbfW_hx mathbfx_t + mathbfW_hm\n        mathbfm_t \n    mathbfi_t = sigma(mathbfW_ix mathbfx_t + mathbfW_im\n        mathbfm_t) \n    mathbfo_t = sigma(mathbfW_ox mathbfx_t + mathbfW_om\n        mathbfm_t) \n    mathbff_t = sigma(mathbfW_fx mathbfx_t + mathbfW_fm\n        mathbfm_t) \n    mathbfc_t = mathbff_t circ mathbfc_t-1 + mathbfi_t circ\n        tanh(hatmathbfh_t) \n    mathbfh_t = tanh(mathbfc_t) circ mathbfo_t\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_bias: Initializer for recurrent bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_multiplicative_bias: Initializer for multiplicative bias. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_multiplicative_weight: Initializer for multiplicative weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W_if W_ic W_ii W_io .\nweight_hh: Concatenated weights to map from hidden space               W_hf W_hc W_hi W_ho \nweight_ph: Concatenated weights to map from multiplicative space               W_ff W_fc W_fi \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Concatenated Bias vector for the hidden-hidden connection (not present if use_bias=false)\nbias_ph: Concatenated Bias vector for the multiplicative-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.MUT1Cell","page":"Cells","title":"LuxRecurrentLayers.MUT1Cell","text":"MUT1Cell(in_dims => out_dims;\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nMutated unit 1 cell.\n\nEquations\n\nbeginaligned\n    z = sigma(W_z x_t + b_z) \n    r = sigma(W_r x_t + U_r h_t + b_r) \n    h_t+1 = tanh(U_h (r odot h_t) + tanh(W_h x_t) + b_h) odot z \n        quad + h_t odot (1 - z)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Weights to map from input space              W .\nweight_hh: Weights to map from hidden space               w_h \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.MUT2Cell","page":"Cells","title":"LuxRecurrentLayers.MUT2Cell","text":"MUT2Cell(in_dims => out_dims;\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nMutated unit 2 cell.\n\nEquations\n\nbeginaligned\n    z = sigma(W_z x_t + U_z h_t + b_z) \n    r = sigma(x_t + U_r h_t + b_r) \n    h_t+1 = tanh(U_h (r odot h_t) + W_h x_t + b_h) odot z \n        quad + h_t odot (1 - z)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Weights to map from input space              W .\nweight_hh: Weights to map from hidden space               w_h \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.MUT3Cell","page":"Cells","title":"LuxRecurrentLayers.MUT3Cell","text":"MUT3Cell(in_dims => out_dims;\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nMutated unit 2 cell.\n\nEquations\n\nbeginaligned\n    z = sigma(W_z x_t + U_z h_t + b_z) \n    r = sigma(x_t + U_r h_t + b_r) \n    h_t+1 = tanh(U_h (r odot h_t) + W_h x_t + b_h) odot z \n        quad + h_t odot (1 - z)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Weights to map from input space              W .\nweight_hh: Weights to map from hidden space               w_h \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.NASCell","page":"Cells","title":"LuxRecurrentLayers.NASCell","text":"NASCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32, init_memory=zeros32)\n\nNeural Architecture Search unit.\n\nEquations\n\nbeginaligned\ntextFirst Layer Outputs  \no_1 = sigma(W_i^(1) x_t + W_h^(1) h_t-1 + b^(1)) \no_2 = textReLU(W_i^(2) x_t + W_h^(2) h_t-1 + b^(2)) \no_3 = sigma(W_i^(3) x_t + W_h^(3) h_t-1 + b^(3)) \no_4 = textReLU(W_i^(4) x_t cdot W_h^(4) h_t-1) \no_5 = tanh(W_i^(5) x_t + W_h^(5) h_t-1 + b^(5)) \no_6 = sigma(W_i^(6) x_t + W_h^(6) h_t-1 + b^(6)) \no_7 = tanh(W_i^(7) x_t + W_h^(7) h_t-1 + b^(7)) \no_8 = sigma(W_i^(8) x_t + W_h^(8) h_t-1 + b^(8)) \n\ntextSecond Layer Computations  \nl_1 = tanh(o_1 cdot o_2) \nl_2 = tanh(o_3 + o_4) \nl_3 = tanh(o_5 cdot o_6) \nl_4 = sigma(o_7 + o_8) \n\ntextInject Cell State  \nl_1 = tanh(l_1 + c_textstate) \n\ntextFinal Layer Computations  \nc_textnew = l_1 cdot l_2 \nl_5 = tanh(l_3 + l_4) \nh_textnew = tanh(c_textnew cdot l_5)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_bias: Initializer for recurrent bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W_if W_ic W_ii W_io .\nweight_hh: Concatenated weights to map from hidden space               W_hf W_hc W_hi W_ho \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Concatenated Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.NBRCell","page":"Cells","title":"LuxRecurrentLayers.NBRCell","text":"NBRCell(in_dims => out_dims;\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nRecurrently neuromodulated bistable recurrent cell.\n\nEquations\n\nbeginaligned\n    mathbfa(t) = 1 + tanhleft(mathbfW_ih^a mathbfx(t) +\n        mathbfb_ih^a + mathbfW_hh^a circ mathbfh(t-1)+\n        mathbfb_hh^a right) \n    mathbfc(t) = sigmaleft(mathbfW_ih^c mathbfx(t) +\n        mathbfb_ih^c + mathbfW_hh^c circ mathbfh(t-1) +\n        mathbfb_hh^c right)\n    mathbfh(t) = mathbfc(t) circ mathbfh(t-1) + (1 - mathbfc(t))\n        circ tanhleft(mathbfW_ih^h mathbfx(t) + mathbfb_ih^h +\n        mathbfa(t) circ mathbfh(t-1)right) \nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for input to hidden bias mathbfb_ih^a mathbfb_ih^c mathbfb_ih^h. Must be a tuple containing 3 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a  3-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden to hidden bias mathbfb_hh^a mathbfb_hh^c. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a  2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input to hidden weights mathbfW_ih^a mathbfW_ih^c mathbfW_ih^h. Must be a tuple containing 3 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 3-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for input to hidden weights mathbfW_hh^a mathbfW_hh^c. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\n- `weight_ih`: Concatenated weights to map from input to the hidden state\n             ``\\{ \\mathbf{W}_{ih}^a, \\mathbf{W}_{ih}^c, \\mathbf{W}_{ih}^h \\}``\nThe initializers in `init_weight` are applied in the order they appear:\nthe first function is used for $\\mathbf{W}_{ih}^a$, the second for $\\mathbf{W}_{ih}^c$,\nand the third for $\\mathbf{W}_{ih}^h$.\n\nweight_hh: Weights to map the hidden state to the hidden state               mathbfW_hh^a mathbfW_hh^c  The initializers in init_weight are applied in the order they appear: the first function is used for mathbfW_hh^a, and the second for mathbfW_hh^c.\nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)               mathbfb_ih^a mathbfb_ih^c mathbfb_ih^h  The initializers in init_bias are applied in the order they appear: the first function is used for mathbfb_ih^z, the second for mathbfb_ih^c, and the third for mathbfb_ih^h.\nbias_hh: Bias vector for the input-hidden connection (not present if use_bias=false)               mathbfb_hh^a mathbfb_hh^c  The initializers in init_bias are applied in the order they appear: the first function is used for mathbfb_hh^z, and the second for mathbfb_hh^c.\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.PeepholeLSTMCell","page":"Cells","title":"LuxRecurrentLayers.PeepholeLSTMCell","text":"PeepholeLSTMCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing, init_peephole_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_peephole_weight=nothing, init_state=zeros32, init_memory=zeros32)\n\nPeephole long short term memory.\n\nEquations\n\nbeginaligned\n    z_t = tanh(W_z x_t + U_z h_t-1 + b_z) \n    i_t = sigma(W_i x_t + U_i h_t-1 + p_i odot c_t-1 + b_i) \n    f_t = sigma(W_f x_t + U_f h_t-1 + p_f odot c_t-1 + b_f) \n    c_t = f_t odot c_t-1 + i_t odot z_t \n    o_t = sigma(W_o x_t + U_o h_t-1 + p_o odot c_t + b_o) \n    h_t = o_t odot tanh(c_t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_bias: Initializer for recurrent bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_peephole_bias: Initializer for peephole bias. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_peephole_weight: Initializer for peephole weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W_if W_ic W_ii W_io .\nweight_hh: Concatenated weights to map from hidden space               W_hf W_hc W_hi W_ho \nweight_ph: Concatenated weights to map from peephole space               W_ff W_fc W_fi \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Concatenated Bias vector for the hidden-hidden connection (not present if use_bias=false)\nbias_ph: Concatenated Bias vector for the peephole-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.RANCell","page":"Cells","title":"LuxRecurrentLayers.RANCell","text":"RANCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32, init_memory=zeros32)\n\nRecurrent Additive Network cell.\n\nEquations\n\nbeginaligned\ntildec_t = W_c x_t \ni_t         = sigma(W_i x_t + U_i h_t-1 + b_i) \nf_t         = sigma(W_f x_t + U_f h_t-1 + b_f) \nc_t         = i_t odot tildec_t + f_t odot c_t-1 \nh_t         = g(c_t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_bias: Initializer for recurrent bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W_if W_ic W_ii W_io .\nweight_hh: Concatenated weights to map from hidden space               W_hf W_hc W_hi W_ho \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Concatenated Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.SCRNCell","page":"Cells","title":"LuxRecurrentLayers.SCRNCell","text":"SCRNCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_weight=nothing, init_recurrent_weight=nothing,\n    init_context_weight=nothing, init_state=zeros32, init_memory=zeros32)\n\nStructurally contraint recurrent unit.\n\nEquations\n\nbeginaligned\ns_t = (1 - alpha) W_s x_t + alpha s_t-1 \nh_t = sigma(W_h s_t + U_h h_t-1 + b_h) \ny_t = f(U_y h_t + W_y s_t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_context_weight: Initializer for context weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W_if W_ic .\nweight_hh: Concatenated Weights to map from hidden space               W_hf W_hc \nweight_hh: Concatenated Weights to map from context space               W_cf W_cc \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Concatenated Bias vector for the hidden-hidden connection (not present if use_bias=false)\nalpha: Initial context strength.\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.SGRNCell","page":"Cells","title":"LuxRecurrentLayers.SGRNCell","text":"SGRNCell(in_dims => out_dims;\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nSimple gated recurrent network.\n\nEquations\n\nbeginaligned\n    mathbff_t = sigma(mathbfW mathbfx_t + mathbfU mathbfh_t-1 +\n        mathbfb) \n    mathbfi_t = 1 - mathbff_t \n    mathbfh_t = tanhleft(mathbfi_t circ (mathbfW mathbfx_t) +\n        mathbff_t circ mathbfh_t-1right)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Weights to map from input space              W .\nweight_hh: Weights to map from hidden space               w_h \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.STARCell","page":"Cells","title":"LuxRecurrentLayers.STARCell","text":"STARCell(in_dims => out_dims;\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nStackable recurrent cell.\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W W_theta  W_eta .\nweight_hh: Concatenated Weights to map from hidden space               W_theta W_eta \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.TGRUCell","page":"Cells","title":"LuxRecurrentLayers.TGRUCell","text":"TGRUCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32, init_memory=zeros32)\n\nStrongly typed gated recurrent unit.\n\nEquations\n\nbeginaligned\n    z_t = mathbfV_z mathbfx_t-1 + mathbfW_z mathbfx_t + mathbfb_z \n    f_t = sigma (mathbfV_f mathbfx_t-1 + mathbfW_f mathbfx_t +\n        mathbfb_f) \n    o_t = tau (mathbfV_o mathbfx_t-1 + mathbfW_o mathbfx_t + mathbfb_o) \n    h_t = f_t odot h_t-1 + z_t odot o_t\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_bias: Initializer for recurrent bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W_if W_ic W_ii W_io .\nweight_hh: Concatenated weights to map from hidden space               W_hf W_hc W_hi W_ho \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Concatenated Bias vector for the hidden-hidden connection (not present if use_bias=false)\nbias_ph: Concatenated Bias vector for the memory-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.TLSTMCell","page":"Cells","title":"LuxRecurrentLayers.TLSTMCell","text":"TLSTMCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32, init_memory=zeros32)\n\nStrongly typed long short term memory cell.\n\nEquations\n\nbeginaligned\n    z_t = mathbfV_z mathbfx_t-1 + mathbfW_z mathbfx_t + mathbfb_z \n    f_t = sigma (mathbfV_f mathbfx_t-1 + mathbfW_f mathbfx_t +\n        mathbfb_f) \n    o_t = tau (mathbfV_o mathbfx_t-1 + mathbfW_o mathbfx_t + mathbfb_o) \n    c_t = f_t odot c_t-1 + (1 - f_t) odot z_t \n    h_t = c_t odot o_t\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_bias: Initializer for recurrent bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W_if W_ic W_ii W_io .\nweight_hh: Concatenated weights to map from hidden space               W_hf W_hc W_hi W_ho \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Concatenated Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.TRNNCell","page":"Cells","title":"LuxRecurrentLayers.TRNNCell","text":"TRNNCell(in_dims => out_dims;\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing,\n    init_state=zeros32)\n\nStrongly typed recurrent unit.\n\nEquations\n\nbeginaligned\n    z_t = mathbfW x_t \n    f_t = sigma (mathbfV x_t + b) \n    h_t = f_t odot h_t-1 + (1 - f_t) odot z_t\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W W_theta  W_eta .\nweight_hh: Concatenated Weights to map from hidden space               W_theta W_eta \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.UnICORNNCell","page":"Cells","title":"LuxRecurrentLayers.UnICORNNCell","text":"UnICORNNCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32, init_memory=zeros32,\n    dt=1.0, alpha=0.0)\n\nUndamped independent controlled oscillatory recurrent neural unit.\n\nEquations\n\nbeginaligned\n    y_n = y_n-1 + Delta t  hatsigma(c) odot z_n \n    z_n = z_n-1 - Delta t  hatsigma(c) odot left \n        sigma left( w odot y_n-1 + V y_n-1 + b right) + \n        alpha y_n-1 right\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_bias: Initializer for recurrent bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W_if W_ic W_ii W_io .\nweight_hh: Concatenated weights to map from hidden space               W_hf W_hc W_hi W_ho `\nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Concatenated Bias vector for the hidden-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#LuxRecurrentLayers.WMCLSTMCell","page":"Cells","title":"LuxRecurrentLayers.WMCLSTMCell","text":"WMCLSTMCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing, init_memory_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_memory_weight=nothing, init_state=zeros32, init_memory=zeros32)\n\nLong short term memory cell with working memory connections.\n\nEquations\n\nbeginaligned\n    mathbfi_t = sigmaleft(mathbfW_ix mathbfx_t + mathbfW_ih\n        mathbfh_t-1 + tanh(mathbfW_ic mathbfc_t-1) +\n        mathbfb_iright) \n    mathbff_t = sigmaleft(mathbfW_fx mathbfx_t + mathbfW_fh\n        mathbfh_t-1 + tanh(mathbfW_fc mathbfc_t-1) +\n        mathbfb_fright) \n    mathbfo_t = sigmaleft(mathbfW_ox mathbfx_t + mathbfW_oh\n        mathbfh_t-1 + tanh(mathbfW_oc mathbfc_t) + mathbfb_oright) \n    mathbfc_t = mathbff_t circ mathbfc_t-1 + mathbfi_t circ\n        sigma_c(mathbfW_c mathbfx_t + mathbfb_c) \n    mathbfh_t = mathbfo_t circ sigma_h(mathbfc_t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_bias: Initializer for recurrent bias. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_memory_bias: Initializer for memory bias. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_memory_weight: Initializer for memory weight. Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4 element tuple. If nothing, then we use uniform distribution with bounds -bound and bound where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated Weights to map from input space               W_if W_ic W_ii W_io .\nweight_hh: Concatenated weights to map from hidden space               W_hf W_hc W_hi W_ho \nweight_ph: Concatenated weights to map from memory space               W_ff W_fc W_fi \nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)\nbias_hh: Concatenated Bias vector for the hidden-hidden connection (not present if use_bias=false)\nbias_ph: Concatenated Bias vector for the memory-hidden connection (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = LuxRecurrentLayers","category":"page"},{"location":"#LuxRecurrentLayers","page":"Home","title":"LuxRecurrentLayers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for LuxRecurrentLayers.","category":"page"}]
}
