var documenterSearchIndex = {"docs":
[{"location":"api/cells/mgucell/#MGUCell","page":"MGUCell","title":"MGUCell","text":"","category":"section"},{"location":"api/cells/mgucell/","page":"MGUCell","title":"MGUCell","text":"    MGUCell","category":"page"},{"location":"api/cells/mgucell/#LuxRecurrentLayers.MGUCell","page":"MGUCell","title":"LuxRecurrentLayers.MGUCell","text":"MGUCell(in_dims => out_dims;\n    use_bias=true, train_state=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nMinimal gated unit.\n\nEquations\n\nbeginaligned\n    mathbff(t) = sigmaleft(\n        mathbfW_ih^f mathbfx(t) + mathbfb_ih^f +\n        mathbfW_hh^f mathbfh(t-1) + mathbfb_hh^f right) \n    tildemathbfh(t) = tanhleft(\n        mathbfW_ih^h mathbfx(t) + mathbfb_ih^h +\n        mathbfW_hh^h left( mathbff(t) circ mathbfh(t-1) right) +\n        mathbfb_hh^h right) \n    mathbfh(t) = left(1 - mathbff(t)right) circ mathbfh(t-1) +\n        mathbff(t) circ tildemathbfh(t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for input-to-hidden biases mathbfb_ih^f mathbfb_ih^h. Must be a tuple of 2 functions. If a single function is provided, it is expanded to 2 copies. If set to nothing, biases are initialized from a uniform distribution in [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden-to-hidden biases mathbfb_hh^f mathbfb_hh^h. Must be a tuple of 2 functions. If a single function is provided, it is expanded to 2 copies. If set to nothing, biases are initialized from a uniform distribution in [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input-to-hidden weights mathbfW_ih^f mathbfW_ih^h. Must be a tuple of 2 functions. If a single function is provided, it is expanded to 2 copies. If set to nothing, weights are initialized from a uniform distribution in [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for hidden-to-hidden weights mathbfW_hh^f mathbfW_hh^h. Must be a tuple of 2 functions. If a single function is provided, it is expanded to 2 copies. If set to nothing, weights are initialized from a uniform distribution in [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Input-to-hidden weights  mathbfW_ih^f mathbfW_ih^h  The functions from init_weight are applied in order: the first initializes mathbfW_ih^f, the second mathbfW_ih^h.\nweight_hh: Hidden-to-hidden weights  mathbfW_hh^f mathbfW_hh^h  The functions from init_recurrent_weight are applied in order: the first initializes mathbfW_hh^f, the second mathbfW_hh^h.\nbias_ih: Input-to-hidden biases (if use_bias=true)  mathbfb_ih^f mathbfb_ih^h  The functions from init_bias are applied in order: the first initializes mathbfb_ih^f, the second mathbfb_ih^h.\nbias_hh: Hidden-to-hidden biases (if use_bias=true)  mathbfb_hh^f mathbfb_hh^h  The functions from init_recurrent_bias are applied in order: the first initializes mathbfb_hh^f, the second mathbfb_hh^h.\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/rancell/#RANCell","page":"RANCell","title":"RANCell","text":"","category":"section"},{"location":"api/cells/rancell/","page":"RANCell","title":"RANCell","text":"    RANCell","category":"page"},{"location":"api/cells/rancell/#LuxRecurrentLayers.RANCell","page":"RANCell","title":"LuxRecurrentLayers.RANCell","text":"RANCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32, init_memory=zeros32)\n\nRecurrent Additive Network cell.\n\nEquations\n\nbeginaligned\n    tildemathbfc(t) = mathbfW_ih^c mathbfx(t) +\n        mathbfb_ih^c \n    mathbfi(t) = sigmaleft( mathbfW_ih^i mathbfx(t) +\n        mathbfb_ih^i + mathbfW_hh^i mathbfh(t-1) +\n        mathbfb_hh^i right) \n    mathbff(t) = sigmaleft( mathbfW_ih^f mathbfx(t) +\n        mathbfb_ih^f + mathbfW_hh^f mathbfh(t-1) +\n        mathbfb_hh^f right) \n    mathbfc(t) = mathbfi(t) circ tildemathbfc(t) +\n        mathbff(t) circ mathbfc(t-1) \n    mathbfh(t) = gleft( mathbfc(t) right)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable.   Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable.   Default set to false.\ninit_bias: Initializer for input-to-hidden biases   mathbfb_ih^c mathbfb_ih^i mathbfb_ih^f mathbfb_ih^h.   Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound]   where bound = inv(sqrt(out_dims)). The functions are applied in order:   the first initializes mathbfb_ih^c, the second mathbfb_ih^i,   the third mathbfb_ih^f, and the fourth mathbfb_ih^h.   Default set to nothing.\ninit_recurrent_bias: Initializer for hidden-to-hidden biases   mathbfb_hh^i mathbfb_hh^f mathbfb_hh^h.   Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound]   where bound = inv(sqrt(out_dims)). The functions are applied in order:   the first initializes mathbfb_hh^i, the second mathbfb_hh^f,   and the third mathbfb_hh^h.   Default set to nothing.\ninit_weight: Initializer for input-to-hidden weights   mathbfW_ih^c mathbfW_ih^i mathbfW_ih^f mathbfW_ih^h.   Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound]   where bound = inv(sqrt(out_dims)). The functions are applied in order:   the first initializes mathbfW_ih^c, the second mathbfW_ih^i,   the third mathbfW_ih^f, and the fourth mathbfW_ih^h.   Default set to nothing.\ninit_recurrent_weight: Initializer for hidden-to-hidden weights   mathbfW_hh^i mathbfW_hh^f mathbfW_hh^h.   Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound]   where bound = inv(sqrt(out_dims)). The functions are applied in order:   the first initializes mathbfW_hh^i, the second mathbfW_hh^f,   and the third mathbfW_hh^h.   Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Input-to-hidden weights    mathbfW_ih^c mathbfW_ih^i mathbfW_ih^f mathbfW_ih^h   \nweight_hh: Hidden-to-hidden weights    mathbfW_hh^i mathbfW_hh^f mathbfW_hh^h   \nbias_ih: Input-to-hidden biases (if use_bias=true)    mathbfb_ih^c mathbfb_ih^i mathbfb_ih^f mathbfb_ih^h   \nbias_hh: Hidden-to-hidden biases (if use_bias=true)    mathbfb_hh^i mathbfb_hh^f mathbfb_hh^h   \nhidden_state: Initial hidden state vector (not present if train_state=false)  \nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/tlstmcell/#TLSTMCell","page":"TLSTMCell","title":"TLSTMCell","text":"","category":"section"},{"location":"api/cells/tlstmcell/","page":"TLSTMCell","title":"TLSTMCell","text":"    TLSTMCell","category":"page"},{"location":"api/cells/tlstmcell/#LuxRecurrentLayers.TLSTMCell","page":"TLSTMCell","title":"LuxRecurrentLayers.TLSTMCell","text":"TLSTMCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32, init_memory=zeros32)\n\nStrongly typed long short term memory cell.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = mathbfW_mh^z mathbfx(t-1) + mathbfb_mh^z +\n        mathbfW_ih^z mathbfx(t) + mathbfb_ih^z \n    mathbff(t) = sigmaleft( mathbfW_mh^f mathbfx(t-1) + mathbfb_mh^f +\n        mathbfW_ih^f mathbfx(t) + mathbfb_ih^f right) \n    mathbfo(t) = tauleft( mathbfW_mh^o mathbfx(t-1) + mathbfb_mh^o +\n        mathbfW_ih^o mathbfx(t) + mathbfb_ih^o right) \n    mathbfc(t) = mathbff(t) circ mathbfc(t-1) + left(1 -\n        mathbff(t)right) circ mathbfz(t) \n    mathbfh(t) = mathbfc(t) circ mathbfo(t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for input-to-hidden biases mathbfb_ih^z mathbfb_ih^f mathbfb_ih^o. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3-element tuple. If set to nothing, biases are initialized from a uniform distribution within [-bound, bound], where bound = \\mathrm{inv}(\\sqrt{\\mathrm{out\\_dims}}). The functions are applied in order to initialize mathbfb_ih^z, mathbfb_ih^f, \\mathbf{b}_{ih}^{o}$. Default set to nothing.\ninit_recurrent_bias: Initializer for memory-to-hidden biases mathbfb_mh^z mathbfb_mh^f mathbfb_mh^o. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3-element tuple. If set to nothing, biases are initialized from a uniform distribution within [-bound, bound], where bound = \\mathrm{inv}(\\sqrt{\\mathrm{out\\_dims}}). The functions are applied in order to initialize mathbfb_mh^z, mathbfb_mh^f, \\mathbf{b}_{mh}^{o}$. Default set to nothing.\ninit_weight: Initializer for input-to-hidden weights mathbfW_ih^z mathbfW_ih^f mathbfW_ih^o. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound], where bound = \\mathrm{inv}(\\sqrt{\\mathrm{out\\_dims}}). The functions are applied in order to initialize mathbfW_ih^z, mathbfW_ih^f, \\mathbf{W}_{ih}^{o}$. Default set to nothing.\ninit_recurrent_weight: Initializer for memory-to-hidden weights mathbfW_mh^z mathbfW_mh^f mathbfW_mh^o. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound], where bound = \\mathrm{inv}(\\sqrt{\\mathrm{out\\_dims}}). The functions are applied in order to initialize mathbfW_mh^z, mathbfW_mh^f, \\mathbf{W}_{mh}^{o}$. Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the         updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights for input-to-hidden transformations  mathbfW_ih^z mathbfW_ih^f mathbfW_ih^o .\nweight_mh: Concatenated weights for memory-to-hidden transformations  mathbfW_mh^z mathbfW_mh^f mathbfW_mh^o .\nbias_ih: Concatenated bias vector for input-to-hidden transformations  mathbfb_ih^z mathbfb_ih^f mathbfb_ih^o . Not present if use_bias = false.\nbias_mh: Concatenated bias vector for memory-to-hidden transformations  mathbfb_mh^z mathbfb_mh^f mathbfb_mh^o . Not present if use_bias = false.\nhidden_state: Initial hidden state vector. Not present if train_state = false.\nmemory: Initial memory state vector. Not present if train_memory = false.\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/mut3cell/#MUT3Cell","page":"MUT3Cell","title":"MUT3Cell","text":"","category":"section"},{"location":"api/cells/mut3cell/","page":"MUT3Cell","title":"MUT3Cell","text":"    MUT3Cell","category":"page"},{"location":"api/cells/mut3cell/#LuxRecurrentLayers.MUT3Cell","page":"MUT3Cell","title":"LuxRecurrentLayers.MUT3Cell","text":"MUT3Cell(in_dims => out_dims;\n    use_bias=true, train_state=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nMutated unit 3 cell.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = sigmaleft( mathbfW_ih^z mathbfx(t) +\n        mathbfb_ih^z + mathbfW_hh^z mathbfh(t) +\n        mathbfb_hh^z right) \n    mathbfr(t) = sigmaleft( mathbfx(t) + mathbfW_hh^r\n        mathbfh(t) + mathbfb_hh^r right) \n    mathbfh(t+1) = left tanhleft( mathbfW_hh^h left(\n        mathbfr(t) circ mathbfh(t) + mathbfb_hh^h right) +\n        mathbfW_ih^h mathbfx(t) + mathbfb_ih^h right) right\n        circ mathbfz(t) \n    quad + mathbfh(t) circ left( 1 - mathbfz(t) right)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for input-to-hidden biases mathbfb_ih^z mathbfb_ih^h. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). The functions are applied in order: the first initializes mathbfb_ih^z, and the second mathbfb_ih^h.\ninit_recurrent_bias: Initializer for hidden-to-hidden biases mathbfb_hh^z mathbfb_hh^r mathbfb_hh^h. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). The functions are applied in order: the first initializes mathbfb_hh^z, the second mathbfb_hh^r, and the third mathbfb_hh^h.\ninit_weight: Initializer for input-to-hidden weights mathbfW_ih^z mathbfW_ih^h. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). The functions are applied in order: the first initializes mathbfW_ih^z, and the second mathbfW_ih^h.\ninit_recurrent_weight: Initializer for hidden-to-hidden weights mathbfW_hh^z mathbfW_hh^r mathbfW_hh^h. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). The functions are applied in order: the first initializes mathbfW_hh^z, the second mathbfW_hh^r, and the third mathbfW_hh^h.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Input-to-hidden weights  mathbfW_ih^z mathbfW_ih^h \nweight_hh: Hidden-to-hidden weights  mathbfW_hh^z mathbfW_hh^r mathbfW_hh^h \nbias_ih: Input-to-hidden biases (if use_bias=true)  mathbfb_ih^z mathbfb_ih^h \nbias_hh: Hidden-to-hidden biases (if use_bias=true)  mathbfb_hh^z mathbfb_hh^r mathbfb_hh^h \nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/brcell/#BRCell","page":"BRCell","title":"BRCell","text":"","category":"section"},{"location":"api/cells/brcell/","page":"BRCell","title":"BRCell","text":"    BRCell","category":"page"},{"location":"api/cells/brcell/#LuxRecurrentLayers.BRCell","page":"BRCell","title":"LuxRecurrentLayers.BRCell","text":"BRCell(in_dims => out_dims;\n    use_bias=true, use_recurrent_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nBistable recurrent cell.\n\nEquations\n\nbeginaligned\n    mathbfa(t) = 1 + tanhleft(mathbfW_ih^a mathbfx(t) +\n        mathbfb_ih^a + mathbfw_hh^a circ mathbfh(t-1) +\n        mathbfb_hh^a right) \n    mathbfc(t) = sigmaleft(mathbfW_ih^c mathbfx(t) +\n        mathbfb_ih^c + mathbfw_hh^c circ mathbfh(t-1) +\n        mathbfb_hh^c right)\n    mathbfh(t) = mathbfc(t) circ mathbfh(t-1) + (1 - mathbfc(t))\n        circ tanhleft(mathbfW_ih^h mathbfx(t) + mathbfb_ih^h +\n        mathbfa(t) circ mathbfh(t-1)right)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias mathbfb_ih in the computation. Default set to true.\nuse_recurrent_bias: Flag to use recurrent bias mathbfb_hh in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for input to hidden bias mathbfb_ih^a mathbfb_ih^c mathbfb_ih^h. Must be a tuple containing 3 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 3-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden to hidden bias mathbfb_hh^a mathbfb_hh^c. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input to hidden weights mathbfW_ih^a mathbfW_ih^c mathbfW_ih^h. Must be a tuple containing 3 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 3-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for input to hidden weights mathbfw_hh^a mathbfw_hh^c. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights to map from input to the hidden state               mathbfW_ih^a mathbfW_ih^c mathbfW_ih^h  The initializers in init_weight are applied in the order they appear: the first function is used for mathbfW_ih^a, the second for mathbfW_ih^c, and the third for mathbfW_ih^h.\nweight_hh: Weights to map the hidden state to the hidden state               mathbfw_hh^a mathbfw_hh^c  The initializers in init_weight are applied in the order they appear: the first function is used for mathbfw_hh^a, and the second for mathbfw_hh^c.\nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)               mathbfb_ih^a mathbfb_ih^c mathbfb_ih^h  The initializers in init_bias are applied in the order they appear: the first function is used for mathbfb_ih^z, the second for mathbfb_ih^c, and the third for mathbfb_ih^h.\nbias_hh: Bias vector for the input-hidden connection (not present if use_bias=false)               mathbfb_hh^a mathbfb_hh^c  The initializers in init_bias are applied in the order they appear: the first function is used for mathbfb_hh^z, and the second for mathbfb_hh^c.\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/lightrucell/#LightRUCell","page":"LightRUCell","title":"LightRUCell","text":"","category":"section"},{"location":"api/cells/lightrucell/","page":"LightRUCell","title":"LightRUCell","text":"    LightRUCell","category":"page"},{"location":"api/cells/lightrucell/#LuxRecurrentLayers.LightRUCell","page":"LightRUCell","title":"LuxRecurrentLayers.LightRUCell","text":"LightRUCell(in_dims => out_dims, [activation];\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nLight recurrent unit.\n\nEquations\n\nbeginaligned\n    tildemathbfh(t) = tanhleft( mathbfW_ih^h mathbfx(t) +\n        mathbfb_ih^h right) \n    mathbff(t) = deltaleft( mathbfW_ih^f mathbfx(t) +\n        mathbfb_ih^f + mathbfW_hh^f mathbfh(t-1) +\n        mathbfb_hh^f right) \n    mathbfh(t) = (1 - mathbff(t)) circ mathbfh(t-1) + mathbff(t)\n        circ tildemathbfh(t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for input-to-hidden biases mathbfb_ih^h mathbfb_ih^f. Must be a tuple of 2 functions. If a single function is passed, it is expanded to 2 copies. If set to nothing, each bias is initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden-to-hidden bias mathbfb_hh^f. Must be a single function. If set to nothing, initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input-to-hidden weights mathbfW_ih^h mathbfW_ih^f. Must be a tuple of 2 functions. If a single function is passed, it is expanded to 2 copies. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for hidden-to-hidden weight mathbfW_hh^f. Must be a single function. If set to nothing, initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Input-to-hidden weights  mathbfW_ih^h mathbfW_ih^f  The functions from init_weight are applied in order: the first initializes mathbfW_ih^h, the second mathbfW_ih^f.\nweight_hh: Hidden-to-hidden weight  mathbfW_hh^f  Initialized via init_recurrent_weight.\nbias_ih: Input-to-hidden biases (if use_bias=true)  mathbfb_ih^h mathbfb_ih^f  The functions from init_bias are applied in order: the first initializes mathbfb_ih^h, the second mathbfb_ih^f.\nbias_hh: Hidden-to-hidden bias (if use_bias=true)  mathbfb_hh^f  Initialized via init_recurrent_bias.\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/sgrncell/#SGRNCell","page":"SGRNCell","title":"SGRNCell","text":"","category":"section"},{"location":"api/cells/sgrncell/","page":"SGRNCell","title":"SGRNCell","text":"    SGRNCell","category":"page"},{"location":"api/cells/sgrncell/#LuxRecurrentLayers.SGRNCell","page":"SGRNCell","title":"LuxRecurrentLayers.SGRNCell","text":"SGRNCell(in_dims => out_dims;\n    use_bias=true, train_state=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nSimple gated recurrent network.\n\nEquations\n\nbeginaligned\n    mathbff(t) = sigmaleft(\n        mathbfW_ih mathbfx(t) + mathbfb_ih +\n        mathbfW_hh mathbfh(t-1) + mathbfb_hh right) \n    mathbfi(t) = 1 - mathbff(t) \n    mathbfh(t) = tanhleft(\n        mathbfi(t) circ left( mathbfW_ih mathbfx(t) + mathbfb_ih right) +\n        mathbff(t) circ mathbfh(t-1) right)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for input-to-hidden bias mathbfb_ih. Must be a single function. If set to nothing, bias is initialized from a uniform distribution within [-bound, bound], where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_bias: Initializer for hidden-to-hidden bias mathbfb_hh. Must be a single function. If set to nothing, bias is initialized from a uniform distribution within [-bound, bound], where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for input-to-hidden weight mathbfW_ih. Must be a single function. If set to nothing, weight is initialized from a uniform distribution within [-bound, bound], where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for hidden-to-hidden weight mathbfW_hh. Must be a single function. If set to nothing, weight is initialized from a uniform distribution within [-bound, bound], where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Input-to-hidden weight  mathbfW \nweight_hh: Hidden-to-hidden weight  mathbfU \nbias_ih: Input-to-hidden bias (not present if use_bias=false)  mathbfb \nbias_hh: Hidden-to-hidden bias (not present if use_bias=false)  mathbfb_hh \nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/janetcell/#JANETCell","page":"JANETCell","title":"JANETCell","text":"","category":"section"},{"location":"api/cells/janetcell/","page":"JANETCell","title":"JANETCell","text":"    JANETCell","category":"page"},{"location":"api/cells/janetcell/#LuxRecurrentLayers.JANETCell","page":"JANETCell","title":"LuxRecurrentLayers.JANETCell","text":"JANETCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32, init_memory=zeros32, beta=1.0)\n\nJust another network unit.\n\nEquations\n\nbeginaligned\n    mathbfs(t) = mathbfW_ih^f mathbfx(t) + mathbfb_ih^f +\n        mathbfW_hh^f mathbfh(t-1) + mathbfb_hh^f \n    tildemathbfc(t) = tanhleft( mathbfW_ih^c mathbfx(t) +\n        mathbfb_ih^c + mathbfW_hh^c mathbfh(t-1) +\n        mathbfb_hh^c right) \n    mathbfc(t) = sigma(mathbfs(t)) circ mathbfc(t-1) + left(1 -\n        sigma(mathbfs(t) - beta)right) circ tildemathbfc(t) \n    mathbfh(t) = mathbfc(t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for input-to-hidden biases mathbfb_ih^f and mathbfb_ih^c. Must be a tuple of 2 functions, e.g., (glorot_uniform, kaiming_uniform). If a single function fn is provided, it is expanded to (fn, fn). If set to nothing, each bias is initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden-to-hidden biases mathbfb_hh^f and mathbfb_hh^c. Must be a tuple of 2 functions, e.g., (glorot_uniform, kaiming_uniform). If a single function fn is provided, it is expanded to (fn, fn). If set to nothing, each bias is initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input-to-hidden weights mathbfW_ih^f and mathbfW_ih^c. Must be a tuple of 2 functions, e.g., (glorot_uniform, kaiming_uniform). If a single function fn is provided, it is expanded to (fn, fn). If set to nothing, each weight is initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for hidden-to-hidden weights mathbfW_hh^f and mathbfW_hh^c. Must be a tuple of 2 functions, e.g., (glorot_uniform, kaiming_uniform). If a single function fn is provided, it is expanded to (fn, fn). If set to nothing, each weight is initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\nbeta: Control parameter over the input data flow. Default is 1.0.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the         updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights mapping from input to hidden units  mathbfW_ih^f mathbfW_ih^c  The functions provided in init_weight are applied in order: the first function initializes mathbfW_ih^f, the second initializes mathbfW_ih^c.\nweight_hh: Concatenated weights mapping from hidden state to hidden units  mathbfW_hh^f mathbfW_hh^c  The functions provided in init_recurrent_weight are applied in order: the first function initializes mathbfW_hh^f, the second initializes mathbfW_hh^c.\nbias_ih: Concatenated input-to-hidden bias vectors (if use_bias=true)  mathbfb_ih^f mathbfb_ih^c  The functions provided in init_bias are applied in order: the first function initializes mathbfb_ih^f, the second initializes mathbfb_ih^c.\nbias_hh: Concatenated hidden-to-hidden bias vectors (if use_bias=true)  mathbfb_hh^f mathbfb_hh^c  The functions provided in init_recurrent_bias are applied in order: the first function initializes mathbfb_hh^f, the second initializes mathbfb_hh^c.\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/ligrucell/#LiGRUCell","page":"LiGRUCell","title":"LiGRUCell","text":"","category":"section"},{"location":"api/cells/ligrucell/","page":"LiGRUCell","title":"LiGRUCell","text":"    LiGRUCell","category":"page"},{"location":"api/cells/ligrucell/#LuxRecurrentLayers.LiGRUCell","page":"LiGRUCell","title":"LuxRecurrentLayers.LiGRUCell","text":"LiGRUCell(in_dims => out_dims, [activation];\n    use_bias=true, train_state=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nLight gated recurrent unit.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = sigmaleft( \n        mathbfW_ih^z mathbfx(t) + mathbfb_ih^z + \n        mathbfW_hh^z mathbfh(t-1) + mathbfb_hh^z right) \n    tildemathbfh(t) = textReLUleft( \n        mathbfW_ih^h mathbfx(t) + mathbfb_ih^h + \n        mathbfW_hh^h mathbfh(t-1) + mathbfb_hh^h right) \n    mathbfh(t) = mathbfz(t) circ mathbfh(t-1) + \n        left(1 - mathbfz(t)right) circ tildemathbfh(t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for input-to-hidden biases   mathbfb_ih^z mathbfb_ih^h.   Must be a tuple of 2 functions. If a single function is passed, it is expanded to 2 copies. If set to nothing, each bias is initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden-to-hidden biases   mathbfb_hh^z mathbfb_hh^h.   Must be a tuple of 2 functions. If a single function is passed, it is expanded to 2 copies. If set to nothing, each bias is initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input-to-hidden weights   mathbfW_ih^z mathbfW_ih^h.   Must be a tuple of 2 functions. If a single function is passed, it is expanded to 2 copies. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for hidden-to-hidden weights   mathbfW_hh^z mathbfW_hh^h.   Must be a tuple of 2 functions. If a single function is passed, it is expanded to 2 copies. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Input-to-hidden weights    mathbfW_ih^z mathbfW_ih^h    The functions from init_weight are applied in order:   the first initializes mathbfW_ih^z, the second mathbfW_ih^h.\nweight_hh: Hidden-to-hidden weights    mathbfW_hh^z mathbfW_hh^h    The functions from init_recurrent_weight are applied in order:   the first initializes mathbfW_hh^z, the second mathbfW_hh^h.\nbias_ih: Input-to-hidden biases (if use_bias=true)    mathbfb_ih^z mathbfb_ih^h    The functions from init_bias are applied in order:   the first initializes mathbfb_ih^z, the second mathbfb_ih^h.\nbias_hh: Hidden-to-hidden biases (if use_bias=true)    mathbfb_hh^z mathbfb_hh^h    The functions from init_recurrent_bias are applied in order:   the first initializes mathbfb_hh^z, the second mathbfb_hh^h.\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/atrcell/#ATRCell","page":"ATRCell","title":"ATRCell","text":"","category":"section"},{"location":"api/cells/atrcell/","page":"ATRCell","title":"ATRCell","text":"    ATRCell","category":"page"},{"location":"api/cells/atrcell/#LuxRecurrentLayers.ATRCell","page":"ATRCell","title":"LuxRecurrentLayers.ATRCell","text":"ATRCell(in_dims => out_dims;\n    use_bias=true, use_recurrent bias=true, train_state=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing, init_state=zeros32)\n\nAddition-subtraction twin-gated recurrent cell.\n\nEquations\n\nbeginaligned\n    mathbfp(t) = mathbfW_ih mathbfx(t) + mathbfb_ih \n    mathbfq(t) = mathbfW_ih mathbfh(t-1) + mathbfb_hh \n    mathbfi(t) = sigma(mathbfp(t) + mathbfq(t)) \n    mathbff(t) = sigma(mathbfp(t) - mathbfq(t)) \n    mathbfh(t) = mathbfi(t) circ mathbfp(t) + mathbff(t)\n        circ mathbfh(t-1)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword arguments\n\nuse_bias: Flag to use bias mathbfb_ih in the computation. Default set to true.\nuse_recurrent_bias: Flag to use recurrent bias mathbfb_hh in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for input to hidden bias mathbfb_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden to hidden bias mathbfb_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input to hidden weights mathbfW_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for recurrent weight mathbfW_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights to map from input to the hidden state mathbfW_ih.\nweight_hh: Concatenated weights to map from hidden to the hidden state mathbfW_hh.\nbias_ih: Bias vector for the input-hidden connection (not present if   use_bias=false) mathbfb_ih.\nbias_hh: Bias vector for the hidden-hidden connection (not present if   use_bias=false) mathbfb_hh.\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/peepholelstmcell/#PeepholeLSTMCell","page":"PeepholeLSTMCell","title":"PeepholeLSTMCell","text":"","category":"section"},{"location":"api/cells/peepholelstmcell/","page":"PeepholeLSTMCell","title":"PeepholeLSTMCell","text":"    PeepholeLSTMCell","category":"page"},{"location":"api/cells/peepholelstmcell/#LuxRecurrentLayers.PeepholeLSTMCell","page":"PeepholeLSTMCell","title":"LuxRecurrentLayers.PeepholeLSTMCell","text":"PeepholeLSTMCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing, init_peephole_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_peephole_weight=nothing, init_state=zeros32, init_memory=zeros32)\n\nPeephole long short term memory.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = tanhleft(\n        mathbfW_ih^z mathbfx(t) + mathbfb_ih^z +\n        mathbfW_hh^z mathbfh(t-1) + mathbfb_hh^z\n        right) \n    mathbfi(t) = sigmaleft(\n        mathbfW_ih^i mathbfx(t) + mathbfb_ih^i +\n        mathbfW_hh^i mathbfh(t-1) + mathbfb_hh^i +\n        mathbfp^i circ mathbfc(t-1) + mathbfb_ph^i\n        right) \n    mathbff(t) = sigmaleft(\n        mathbfW_ih^f mathbfx(t) + mathbfb_ih^f +\n        mathbfW_hh^f mathbfh(t-1) + mathbfb_hh^f +\n        mathbfp^f circ mathbfc(t-1) + mathbfb_ph^f\n        right) \n    mathbfc(t) = mathbff(t) circ mathbfc(t-1) + mathbfi(t) circ\n        mathbfz(t) \n    mathbfo(t) = sigmaleft(\n        mathbfW_ih^o mathbfx(t) + mathbfb_ih^o +\n        mathbfW_hh^o mathbfh(t-1) + mathbfb_hh^o +\n        mathbfp^o circ mathbfc(t) + mathbfb_ph^o\n        right) \n    mathbfh(t) = mathbfo(t) circ tanhleft( mathbfc(t) right)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable.   Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable.   Default set to false.\ninit_bias: Initializer for input-to-hidden biases   mathbfb_ih^z mathbfb_ih^i mathbfb_ih^f mathbfb_ih^o.   Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound], where bound = inv(sqrt(out_dims)).   The functions are applied in order: the first initializes mathbfb_ih^z, the second mathbfb_ih^i, the third mathbfb_ih^f, and the fourth mathbfb_ih^o. Default set to nothing.\ninit_recurrent_bias: Initializer for hidden-to-hidden biases   mathbfb_hh^z mathbfb_hh^i mathbfb_hh^f mathbfb_hh^o.   Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound], where bound = inv(sqrt(out_dims)). The functions are applied in order:   the first initializes mathbfb_hh^z, the second mathbfb_hh^i,   the third mathbfb_hh^f, and the fourth mathbfb_hh^o.   Default set to nothing.\ninit_peephole_bias: Initializer for peephole biases   mathbfb_ph^i mathbfb_ph^f mathbfb_ph^o.   Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound],  where bound = inv(sqrt(out_dims)). The functions are applied in order:   the first initializes mathbfb_ph^i, the second mathbfb_ph^f,   and the third mathbfb_ph^o.   Default set to nothing.\ninit_weight: Initializer for input-to-hidden weights   mathbfW_ih^z mathbfW_ih^i mathbfW_ih^f mathbfW_ih^o.   Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound],   where bound = inv(sqrt(out_dims)). The functions are applied in order:   the first initializes mathbfW_ih^z, the second mathbfW_ih^i,   the third mathbfW_ih^f, and the fourth mathbfW_ih^o.   Default set to nothing.\ninit_recurrent_weight: Initializer for hidden-to-hidden weights   mathbfW_hh^z mathbfW_hh^i mathbfW_hh^f mathbfW_hh^o.   Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4-element tuple. If set to nothing, weights are initialized  from a uniform distribution within [-bound, bound],   where bound = inv(sqrt(out_dims)). The functions are applied in order:   the first initializes mathbfW_hh^z, the second mathbfW_hh^i,   the third mathbfW_hh^f, and the fourth mathbfW_hh^o.   Default set to nothing.\ninit_peephole_weight: Initializer for peephole weights   mathbfp^i mathbfp^f mathbfp^o.   Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound],   where bound = inv(sqrt(out_dims)). The functions are applied in order:   the first initializes mathbfp^i, the second mathbfp^f,   and the third mathbfp^o.   Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Input-to-hidden weights    mathbfW_ih^z mathbfW_ih^i mathbfW_ih^f mathbfW_ih^o   \nweight_hh: Hidden-to-hidden weights    mathbfW_hh^z mathbfW_hh^i mathbfW_hh^f mathbfW_hh^o   \nweight_ph: Peephole weights    mathbfp^i mathbfp^f mathbfp^o   \nbias_ih: Input-to-hidden biases (if use_bias=true)    mathbfb_ih^z mathbfb_ih^i mathbfb_ih^f mathbfb_ih^o   \nbias_hh: Hidden-to-hidden biases (if use_bias=true)    mathbfb_hh^z mathbfb_hh^i mathbfb_hh^f mathbfb_hh^o   \nbias_ph: Peephole biases (if use_bias=true)    mathbfb_ph^i mathbfb_ph^f mathbfb_ph^o   \nhidden_state: Initial hidden state vector (not present if train_state=false)  \nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/indrnncell/#IndRNNCell","page":"IndRNNCell","title":"IndRNNCell","text":"","category":"section"},{"location":"api/cells/indrnncell/","page":"IndRNNCell","title":"IndRNNCell","text":"    IndRNNCell","category":"page"},{"location":"api/cells/indrnncell/#LuxRecurrentLayers.IndRNNCell","page":"IndRNNCell","title":"LuxRecurrentLayers.IndRNNCell","text":"IndRNNCell(in_dims => out_dims, [activation];\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nIndependently recurrent cell.\n\nEquations\n\nbeginequation\n    mathbfh(t) = sigmaleft( mathbfW_ih mathbfx(t) + mathbfb_ih +\n        mathbfw_hh circ mathbfh(t-1) + mathbfb_hh right)\nendequation\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n'activation': Activation function. Defaults to tanh_fast\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for bias mathbfb_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for bias mathbfb_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input to hidden weight mathbfW_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for hidden to hidden weight mathbfw_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Weights to map from input space mathbfW_ih.\nweight_hh: Weights to map from hidden space mathbfW_hh.\nbias_ih: Bias vector for the input-hidden connection mathbfb_ih (not present if use_bias=false)\nbias_hh: Bias vector for the hidden-hidden connection mathbfb_hh (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/fastgrnncell/#FastGRNNCell","page":"FastGRNNCell","title":"FastGRNNCell","text":"","category":"section"},{"location":"api/cells/fastgrnncell/","page":"FastGRNNCell","title":"FastGRNNCell","text":"    FastGRNNCell","category":"page"},{"location":"api/cells/fastgrnncell/#LuxRecurrentLayers.FastGRNNCell","page":"FastGRNNCell","title":"LuxRecurrentLayers.FastGRNNCell","text":"FastGRNNCell(input_size => hidden_size, [activation];\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_recurrent_bias=nothing, init_weight=nothing,\n    init_recurrent_weight=nothing, init_state=zeros32,\n    init_zeta=1.0, init_nu=4.0)\n\nFast gated recurrent neural network cell.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = sigmaleft( mathbfW_ih mathbfx(t) +\n        mathbfb_ih^z + mathbfW_hh mathbfh(t-1) +\n        mathbfb_hh^z right) \n    tildemathbfh(t) = tanhleft( mathbfW_ih mathbfx(t) +\n        mathbfb_ih^h + mathbfW_hh mathbfh(t-1) +\n        mathbfb_hh^h right) \n    mathbfh(t) = left( left( sigma(zeta) (1 - mathbfz(t)) +\n        sigma(nu) right)\n        circ tildemathbfh(t) right) + mathbfz(t) circ\n        mathbfh(t-1)\nendaligned\n\nArguments\n\nin_dims: Input dimension\nout_dims: Output (Hidden State & Memory) dimension\nactivation: activation function. Default is tanh\n\nKeyword arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for input to hidden bias mathbfb_ih^z mathbfb_ih^h. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden to hidden bias mathbfb_hh^z mathbfb_hh^h. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for weight mathbfW_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for recurrent weight mathbfW_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_zeta: initializer for the zeta learnable parameter. Default is 1.0.\ninit_nu: initializer for the nu learnable parameter. Default is 4.0.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights to map from input to the hidden state mathbfW_ih.\nweight_hh: Concatenated weights to map from hidden to the hidden state mathbfW_hh.\nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)               mathbfb_ih^z mathbfb_ih^h  The initializers in init_bias are applied in the order they appear: the first function is used for mathbfb_ih^z, and the second for mathbfb_ih^h.\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false)               mathbfb_hh^z mathbfb_hh^h  The initializers in init_bias are applied in the order they appear: the first function is used for mathbfb_hh^z, and the second for mathbfb_hh^h.\nhidden_state: Initial hidden state vector (not present if train_state=false)\nzeta: Learnable scalar to modulate candidate state.\nnu: Learnable scalar to modulate previous state.\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/gatedantisymmetricrnncell/#GatedAntisymmetricRNNCell","page":"GatedAntisymmetricRNNCell","title":"GatedAntisymmetricRNNCell","text":"","category":"section"},{"location":"api/cells/gatedantisymmetricrnncell/","page":"GatedAntisymmetricRNNCell","title":"GatedAntisymmetricRNNCell","text":"    GatedAntisymmetricRNNCell","category":"page"},{"location":"api/cells/gatedantisymmetricrnncell/#LuxRecurrentLayers.GatedAntisymmetricRNNCell","page":"GatedAntisymmetricRNNCell","title":"LuxRecurrentLayers.GatedAntisymmetricRNNCell","text":"GatedAntisymmetricRNNCell(in_dims => out_dims, [activation];\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_recurrent_bias=nothing, init_weight=nothing,\n    init_recurrent_weight=nothing, init_state=zeros32,\n    epsilon=1.0, gamma=0.0)\n\nAntisymmetric recurrent cell with gating.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = sigmaleft(\n        (mathbfW_hh - mathbfW_hh^top - gamma cdot mathbfI)\n        mathbfh(t-1) + mathbfb_hh + mathbfW_ih^z mathbfx(t)\n        + mathbfb_ih^z right) \n    mathbfh(t) = mathbfh(t-1) + epsilon cdot mathbfz(t) circ\n        tanhleft( (mathbfW_hh - mathbfW_hh^top - gamma cdot\n        mathbfI) mathbfh(t-1) + mathbfb_hh + mathbfW_ih^x\n        mathbfx(t) + mathbfb_ih^h right)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\nactivation: activation function. Default is tanh\n\nKeyword arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for input to hidden bias mathbfb_ih^z mathbfb_ih^h. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden to hidden bias mathbfb_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input to hidden weights mathbfW_ih^z mathbfW_ih^x. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for recurrent weight mathbfW_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\nepsilon: step size. Default is 1.0.\ngamma: strength of diffusion. Default is 0.0.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights to map from input to the hidden state.               mathbfW_ih^z mathbfW_ih^h  The initializers in init_weight are applied in the order they appear: the first function is used for mathbfW_ih^z, and the second for mathbfW_ih^h.\nweight_hh: Weights to map the hidden state to the hidden state mathbfW_hh.\nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)               mathbfb_ih^z mathbfb_ih^h  The initializers in init_bias are applied in the order they appear: the first function is used for mathbfb_ih^z, and the second for mathbfb_ih^h.\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false) mathbfb_hh\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/cfncell/#CFNCell","page":"CFNCell","title":"CFNCell","text":"","category":"section"},{"location":"api/cells/cfncell/","page":"CFNCell","title":"CFNCell","text":"    CFNCell","category":"page"},{"location":"api/cells/cfncell/#LuxRecurrentLayers.CFNCell","page":"CFNCell","title":"LuxRecurrentLayers.CFNCell","text":"CFNCell(in_dims => out_dims, [activation];\n    use_bias=true, use_recurrent_bias=true, train_state=false, init_bias=nothing,\n    init_recurrent_bias=nothing, init_weight=nothing,\n    init_recurrent_weight=nothing, init_state=zeros32)\n\nChaos free network unit.\n\nEquations\n\nbeginaligned\n    boldsymboltheta(t) = sigmaleft(mathbfW_ih^theta mathbfx(t)\n        + mathbfb_ih^theta + mathbfW_hh^theta mathbfh(t-1) +\n        mathbfb_hh^thetaright) \n    boldsymboleta(t) = sigmaleft(mathbfW_ih^eta mathbfx(t) +\n        mathbfb_ih^eta + mathbfW_hh^eta mathbfh(t-1) +\n        mathbfb_hh^eta right) \n    mathbfh(t) = boldsymboltheta(t) circ tanh(mathbfh(t-1)) +\n        boldsymboleta(t) circ tanh(mathbfW_ih^h mathbfx(t) +\n        mathbfb_ih^h)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\nactivation: activation function. Default is tanh\n\nKeyword arguments\n\nuse_bias: Flag to use bias mathbfb_ih in the computation. Default set to true.\nuse_recurrent_bias: Flag to use recurrent bias mathbfb_hh in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for input to hidden bias mathbfb_ih^theta mathbfb_ih^eta mathbfb_ih^h. Must be a tuple containing 3 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 3-element tuple (fn, fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden to hidden bias mathbfb_hh^theta mathbfb_hh^eta. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input to hidden weights mathbfW_ih^theta mathbfW_ih^eta mathbfW_ih^h. Must be a tuple containing 3 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 3-element tuple (fn, fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for input to hidden weights mathbfW_hh^theta mathbfW_hh^eta. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights to map from input to the hidden state.               mathbfW_ih^theta mathbfW_ih^eta mathbfW_ih^h  The initializers in init_weight are applied in the order they appear: the first function is used for mathbfW_ih^theta, the second for mathbfW_ih^eta, and the third for mathbfW_ih^h.\nweight_hh: Concatenated weights to map from hidden to hidden state.               mathbfW_hh^theta mathbfW_hh^eta  The initializers in init_recurrent_weight are applied in the order they appear: the first function is used for mathbfW_hh^theta, and the second for mathbfW_hh^eta.\nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)               mathbfb_ih^theta mathbfb_ih^eta mathbfb_ih^h  The initializers in init_bias are applied in the order they appear: the first function is used for mathbfb_ih^theta, the second for   mathbfb_ih^eta, and the third for mathbfb_ih^h.\nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)               mathbfb_hh^theta mathbfb_hh^eta  The initializers in init_recurrent_bias are applied in the order they appear: the first function is used for mathbfb_hh^theta, and the second for   mathbfb_hh^eta.\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/cornncell/#coRNNCell","page":"coRNNCell","title":"coRNNCell","text":"","category":"section"},{"location":"api/cells/cornncell/","page":"coRNNCell","title":"coRNNCell","text":"    coRNNCell","category":"page"},{"location":"api/cells/cornncell/#LuxRecurrentLayers.coRNNCell","page":"coRNNCell","title":"LuxRecurrentLayers.coRNNCell","text":"coRNNCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing, init_cell_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_cell_weight=nothing, init_state=zeros32, init_memory=zeros32,\n    gamma=0.0, epsilon=0.0, dt=1.0)\n\nCoupled oscillatory recurrent neural unit.\n\nEquations\n\nbeginaligned\n    mathbfc(t) = mathbfc(t-1) + Delta t  sigmaleft(\n        mathbfW_ih mathbfx(t) + mathbfb_ih +\n        mathbfW_hh mathbfh(t-1) + mathbfb_hh +\n        mathbfW_ch mathbfc(t-1) + mathbfb_ch right)\n        - Delta t  gamma  mathbfh(t-1) - Delta t  epsilon \n        mathbfc(t) \n    mathbfh(t) = mathbfh(t-1) + Delta t  mathbfc(t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for input to hidden bias mathbfb_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden to hidden bias mathbfb_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_cell_bias: Initializer for cell to hidden bias mathbfb_ch. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input to hidden weight mathbfW_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for hidden to hidden weight mathbfW_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_cell_weight: Initializer for cell to hidden weight mathbfW_ch. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\ndt: time step. Default is 1.0.\ngamma: Damping for state. Default is 0.0.\nepsilon: Damping for candidate state. Default is 0.0.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the         updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Weights to map the input to the hidden state mathbfW_ih.\nweight_hh: Weights to map the hidden state to the hidden state mathbfW_hh.\nweight_ch: Weights to map the cell state to the hidden state mathbfW_ch.\nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false) mathbfb_ih\nbias_hh: Bias vector for the hidden-hidden connection (not present if use_bias=false) mathbfb_hh\nbias_ch: Bias vector for the cell-hidden connection (not present if use_bias=false) mathbfb_ch\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/fastrnncell/#FastRNNCell","page":"FastRNNCell","title":"FastRNNCell","text":"","category":"section"},{"location":"api/cells/fastrnncell/","page":"FastRNNCell","title":"FastRNNCell","text":"    FastRNNCell","category":"page"},{"location":"api/cells/fastrnncell/#LuxRecurrentLayers.FastRNNCell","page":"FastRNNCell","title":"LuxRecurrentLayers.FastRNNCell","text":"FastRNNCell(in_dims => out_dims, [activation];\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_recurrent_bias=nothing, init_weight=nothing,\n    init_recurrent_weight=nothing, init_state=zeros32,\n    init_alpha=-3.0, init_beta=3.0)\n\nFast recurrent neural network cell.\n\nEquations\n\nbeginaligned\n    tildemathbfh(t) = sigmaleft( mathbfW_ih mathbfx(t) +\n        mathbfb_ih + mathbfW_hh mathbfh(t-1) + mathbfb_hh\n        right) \n    mathbfh(t) = sigma(alpha)  tildemathbfh(t) + sigma(beta) \n        mathbfh(t-1)\nendaligned\n\nArguments\n\nin_dims: Input dimension\nout_dims: Output (Hidden State & Memory) dimension\nactivation: activation function. Default is tanh\n\nKeyword arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for bias mathbfb_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden to hidden bias mathbfb_hh^z mathbfb_hh^h. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for weight mathbfW_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for recurrent weight mathbfW_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_alpha: initializer for the alpha learnable parameter. Default is -3.0.\ninit_beta: initializer for the beta learnable parameter. Default is 3.0.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights to map from input to the hidden state mathbfW_ih.\nweight_hh: Concatenated weights to map from hidden to the hidden state mathbfW_hh.\nbias_ih: Bias vector for the input-hidden connection (not present if   use_bias=false) mathbfb_ih.\nbias_hh: Bias vector for the hidden-hidden connection (not present if   use_bias=false) mathbfb_hh.\nhidden_state: Initial hidden state vector (not present if train_state=false)\nalpha: Learnable scalar to modulate candidate state.\nbeta: Learnable scalar to modulate previous state.\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/multiplicativelstmcell/#MinimalRNNCell","page":"MinimalRNNCell","title":"MinimalRNNCell","text":"","category":"section"},{"location":"api/cells/multiplicativelstmcell/","page":"MinimalRNNCell","title":"MinimalRNNCell","text":"    MinimalRNNCell","category":"page"},{"location":"api/cells/multiplicativelstmcell/#LuxRecurrentLayers.MinimalRNNCell","page":"MinimalRNNCell","title":"LuxRecurrentLayers.MinimalRNNCell","text":"MinimalRNNCell(in_dims => out_dims;\n    use_bias=true, train_state=false,\n    init_encoder_bias=nothing, init_recurrent_bias=nothing,\n    init_memory_bias=nothing, init_encoder_weight=nothing,\n    init_recurrent_weight=nothing, init_memory_weight=nothing,\n    init_state=zeros32,)\n\nMinimal recurrent neural network unit.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = tanhleft( mathbfW_ih^z mathbfx(t) +\n        mathbfb_ih^z right) \n    mathbfu(t) = sigmaleft( mathbfW_hh^u mathbfh(t-1) +\n        mathbfW_zh^u mathbfz(t) + mathbfb_hh^u right) \n    mathbfh(t) = mathbfu(t) circ mathbfh(t-1) + left(1 -\n        mathbfu(t)right) circ mathbfz(t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword arguments\n\nuse_bias: Set to false to deactivate bias\ntrain_state: Trainable initial hidden state can be activated by setting this to true\ntrain_memory: Trainable initial memory can be activated by setting this to true\ninit_encoder_bias: Initializer for encoder bias mathbfb_ih^z. Must be a single function. If nothing, initialized from a uniform distribution in [-bound, bound] where bound = inv(sqrt(out_dims)).\ninit_recurrent_bias: Initializer for recurrent bias mathbfb_hh^u. Must be a single function. If nothing, initialized from a uniform distribution in [-bound, bound] where bound = inv(sqrt(out_dims)).\ninit_memory_bias: Initializer for memory bias mathbfb_zh^u. Must be a single function. If nothing, initialized from a uniform distribution in [-bound, bound] where bound = inv(sqrt(out_dims)).\ninit_encoder_weight: Initializer for encoder weight mathbfW_ih^z. Must be a single function. If nothing, initialized from a uniform distribution in [-bound, bound] where bound = inv(sqrt(out_dims)).\ninit_recurrent_weight: Initializer for recurrent weight mathbfW_hh^u. Must be a single function. If nothing, initialized from a uniform distribution in [-bound, bound] where bound = inv(sqrt(out_dims)).\ninit_memory_weight: Initializer for memory weight mathbfW_zh^u. Must be a single function. If nothing, initialized from a uniform distribution in [-bound, bound] where bound = inv(sqrt(out_dims)).\ninit_state: Initializer for hidden state\ninit_memory: Initializer for memory\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the         updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Encoder weights  mathbfW_ih^z \nweight_hh: Recurrent weights  mathbfW_hh^u \nweight_mm: Memory weights  mathbfW_zh^u \nbias_ih: Encoder bias (if use_bias=true)  mathbfb_ih^z \nbias_hh: Recurrent bias (if use_bias=true)  mathbfb_hh^u \nbias_mm: Memory bias (if use_bias=true)  mathbfb_zh^u \nhidden_state: Initial hidden state vector mathbfh(0) (not present if train_state=false).\nmemory: Initial memory vector mathbfc(0) (not present if train_memory=false).\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/wmclstmcell/#WMCLSTMCell","page":"WMCLSTMCell","title":"WMCLSTMCell","text":"","category":"section"},{"location":"api/cells/wmclstmcell/","page":"WMCLSTMCell","title":"WMCLSTMCell","text":"    WMCLSTMCell","category":"page"},{"location":"api/cells/wmclstmcell/#LuxRecurrentLayers.WMCLSTMCell","page":"WMCLSTMCell","title":"LuxRecurrentLayers.WMCLSTMCell","text":"WMCLSTMCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing, init_memory_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_memory_weight=nothing, init_state=zeros32, init_memory=zeros32)\n\nLong short term memory cell with working memory connections.\n\nEquations\n\nbeginaligned\n    mathbfi(t) = sigmaleft( mathbfW_ih^i mathbfx(t) +\n        mathbfb_ih^i + mathbfW_hh^i mathbfh(t-1) +\n        mathbfb_hh^i + mathbfW_mh^i mathbfc(t-1) +\n        mathbfb_mh^i right) \n    mathbff(t) = sigmaleft( mathbfW_ih^f mathbfx(t) +\n        mathbfb_ih^f + mathbfW_hh^f mathbfh(t-1) +\n        mathbfb_hh^f + mathbfW_mh^f mathbfc(t-1) +\n        mathbfb_mh^f right) \n    mathbfc(t) = mathbff(t) circ mathbfc(t-1) + mathbfi(t) circ\n        sigma_cleft( mathbfW_ih^c mathbfx(t) + mathbfb_ih^c\n        right) \n    mathbfo(t) = sigmaleft( mathbfW_ih^o mathbfx(t) +\n        mathbfb_ih^o + mathbfW_hh^o mathbfh(t-1) +\n        mathbfb_hh^o + mathbfW_mh^o mathbfc(t) +\n        mathbfb_mh^o right) \n    mathbfh(t) = mathbfo(t) circ sigma_hleft( mathbfc(t) right)\nendaligned\n\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable.   Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable.   Default set to false.\ninit_bias: Initializer for input-to-hidden biases   mathbfb_ih^i mathbfb_ih^f mathbfb_ih^c mathbfb_ih^o.   Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4-element tuple. If set to nothing, biases are initialized from a uniform distribution within [-bound, bound],   where bound = \\mathrm{inv}(\\sqrt{\\mathrm{out\\_dims}}).   The functions are applied in order:   the first initializes mathbfb_ih^i, the second mathbfb_ih^f,   the third mathbfb_ih^c, the fourth mathbfb_ih^o.   Default set to nothing.\ninit_recurrent_bias: Initializer for hidden-to-hidden biases   mathbfb_hh^i mathbfb_hh^f mathbfb_hh^o.   Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3-element tuple. If set to nothing, biases are initialized from a uniform distribution within [-bound, bound],   where bound = \\mathrm{inv}(\\sqrt{\\mathrm{out\\_dims}}).   The functions are applied in order:   the first initializes mathbfb_hh^i, the second mathbfb_hh^f,   the third mathbfb_hh^o.   Default set to nothing.\ninit_memory_bias: Initializer for memory-to-hidden biases   mathbfb_mh^i mathbfb_mh^f mathbfb_mh^o.   Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3-element tuple. If set to nothing, biases are initialized from a uniform distribution within [-bound, bound],   where bound = \\mathrm{inv}(\\sqrt{\\mathrm{out\\_dims}}).   The functions are applied in order:   the first initializes mathbfb_mh^i, the second mathbfb_mh^f,   the third mathbfb_mh^o.   Default set to nothing.\ninit_weight: Initializer for input-to-hidden weights   mathbfW_ih^i mathbfW_ih^f mathbfW_ih^c mathbfW_ih^o.   Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound],   where bound = \\mathrm{inv}(\\sqrt{\\mathrm{out\\_dims}}).   The functions are applied in order:   the first initializes mathbfW_ih^i, the second mathbfW_ih^f,   the third mathbfW_ih^c, the fourth mathbfW_ih^o.   Default set to nothing.\ninit_recurrent_weight: Initializer for hidden-to-hidden weights   mathbfW_hh^i mathbfW_hh^f mathbfW_hh^o.   Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound],   where bound = \\mathrm{inv}(\\sqrt{\\mathrm{out\\_dims}}).   The functions are applied in order:   the first initializes mathbfW_hh^i, the second mathbfW_hh^f,   the third mathbfW_hh^o.   Default set to nothing.\ninit_memory_weight: Initializer for memory-to-hidden weights   mathbfW_mh^i mathbfW_mh^f mathbfW_mh^o.   Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound],   where bound = \\mathrm{inv}(\\sqrt{\\mathrm{out\\_dims}}).   The functions are applied in order:   the first initializes mathbfW_mh^i, the second mathbfW_mh^f,   the third mathbfW_mh^o.   Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights to map from input space    mathbfW_ih^f mathbfW_ih^c mathbfW_ih^i mathbfW_ih^o .\nweight_hh: Concatenated weights to map from hidden space    mathbfW_hh^f mathbfW_hh^c mathbfW_hh^i mathbfW_hh^o .\nweight_mh: Concatenated weights to map from memory space    mathbfW_mh^f mathbfW_mh^c mathbfW_mh^i .\nbias_ih: Concatenated bias vector for the input-hidden connection (not present if use_bias=false)    mathbfb_ih^f mathbfb_ih^c mathbfb_ih^i mathbfb_ih^o .\nbias_hh: Concatenated bias vector for the hidden-hidden connection (not present if use_bias=false)    mathbfb_hh^f mathbfb_hh^i mathbfb_hh^o .\nbias_mh: Concatenated bias vector for the memory-hidden connection (not present if use_bias=false)    mathbfb_mh^f mathbfb_mh^i mathbfb_mh^o .\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/minimalrnncell/#MultiplicativeLSTMCell","page":"MultiplicativeLSTMCell","title":"MultiplicativeLSTMCell","text":"","category":"section"},{"location":"api/cells/minimalrnncell/","page":"MultiplicativeLSTMCell","title":"MultiplicativeLSTMCell","text":"    MultiplicativeLSTMCell","category":"page"},{"location":"api/cells/minimalrnncell/#LuxRecurrentLayers.MultiplicativeLSTMCell","page":"MultiplicativeLSTMCell","title":"LuxRecurrentLayers.MultiplicativeLSTMCell","text":"MultiplicativeLSTMCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_multiplicative_bias=nothing, init_weight=nothing,\n    init_recurrent_weight=nothing, init_multiplicative_weight=nothing,\n    init_state=zeros32, init_memory=zeros32)\n\nMultiplicative long short term memory cell.\n\nEquations\n\nbeginaligned\n    mathbfm(t) = left( mathbfW_ih^m mathbfx(t) + mathbfb_ih^m\n        right) circ left( mathbfW_hh^m mathbfh(t-1) +\n        mathbfb_hh^m right) \n    hatmathbfh(t) = mathbfW_ih^h mathbfx(t) + mathbfb_ih^h\n        + mathbfW_mh^h mathbfm(t) + mathbfb_mh^h \n    mathbfi(t) = sigmaleft( mathbfW_ih^i mathbfx(t) +\n        mathbfb_ih^i + mathbfW_mh^i mathbfm(t) +\n        mathbfb_mh^i right) \n    mathbfo(t) = sigmaleft( mathbfW_ih^o mathbfx(t) +\n        mathbfb_ih^o + mathbfW_mh^o mathbfm(t) +\n        mathbfb_mh^o right) \n    mathbff(t) = sigmaleft( mathbfW_ih^f mathbfx(t) +\n        mathbfb_ih^f + mathbfW_mh^f mathbfm(t) +\n        mathbfb_mh^f right) \n    mathbfc(t) = mathbff(t) circ mathbfc(t-1) + mathbfi(t)\n        circ tanhleft( hatmathbfh(t) right) \n    mathbfh(t) = tanhleft( mathbfc(t) right) circ mathbfo(t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for input-to-hidden biases   mathbfb_ih^m mathbfb_ih^h mathbfb_ih^i mathbfb_ih^o mathbfb_ih^f.   Must be a tuple containing 5 functions. If a single value is passed, it is copied into a 5-element tuple.   If set to nothing, weights are initialized from a uniform distribution within [-bound, bound]   where bound = inv(sqrt(out_dims)).   The functions are applied in order:   the first initializes mathbfb_ih^m, the second mathbfb_ih^h, the third mathbfb_ih^i,   the fourth mathbfb_ih^o, and the fifth mathbfb_ih^f.\ninit_recurrent_bias: Initializer for hidden-to-hidden biases   mathbfb_hh^m.   Must be a tuple containing 1 function. If a single value is passed, it is used directly.   If set to nothing, weights are initialized from a uniform distribution within [-bound, bound]   where bound = inv(sqrt(out_dims)).  \ninit_multiplicative_bias: Initializer for multiplicative-to-hidden biases   mathbfb_mh^h mathbfb_mh^i mathbfb_mh^o mathbfb_mh^f.   Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4-element tuple.   If set to nothing, weights are initialized from a uniform distribution within [-bound, bound]   where bound = inv(sqrt(out_dims)).   The functions are applied in order:   the first initializes mathbfb_mh^h, the second mathbfb_mh^i,   the third mathbfb_mh^o, and the fourth mathbfb_mh^f.\ninit_weight: Initializer for input-to-hidden weights   mathbfW_ih^m mathbfW_ih^h mathbfW_ih^i mathbfW_ih^o mathbfW_ih^f.   Must be a tuple containing 5 functions. If a single value is passed, it is copied into a 5-element tuple.   If set to nothing, weights are initialized from a uniform distribution within [-bound, bound]   where bound = inv(sqrt(out_dims)).   The functions are applied in order:   the first initializes mathbfW_ih^m, the second mathbfW_ih^h,   the third mathbfW_ih^i, the fourth mathbfW_ih^o, and the fifth mathbfW_ih^f.\ninit_recurrent_weight: Initializer for hidden-to-hidden weights   mathbfW_hh^m.   Must be a tuple containing 1 function. If a single value is passed, it is used directly.   If set to nothing, weights are initialized from a uniform distribution within [-bound, bound]   where bound = inv(sqrt(out_dims)).  \ninit_multiplicative_weight: Initializer for multiplicative-to-hidden weights   mathbfW_mh^h mathbfW_mh^i mathbfW_mh^o mathbfW_mh^f.   Must be a tuple containing 4 functions. If a single value is passed, it is copied into a 4-element tuple.   If set to nothing, weights are initialized from a uniform distribution within [-bound, bound]   where bound = inv(sqrt(out_dims)).   The functions are applied in order:   the first initializes mathbfW_mh^h, the second mathbfW_mh^i,   the third mathbfW_mh^o, and the fourth mathbfW_mh^f.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Input-to-hidden weights    mathbfW_ih^m mathbfW_ih^h mathbfW_ih^i mathbfW_ih^o mathbfW_ih^f   \nweight_hh: Hidden-to-hidden weights    mathbfW_hh^m   \nweight_mh: Multiplicative-to-hidden weights    mathbfW_mh^h mathbfW_mh^i mathbfW_mh^o mathbfW_mh^f   \nbias_ih: Input-to-hidden biases (if use_bias=true)    mathbfb_ih^m mathbfb_ih^h mathbfb_ih^i mathbfb_ih^o mathbfb_ih^f   \nbias_hh: Hidden-to-hidden biases (if use_bias=true)    mathbfb_hh^m   \nbias_mh: Multiplicative-to-hidden biases (if use_bias=true)    mathbfb_mh^h mathbfb_mh^i mathbfb_mh^o mathbfb_mh^f   \nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/scrncell/#SCRNCell","page":"SCRNCell","title":"SCRNCell","text":"","category":"section"},{"location":"api/cells/scrncell/","page":"SCRNCell","title":"SCRNCell","text":"    SCRNCell","category":"page"},{"location":"api/cells/scrncell/#LuxRecurrentLayers.SCRNCell","page":"SCRNCell","title":"LuxRecurrentLayers.SCRNCell","text":"SCRNCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_weight=nothing, init_recurrent_weight=nothing,\n    init_context_weight=nothing, init_state=zeros32, init_memory=zeros32)\n\nStructurally contraint recurrent unit.\n\nEquations\n\nbeginaligned\n    mathbfs(t) = (1 - alpha) left( mathbfW_ih^s mathbfx(t) +\n        mathbfb_ih^s right) + alpha  mathbfs(t-1) \n    mathbfh(t) = sigmaleft(\n        mathbfW_ch^h mathbfs(t) + mathbfb_ch^h +\n        mathbfW_ih^h mathbfx(t) + mathbfb_ih^h +\n        mathbfW_hh^h mathbfh(t-1) + mathbfb_hh^h\n        right) \n    mathbfy(t) = fleft(\n        mathbfW_ch^y mathbfs(t) + mathbfb_ch^y +\n        mathbfW_hh^y mathbfh(t) + mathbfb_hh^y\n    right)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for input-to-hidden biases mathbfb_ih^s mathbfb_ih^h. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2-element tuple. If set to nothing, biases are initialized from a uniform distribution within [-bound, bound], where bound = inv(sqrt(out_dims)). The functions are applied in order: the first initializes mathbfb_ih^s, the second mathbfb_ih^h. Default set to nothing.\ninit_recurrent_bias: Initializer for hidden-to-hidden biases mathbfb_hh^h mathbfb_hh^y. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2-element tuple. If set to nothing, biases are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_context_bias: Initializer for context biases mathbfb_ch^h mathbfb_ch^y. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2-element tuple. If set to nothing, biases are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for input-to-hidden weights mathbfW_ih^s mathbfW_ih^h. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound], where bound = inv(sqrt(out_dims)). The functions are applied in order: the first initializes mathbfW_ih^s, the second mathbfW_ih^h. Default set to nothing.\ninit_recurrent_weight: Initializer for hidden-to-hidden weights mathbfW_hh^h mathbfW_hh^y. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_context_weight: Initializer for context weights mathbfW_ch^h mathbfW_ch^y. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the         updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ch: Context-to-hidden weights  mathbfW_ch^h mathbfW_ch^y \nweight_ih: Input-to-hidden weights  mathbfW_ih^s mathbfW_ih^h \nweight_hh: Hidden-to-hidden weights  mathbfW_hh^h mathbfW_hh^y \nbias_ch: Context-to-hidden biases (not present if use_bias=false)  mathbfb_ch^h mathbfb_ch^y \nbias_ih: Input-to-hidden biases (not present if use_bias=false)  mathbfb_ih^s mathbfb_ih^h \nbias_hh: Hidden-to-hidden biases (not present if use_bias=false)  mathbfb_hh^h mathbfb_hh^y \nalpha: Initial context strength\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/antisymmetricrnncell/#AntysimmetricRNNCell","page":"AntysimmetricRNNCell","title":"AntysimmetricRNNCell","text":"","category":"section"},{"location":"api/cells/antisymmetricrnncell/","page":"AntysimmetricRNNCell","title":"AntysimmetricRNNCell","text":"    AntisymmetricRNNCell","category":"page"},{"location":"api/cells/antisymmetricrnncell/#LuxRecurrentLayers.AntisymmetricRNNCell","page":"AntysimmetricRNNCell","title":"LuxRecurrentLayers.AntisymmetricRNNCell","text":"AntisymmetricRNNCell(in_dims => out_dims, [activation];\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_recurrent_bias=nothing, init_weight=nothing,\n    init_recurrent_weight=nothing, init_state=zeros32,\n    epsilon=1.0, gamma=0.0)\n\nAntisymmetric recurrent cell.\n\nEquations\n\nbeginequation\n    mathbfh(t) = mathbfh(t-1) + epsilon cdot tanhleft(\n        mathbfW_ih mathbfx(t) + mathbfb_ih +\n        (mathbfW_hh - mathbfW_hh^top - gamma cdot mathbfI)\n        mathbfh(t-1) + mathbfb_hh right)\nendequation\n\nArguments\n\nin_dims: Input dimension\nout_dims: Output (Hidden State & Memory) dimension\nactivation: activation function. Default is tanh\n\nKeyword arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for bias mathbfb_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_bias: Initializer for recurrent bias mathbfb_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for weight mathbfW_ih. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for recurrent weight mathbfW_hh. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\nepsilon: step size epsilon. Default is 1.0.\ngamma: strength of diffusion gamma. Default is 0.0.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights to map from input to the hidden state mathbfW_ih.\nweight_hh: Concatenated weights to map from hidden to the hidden state mathbfW_hh.\nbias_ih: Bias vector for the input-hidden connection (not present if   use_bias=false) mathbfb_ih.\nbias_hh: Bias vector for the hidden-hidden connection (not present if   use_bias=false) mathbfb_hh.\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/mut2cell/#MUT2Cell","page":"MUT2Cell","title":"MUT2Cell","text":"","category":"section"},{"location":"api/cells/mut2cell/","page":"MUT2Cell","title":"MUT2Cell","text":"    MUT2Cell","category":"page"},{"location":"api/cells/mut2cell/#LuxRecurrentLayers.MUT2Cell","page":"MUT2Cell","title":"LuxRecurrentLayers.MUT2Cell","text":"MUT2Cell(in_dims => out_dims;\n    use_bias=true, train_state=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nMutated unit 2 cell.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = sigmaleft( mathbfW_ih^z mathbfx(t) +\n        mathbfb_ih^z + mathbfW_hh^z mathbfh(t) +\n        mathbfb_hh^z right) \n    mathbfr(t) = sigmaleft( mathbfW_ih^r mathbfx(t) +\n        mathbfb_ih^r + mathbfW_hh^r mathbfh(t) +\n        mathbfb_hh^r right) \n    mathbfh(t+1) = tanhleft( mathbfW_hh^h left( mathbfr(t)\n        circ mathbfh(t) + mathbfb_hh^h right) +\n        mathbfW_ih^h mathbfx(t) + mathbfb_ih^h right) circ\n        mathbfz(t) \n    quad + mathbfh(t) circ left( 1 - mathbfz(t) right)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for input-to-hidden biases mathbfb_ih^z mathbfb_ih^r mathbfb_ih^h. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). The functions are applied in order: the first initializes mathbfb_ih^z, the second mathbfb_ih^r, and the third mathbfb_ih^h.\ninit_recurrent_bias: Initializer for hidden-to-hidden biases mathbfb_hh^z mathbfb_hh^r mathbfb_hh^h. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). The functions are applied in order: the first initializes mathbfb_hh^z, the second mathbfb_hh^r, and the third mathbfb_hh^h.\ninit_weight: Initializer for input-to-hidden weights mathbfW_ih^z mathbfW_ih^r mathbfW_ih^h. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). The functions are applied in order: the first initializes mathbfW_ih^z, the second mathbfW_ih^r, and the third mathbfW_ih^h.\ninit_recurrent_weight: Initializer for hidden-to-hidden weights mathbfW_hh^z mathbfW_hh^r mathbfW_hh^h. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). The functions are applied in order: the first initializes mathbfW_hh^z, the second mathbfW_hh^r, and the third mathbfW_hh^h.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Input-to-hidden weights  mathbfW_ih^z mathbfW_ih^r mathbfW_ih^h \nweight_hh: Hidden-to-hidden weights  mathbfW_hh^z mathbfW_hh^r mathbfW_hh^h \nbias_ih: Input-to-hidden biases (if use_bias=true)  mathbfb_ih^z mathbfb_ih^r mathbfb_ih^h \nbias_hh: Hidden-to-hidden biases (if use_bias=true)  mathbfb_hh^z mathbfb_hh^r mathbfb_hh^h \nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/trnncell/#TRNNCell","page":"TRNNCell","title":"TRNNCell","text":"","category":"section"},{"location":"api/cells/trnncell/","page":"TRNNCell","title":"TRNNCell","text":"    TRNNCell","category":"page"},{"location":"api/cells/trnncell/#LuxRecurrentLayers.TRNNCell","page":"TRNNCell","title":"LuxRecurrentLayers.TRNNCell","text":"TRNNCell(in_dims => out_dims;\n    use_bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_state=zeros32)\n\nStrongly typed recurrent unit.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = mathbfW_ih^z mathbfx(t) + mathbfb_ih^z \n    mathbff(t) = sigmaleft( mathbfW_ih^f mathbfx(t) +\n        mathbfb_ih^f right) \n    mathbfh(t) = mathbff(t) circ mathbfh(t-1) + left(1 -\n        mathbff(t)right) circ mathbfz(t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for input-to-hidden biases mathbfb_ih^z mathbfb_ih^f. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2-element tuple. If set to nothing, biases are initialized from a uniform distribution within [-bound, bound], where bound = \\mathrm{inv}(\\sqrt{\\mathrm{out\\_dims}}). The functions are applied in order: the first initializes mathbfb_ih^z, the second mathbfb_ih^f. Default set to nothing.\ninit_weight: Initializer for input-to-hidden weights mathbfW_ih^z mathbfW_ih^f. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound], where bound = \\mathrm{inv}(\\sqrt{\\mathrm{out\\_dims}}). The functions are applied in order: the first initializes mathbfW_ih^z, the second mathbfW_ih^f. Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights to map from input space  mathbfW_ih^z mathbfW_ih^f \nbias_ih: Concatenated bias vector for input-hidden connections  mathbfb_ih^z mathbfb_ih^f  (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/#Cells","page":"Cells","title":"Cells","text":"","category":"section"},{"location":"api/cells/","page":"Cells","title":"Cells","text":"This module provides several recurrent layers. In this page you can access their API documentation from this list:","category":"page"},{"location":"api/cells/","page":"Cells","title":"Cells","text":"AntisymmetricRNNCell\nATRCell\nBRCell\nCFNCell\ncoRNNCell\nFastGRNNCell\nFastRNNCell\nGatedAntisymmetricRNNCell\nIndRNNCell\nJANETCell\nLEMCell\nLightRUCell\nLiGRUCell\nMGUCell\nMinimalRNNCell\nMultiplicativeLSTMCell\nMUT1Cell\nMUT2Cell\nMUT3Cell\nNASCell\nNBRCell\nPeepholeLSTMCell\nRANCell\nSCRNCell\nSGRNCell\nSTARCell\nTGRUCell\nTLSTMCell\nTRNNCell\nUnICORNNCell\nWMCLSTMCell","category":"page"},{"location":"api/cells/lemcell/#LEMCell","page":"LEMCell","title":"LEMCell","text":"","category":"section"},{"location":"api/cells/lemcell/","page":"LEMCell","title":"LEMCell","text":"    LEMCell","category":"page"},{"location":"api/cells/lemcell/#LuxRecurrentLayers.LEMCell","page":"LEMCell","title":"LuxRecurrentLayers.LEMCell","text":"LEMCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32, init_memory=zeros32, dt=1.0)\n\nLong expressive memory unit.\n\nEquations\n\nbeginaligned\n    boldsymbolDelta t(t) = Delta t cdot hatsigma left(\n        mathbfW_ih^1 mathbfx(t) + mathbfb_ih^1 +\n        mathbfW_hh^1 mathbfh(t-1) + mathbfb_hh^1 right) \n    overlineboldsymbolDelta t(t) = Delta t cdot hatsigma left(\n        mathbfW_ih^2 mathbfx(t) + mathbfb_ih^2 +\n        mathbfW_hh^2 mathbfh(t-1) + mathbfb_hh^2 right) \n    mathbfc(t) = left(1 - boldsymbolDelta t(t)right) circ mathbfc(t-1) +\n        boldsymbolDelta t(t) circ sigmaleft(\n        mathbfW_ih^c mathbfx(t) + mathbfb_ih^c +\n        mathbfW_hh^c mathbfh(t-1) + mathbfb_hh^c right) \n    mathbfh(t) = left(1 - boldsymbolDelta t(t)right) circ mathbfh(t-1) +\n        boldsymbolDelta t(t) circ sigmaleft(\n        mathbfW_ih^h mathbfx(t) + mathbfb_ih^h +\n        mathbfW_ch mathbfc(t) + mathbfb_ch right)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for input-to-hidden biases mathbfb_ih^1 mathbfb_ih^2 mathbfb_ih^c mathbfb_ih^h. Must be a tuple of 4 functions, e.g., (glorot_uniform, kaiming_uniform, lecun_normal, zeros). If a single function is passed, it is expanded to a 4-element tuple. If set to nothing, biases are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden-to-hidden biases mathbfb_hh^1 mathbfb_hh^2 mathbfb_hh^c mathbfb_hh^h. Must be a tuple of 3 functions. If a single function is passed, it is expanded to a 3-element tuple. If set to nothing, biases are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_cell_bias: Initializer for bias mathbfb_ch. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input-to-hidden weights mathbfW_ih^1 mathbfW_ih^2 mathbfW_ih^c mathbfW_ih^h. Must be a tuple of 4 functions. If a single function is passed, it is expanded to a 4-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for hidden-to-hidden weights mathbfW_hh^1 mathbfW_hh^2 mathbfW_hh^c mathbfW_hh^h. Must be a tuple of 3 functions. If a single function is passed, it is expanded to a 3-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_cell_weight: Initializer for input to hidden weight mathbfW_ch. If set to\nnothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\ndt: timestep. Defaul is 1.0.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the         updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights mapping from input to internal units  mathbfW_ih^1 mathbfW_ih^2 mathbfW_ih^c mathbfW_ih^h  The functions provided in init_weight are applied in order: the first initializes mathbfW_ih^1, the second mathbfW_ih^2, the third mathbfW_ih^c, and the fourth mathbfW_ih^h.\nweight_hh: Concatenated weights mapping from hidden state to internal units  mathbfW_hh^1 mathbfW_hh^2 mathbfW_hh^c  The functions provided in init_recurrent_weight are applied in order: the first initializes mathbfW_hh^1, the second mathbfW_hh^2, and the third mathbfW_hh^c.\nweight_ch: Weights to map from cell space mathbfW_ch.\nbias_ih: Concatenated input-to-hidden bias vectors (not present if use_bias=false)  mathbfb_ih^1 mathbfb_ih^2 mathbfb_ih^c mathbfb_ih^h  The functions provided in init_bias are applied in order: the first initializes mathbfb_ih^1, the second mathbfb_ih^2, the third mathbfb_ih^c, and the fourth mathbfb_ih^h.\nbias_hh: Concatenated hidden-to-hidden bias vectors (not present if use_bias=false)  mathbfb_hh^1 mathbfb_hh^2 mathbfb_hh^c  The functions provided in init_recurrent_bias are applied in order: the first initializes mathbfb_hh^1, the second mathbfb_hh^2, and the third mathbfb_hh^c.\nbias_ch: Bias vector for the cell-hidden connection mathbfb_ch (not present if use_bias=false)\nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/nbrcell/#NBRCell","page":"NBRCell","title":"NBRCell","text":"","category":"section"},{"location":"api/cells/nbrcell/","page":"NBRCell","title":"NBRCell","text":"    NBRCell","category":"page"},{"location":"api/cells/nbrcell/#LuxRecurrentLayers.NBRCell","page":"NBRCell","title":"LuxRecurrentLayers.NBRCell","text":"NBRCell(in_dims => out_dims;\n    use_bias=true, use_recurrent_ bias=true, train_state=false, init_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nRecurrently neuromodulated bistable recurrent cell.\n\nEquations\n\nbeginaligned\n    mathbfa(t) = 1 + tanhleft(mathbfW_ih^a mathbfx(t) +\n        mathbfb_ih^a + mathbfW_hh^a circ mathbfh(t-1)+\n        mathbfb_hh^a right) \n    mathbfc(t) = sigmaleft(mathbfW_ih^c mathbfx(t) +\n        mathbfb_ih^c + mathbfW_hh^c circ mathbfh(t-1) +\n        mathbfb_hh^c right)\n    mathbfh(t) = mathbfc(t) circ mathbfh(t-1) + (1 - mathbfc(t))\n        circ tanhleft(mathbfW_ih^h mathbfx(t) + mathbfb_ih^h +\n        mathbfa(t) circ mathbfh(t-1)right)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias mathbfb_ih in the computation. Default set to true.\nuse_recurrent_bias: Flag to use recurrent bias mathbfb_hh in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for input to hidden bias mathbfb_ih^a mathbfb_ih^c mathbfb_ih^h. Must be a tuple containing 3 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 3-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_bias: Initializer for hidden to hidden bias mathbfb_hh^a mathbfb_hh^c. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_weight: Initializer for input to hidden weights mathbfW_ih^a mathbfW_ih^c mathbfW_ih^h. Must be a tuple containing 3 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 3-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_recurrent_weight: Initializer for input to hidden weights mathbfW_hh^a mathbfW_hh^c. Must be a tuple containing 2 functions, e.g., (glorot_normal, kaiming_uniform). If a single function fn is provided, it is automatically expanded into a 2-element tuple (fn, fn). If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). Default is nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights to map from input to the hidden state               mathbfW_ih^a mathbfW_ih^c mathbfW_ih^h  The initializers in init_weight are applied in the order they appear: the first function is used for mathbfW_ih^a, the second for mathbfW_ih^c, and the third for mathbfW_ih^h.\nweight_hh: Weights to map the hidden state to the hidden state               mathbfW_hh^a mathbfW_hh^c  The initializers in init_weight are applied in the order they appear: the first function is used for mathbfW_hh^a, and the second for mathbfW_hh^c.\nbias_ih: Bias vector for the input-hidden connection (not present if use_bias=false)               mathbfb_ih^a mathbfb_ih^c mathbfb_ih^h  The initializers in init_bias are applied in the order they appear: the first function is used for mathbfb_ih^z, the second for mathbfb_ih^c, and the third for mathbfb_ih^h.\nbias_hh: Bias vector for the input-hidden connection (not present if use_bias=false)               mathbfb_hh^a mathbfb_hh^c  The initializers in init_bias are applied in the order they appear: the first function is used for mathbfb_hh^z, and the second for mathbfb_hh^c.\nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/starcell/#STARCell","page":"STARCell","title":"STARCell","text":"","category":"section"},{"location":"api/cells/starcell/","page":"STARCell","title":"STARCell","text":"    STARCell","category":"page"},{"location":"api/cells/starcell/#LuxRecurrentLayers.STARCell","page":"STARCell","title":"LuxRecurrentLayers.STARCell","text":"STARCell(in_dims => out_dims;\n    use_bias=true, train_state=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nStackable recurrent cell.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = tanhleft(mathbfW_ih^z mathbfx(t) +\n        mathbfb_ih^zright) \n    mathbfk(t) = sigmaleft(mathbfW_ih^k mathbfx(t) +\n        mathbfb_ih^k + mathbfW_hh^k mathbfh(t-1) +\n        mathbfb_hh^kright) \n    mathbfh(t) = tanhleft((1 - mathbfk(t)) circ mathbfh(t-1) +\n        mathbfk(t) circ mathbfz(t)right)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable.   Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable.   Default set to false.\ninit_bias: Initializer for input-to-hidden biases   mathbfb_ih^z mathbfb_ih^k.   Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2-element tuple. If set to nothing, biases are initialized from a uniform distribution within [-bound, bound],   where bound = inv(sqrt(out_dims)).   The functions are applied in order: the first initializes   mathbfb_ih^z, the second mathbfb_ih^k.   Default set to nothing.\ninit_recurrent_bias: Initializer for hidden-to-hidden bias   mathbfb_hh^k.   Must be a single function. If set to nothing, bias is initialized   from a uniform distribution within [-bound, bound],   where bound = inv(sqrt(out_dims)).   Default set to nothing.\ninit_weight: Initializer for input-to-hidden weights   mathbfW_ih^z mathbfW_ih^k.   Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound],   where bound = inv(sqrt(out_dims)).   The functions are applied in order: the first initializes   mathbfW_ih^z, the second mathbfW_ih^k.   Default set to nothing.\ninit_recurrent_weight: Initializer for hidden-to-hidden weight   mathbfW_hh^k.   Must be a single function. If set to nothing, weight is initialized   from a uniform distribution within [-bound, bound],   where bound = inv(sqrt(out_dims)).   Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Input-to-hidden weights    mathbfW_ih^z mathbfW_ih^k \nweight_hh: Hidden-to-hidden weights    mathbfW_hh^k \nbias_ih: Input-to-hidden biases (not present if use_bias=false)    mathbfb_ih^z mathbfb_ih^k \nbias_hh: Hidden-to-hidden bias (not present if use_bias=false)    mathbfb_hh^k \nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/nascell/#NASCell","page":"NASCell","title":"NASCell","text":"","category":"section"},{"location":"api/cells/nascell/","page":"NASCell","title":"NASCell","text":"    NASCell","category":"page"},{"location":"api/cells/nascell/#LuxRecurrentLayers.NASCell","page":"NASCell","title":"LuxRecurrentLayers.NASCell","text":"NASCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32, init_memory=zeros32)\n\nNeural Architecture Search unit.\n\nEquations\n\nbeginaligned\n    mathbfo_1(t) = sigmaleft( mathbfW_ih^(1) mathbfx(t) +\n        mathbfb_ih^(1) + mathbfW_hh^(1) mathbfh(t-1) +\n        mathbfb_hh^(1) right) \n    mathbfo_2(t) = textReLUleft( mathbfW_ih^(2) mathbfx(t) +\n        mathbfb_ih^(2) + mathbfW_hh^(2) mathbfh(t-1) +\n        mathbfb_hh^(2) right) \n    mathbfo_3(t) = sigmaleft( mathbfW_ih^(3) mathbfx(t) +\n        mathbfb_ih^(3) + mathbfW_hh^(3) mathbfh(t-1) +\n        mathbfb_hh^(3) right) \n    mathbfo_4(t) = textReLUleft( left( mathbfW_ih^(4)\n        mathbfx(t) + mathbfb_ih^(4) right) circ left(\n        mathbfW_hh^(4) mathbfh(t-1) + mathbfb_hh^(4) right)\n        right) \n    mathbfo_5(t) = tanhleft( mathbfW_ih^(5) mathbfx(t) +\n        mathbfb_ih^(5) + mathbfW_hh^(5) mathbfh(t-1) +\n        mathbfb_hh^(5) right) \n    mathbfo_6(t) = sigmaleft( mathbfW_ih^(6) mathbfx(t) +\n        mathbfb_ih^(6) + mathbfW_hh^(6) mathbfh(t-1) +\n        mathbfb_hh^(6) right) \n    mathbfo_7(t) = tanhleft( mathbfW_ih^(7) mathbfx(t) +\n        mathbfb_ih^(7) + mathbfW_hh^(7) mathbfh(t-1) +\n        mathbfb_hh^(7) right) \n    mathbfo_8(t) = sigmaleft( mathbfW_ih^(8) mathbfx(t) +\n        mathbfb_ih^(8) + mathbfW_hh^(8) mathbfh(t-1) +\n        mathbfb_hh^(8) right) \n    mathbfl_1(t) = tanhleft( mathbfo_1(t) circ mathbfo_2(t)\n        right) \n    mathbfl_2(t) = tanhleft( mathbfo_3(t) + mathbfo_4(t) right) \n    mathbfl_3(t) = tanhleft( mathbfo_5(t) circ mathbfo_6(t) right) \n    mathbfl_4(t) = sigmaleft( mathbfo_7(t) + mathbfo_8(t) right) \n    mathbfl_1(t) = tanhleft( mathbfl_1(t) + mathbfc(t-1) right) \n    mathbfc(t) = mathbfl_1(t) circ mathbfl_2(t) \n    mathbfl_5(t) = tanhleft( mathbfl_3(t) + mathbfl_4(t) right) \n    mathbfh(t) = tanhleft( mathbfc(t) circ mathbfl_5(t) right)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable.   Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable.   Default set to false.\ninit_bias: Initializer for input-to-hidden biases    mathbfb_ih^(1) mathbfb_ih^(2) dots mathbfb_ih^(8) .   Must be a tuple containing 8 functions. If a single value is passed, it is copied into an 8-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound], where bound = inv(sqrt(out_dims)). The functions are applied in order to initialize mathbfb_ih^(1) through mathbfb_ih^(8). Default set to nothing.\ninit_recurrent_bias: Initializer for hidden-to-hidden biases    mathbfb_hh^(1) mathbfb_hh^(2) dots mathbfb_hh^(8) .   Must be a tuple containing 8 functions. If a single value is passed, it is copied into an 8-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound],   where bound = inv(sqrt(out_dims)). The functions are applied in order to initialize mathbfb_hh^(1) through mathbfb_hh^(8). Default set to nothing.\ninit_weight: Initializer for input-to-hidden weights    mathbfW_ih^(1) mathbfW_ih^(2) dots mathbfW_ih^(8) .   Must be a tuple containing 8 functions. If a single value is passed, it is copied into an 8-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound],   where bound = inv(sqrt(out_dims)). The functions are applied in order to initialize mathbfW_ih^(1) through mathbfW_ih^(8). Default set to nothing.\ninit_recurrent_weight: Initializer for hidden-to-hidden weights    mathbfW_hh^(1) mathbfW_hh^(2) dots mathbfW_hh^(8) .   Must be a tuple containing 8 functions. If a single value is passed, it is copied into an 8-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound],   where bound = inv(sqrt(out_dims)). The functions are applied in order to initialize mathbfW_hh^(1) through mathbfW_hh^(8). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the          updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Input-to-hidden weights    mathbfW_ih^(1) mathbfW_ih^(2) dots mathbfW_ih^(8)   \nweight_hh: Hidden-to-hidden weights    mathbfW_hh^(1) mathbfW_hh^(2) dots mathbfW_hh^(8)   \nbias_ih: Input-to-hidden biases (if use_bias=true)    mathbfb_ih^(1) mathbfb_ih^(2) dots mathbfb_ih^(8)   \nbias_hh: Hidden-to-hidden biases (if use_bias=true)    mathbfb_hh^(1) mathbfb_hh^(2) dots mathbfb_hh^(8)   \nhidden_state: Initial hidden state vector (not present if train_state=false)  \nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/tgrucell/#TGRUCell","page":"TGRUCell","title":"TGRUCell","text":"","category":"section"},{"location":"api/cells/tgrucell/","page":"TGRUCell","title":"TGRUCell","text":"    TGRUCell","category":"page"},{"location":"api/cells/tgrucell/#LuxRecurrentLayers.TGRUCell","page":"TGRUCell","title":"LuxRecurrentLayers.TGRUCell","text":"TGRUCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32, init_memory=zeros32)\n\nStrongly typed gated recurrent unit.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = mathbfW_ih^z  mathbfx(t) + mathbfb_ih^z\n        + mathbfW_mh^z  mathbfx(t-1) + mathbfb_mh^z \n    mathbff(t) = sigmaleft( mathbfW_ih^f  mathbfx(t) +\n        mathbfb_ih^f + mathbfW_mh^f  mathbfx(t-1) +\n        mathbfb_mh^f right) \n    mathbfo(t) = tauleft( mathbfW_ih^o  mathbfx(t) +\n        mathbfb_ih^o + mathbfW_mh^o  mathbfx(t-1) +\n        mathbfb_mh^o right) \n    mathbfh(t) = mathbff(t) circ mathbfh(t-1) + mathbfz(t) circ\n        mathbfo(t)\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for input-to-hidden biases mathbfb_ih^z mathbfb_ih^f mathbfb_ih^o. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3-element tuple. If set to nothing, biases are initialized from a uniform distribution within [-bound, bound], where bound = \\mathrm{inv}(\\sqrt{\\mathrm{out\\_dims}}). The functions are applied in order: the first initializes mathbfb_ih^z, then mathbfb_ih^f, and mathbfb_ih^o. Default set to nothing.\ninit_recurrent_bias: Initializer for hidden-to-hidden biases mathbfb_mh^z mathbfb_mh^f mathbfb_mh^o. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3-element tuple. If set to nothing, biases are initialized from a uniform distribution within [-bound, bound], where bound = \\mathrm{inv}(\\sqrt{\\mathrm{out\\_dims}}). Default set to nothing.\ninit_weight: Initializer for input-to-hidden weights mathbfW_ih^z mathbfW_ih^f mathbfW_ih^o. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound], where bound = \\mathrm{inv}(\\sqrt{\\mathrm{out\\_dims}}). The functions are applied in order: the first initializes mathbfW_ih^z, then mathbfW_ih^f, and mathbfW_ih^o. Default set to nothing.\ninit_recurrent_weight: Initializer for hidden-to-hidden weights mathbfW_mh^z mathbfW_mh^f mathbfW_mh^o. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound], where bound = \\mathrm{inv}(\\sqrt{\\mathrm{out\\_dims}}). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the         updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Concatenated weights for input-to-hidden transformations  mathbfW_ih^z mathbfW_ih^f mathbfW_ih^o .\nweight_hh: Concatenated weights for hidden-to-hidden transformations  mathbfW_mh^z mathbfW_mh^f mathbfW_mh^o .\nbias_ih: Input-to-hidden bias vector  mathbfb_ih^z mathbfb_ih^f mathbfb_ih^o . (not present if use_bias=false).\nbias_hh: Hidden-to-hidden bias vector  mathbfb_mh^z mathbfb_mh^f mathbfb_mh^o . (not present if use_bias=false).\nhidden_state: Initial hidden state vector (not present if train_state=false).\nmemory: Initial memory vector (not present if train_memory=false).\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/cells/unicornncell/#UnICORNNCell","page":"UnICORNNCell","title":"UnICORNNCell","text":"","category":"section"},{"location":"api/cells/unicornncell/","page":"UnICORNNCell","title":"UnICORNNCell","text":"    UnICORNNCell","category":"page"},{"location":"api/cells/unicornncell/#LuxRecurrentLayers.UnICORNNCell","page":"UnICORNNCell","title":"LuxRecurrentLayers.UnICORNNCell","text":"UnICORNNCell(in_dims => out_dims;\n    use_bias=true, train_state=false, train_memory=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32, init_memory=zeros32,\n    dt=1.0, alpha=0.0)\n\nUndamped independent controlled oscillatory recurrent neural unit.\n\nEquations\n\nbeginaligned\n    mathbfh(t) = mathbfh(t-1) + Delta t cdot hatsigma(mathbfw_ch)\n        circ mathbfz(t) \n    mathbfz(t) = mathbfz(t-1) - Delta t cdot hatsigma(mathbfw_ch)\n        circ left sigma left( mathbfw_hh circ mathbfh(t-1) +\n        mathbfW_ih mathbfx(t) + mathbfb_ih right) + alpha cdot\n        mathbfh(t-1) right\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ntrain_memory: Flag to set the initial memory state as trainable. Default set to false.\ninit_bias: Initializer for input bias mathbfb_ih. Must be a single function. If set to nothing, the bias is initialized from a uniform distribution within [-bound, bound], where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_weight: Initializer for input weight mathbfW_ih. Must be a single function. If set to nothing, the weight is initialized from a uniform distribution within [-bound, bound], where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_recurrent_weight: Initializer for recurrent weight mathbfw_hh. Must be a single function. If set to nothing, the weight is initialized from a uniform distribution within [-bound, bound], where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_control_weight: Initializer for control weight mathbfw_ch. Must be a single function. If set to nothing, the weight is initialized from a uniform distribution within [-bound, bound], where bound = inv(sqrt(out_dims)). Default set to nothing.\ninit_state: Initializer for hidden state. Default set to zeros32.\ninit_memory: Initializer for memory. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to false - Creates a hidden state using          init_state, hidden memory using init_memory and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to false - Repeats hidden_state vector          from the parameters to match the shape of x, creates hidden memory using          init_memory and proceeds to Case 2.\nCase 1c: Only a single input x of shape (in_dims, batch_size), train_state is set          to false, train_memory is set to true - Creates a hidden state using          init_state, repeats the memory vector from parameters to match the shape of          x and proceeds to Case 2.\nCase 1d: Only a single input x of shape (in_dims, batch_size), train_state is set          to true, train_memory is set to true - Repeats the hidden state and          memory vectors from the parameters to match the shape of  x and proceeds to          Case 2.\nCase 2: Tuple (x, (h, c)) is provided, then the output and a tuple containing the         updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new and new memory c_new\nUpdated model state\n\nParameters\n\nweight_ih: Input-to-hidden weight  mathbfW_ih \nweight_hh: Elementwise recurrent weight  mathbfw_hh \nweight_ch: Elementwise control weight  mathbfw_ch \nbias_ih: Input-to-hidden bias (not present if use_bias=false)  mathbfb_ih \nhidden_state: Initial hidden state vector (not present if train_state=false)\nmemory: Initial memory vector (not present if train_memory=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = LuxRecurrentLayers","category":"page"},{"location":"#LuxRecurrentLayers","page":"Home","title":"LuxRecurrentLayers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for LuxRecurrentLayers.","category":"page"},{"location":"api/cells/mut1cell/#MUT1Cell","page":"MUT1Cell","title":"MUT1Cell","text":"","category":"section"},{"location":"api/cells/mut1cell/","page":"MUT1Cell","title":"MUT1Cell","text":"    MUT1Cell","category":"page"},{"location":"api/cells/mut1cell/#LuxRecurrentLayers.MUT1Cell","page":"MUT1Cell","title":"LuxRecurrentLayers.MUT1Cell","text":"MUT1Cell(in_dims => out_dims;\n    use_bias=true, train_state=false,\n    init_bias=nothing, init_recurrent_bias=nothing,\n    init_weight=nothing, init_recurrent_weight=nothing,\n    init_state=zeros32)\n\nMutated unit 1 cell.\n\nEquations\n\nbeginaligned\n    mathbfz(t) = sigmaleft(mathbfW_ih^z mathbfx(t) +\n        mathbfb_ih^zright) \n    mathbfr(t) = sigmaleft(mathbfW_ih^r mathbfx(t) +\n        mathbfb_ih^r + mathbfW_hh^r mathbfh(t) +\n        mathbfb_hh^rright) \n    mathbfh(t+1) = left tanhleft( mathbfW_hh^h left(\n        mathbfr(t) circ mathbfh(t) + mathbfb_hh^h right) + tanh(mathbfW_ih^h\n        mathbfx(t)) + mathbfb_ih^h right) right circ mathbfz(t)\n        + mathbfh(t) circ (1 - mathbfz(t))\nendaligned\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\n\nKeyword Arguments\n\nuse_bias: Flag to use bias in the computation. Default set to true.\ntrain_state: Flag to set the initial hidden state as trainable. Default set to false.\ninit_bias: Initializer for input-to-hidden biases mathbfb_ih^z mathbfb_ih^r mathbfb_ih^h. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). The functions are applied in order: the first initializes mathbfb_ih^z, the second mathbfb_ih^r, and the third mathbfb_ih^h.\ninit_recurrent_bias: Initializer for hidden-to-hidden biases mathbfb_hh^r mathbfb_hh^h. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). The functions are applied in order: the first initializes mathbfb_hh^r, and the second mathbfb_hh^h.\ninit_weight: Initializer for input-to-hidden weights mathbfW_ih^z mathbfW_ih^r mathbfW_ih^h. Must be a tuple containing 3 functions. If a single value is passed, it is copied into a 3-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). The functions are applied in order: the first initializes mathbfW_ih^z, the second mathbfW_ih^r, and the third mathbfW_ih^h.\ninit_recurrent_weight: Initializer for hidden-to-hidden weights mathbfW_hh^r mathbfW_hh^h. Must be a tuple containing 2 functions. If a single value is passed, it is copied into a 2-element tuple. If set to nothing, weights are initialized from a uniform distribution within [-bound, bound] where bound = inv(sqrt(out_dims)). The functions are applied in order: the first initializes mathbfW_hh^r, and the second mathbfW_hh^h.\ninit_state: Initializer for hidden state. Default set to zeros32.\n\nInputs\n\nCase 1a: Only a single input x of shape (in_dims, batch_size), train_state is set          to false - Creates a hidden state using init_state and proceeds to Case 2.\nCase 1b: Only a single input x of shape (in_dims, batch_size), train_state is set          to true - Repeats hidden_state from parameters to match the shape of x          and proceeds to Case 2.\nCase 2: Tuple (x, (h, )) is provided, then the output and a tuple containing the         updated hidden state is returned.\n\nReturns\n\nTuple containing\nOutput h_new of shape (out_dims, batch_size)\nTuple containing new hidden state h_new\nUpdated model state\n\nParameters\n\nweight_ih: Input-to-hidden weights  mathbfW_ih^z mathbfW_ih^r mathbfW_ih^h \nweight_hh: Hidden-to-hidden weights  mathbfW_hh^r mathbfW_hh^h \nbias_ih: Input-to-hidden biases (if use_bias=true)  mathbfb_ih^z mathbfb_ih^r mathbfb_ih^h \nbias_hh: Hidden-to-hidden biases (if use_bias=true)  mathbfb_hh^r mathbfb_hh^h \nhidden_state: Initial hidden state vector (not present if train_state=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"}]
}
